{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340a1f23-3471-4ebc-9d48-68b45457c362",
   "metadata": {},
   "source": [
    "# **TweetEval: Sentiment**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2247fca-a5f8-4752-acf1-146b524a7c7d",
   "metadata": {},
   "source": [
    "Данный код базируется на коде автора: https://github.com/marco-siino/text_preprocessing_impact/blob/main/20N_DS/LR_20N_TextPreProImpact_NB.ipynb. Научная статья разработчика указана в списке использованной литературы и специально дублируется здесь: https://www.sciencedirect.com/science/article/pii/S0306437923001783?ref=cra_js_challenge&fr=RR-1 \n",
    "\n",
    "В курсовой работе уже используются предложенные комбинации методов предобработки для Логистической регрессии, Наивного Байесовского классификатора и Метода опорных векторов. Для отсутствующих моделей: XGBoost, AdaBoost, RandomForest, DecisionTree - эксперимент воспроизводится с изменениями в процессе загрузки датасетов и некоторыми вытекающими правками в коде. Использование оригинального кода позволяет получить недостающие данные и сохранить полноту курсовой работы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca026184-019b-4e83-bf68-8d095414c821",
   "metadata": {},
   "source": [
    "# Импорт модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7f7965-e80c-44e8-94bf-0f753bc7db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.environ['TF_CUDNN_DETERMINISTIC']='true'\n",
    "os.environ['TF_DETERMINISTIC_OPS']='true'\n",
    "\n",
    "import textblob.download_corpora as dl\n",
    "dl.download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309e2f2-e229-4c93-a9cd-8cf52d68624e",
   "metadata": {},
   "source": [
    "# Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a371f96-4904-40c9-821b-57329873a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/raw/tweet_eval_sentiment/test.csv'\n",
    "train_dir = '../data/raw/tweet_eval_sentiment/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f012ae72-6ee1-47d4-9929-dfb77b7a10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_dir)\n",
    "train_df = pd.read_csv(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e2863-5931-4d5d-ac7a-0e4783cc15f7",
   "metadata": {},
   "source": [
    "# Создаем обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a8f8ecb-3bd2-44c5-af20-5ce7d10a1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train_df[\"text\"].astype(str).values\n",
    "y_train = train_df[\"label\"].values\n",
    "X_test_text = test_df[\"text\"].astype(str).values\n",
    "y_test = test_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6693d80c-ce4b-4e7e-9e17-8d3962ce5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_text, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0f9a3-0dfd-41bf-8738-2707c9012ef4",
   "metadata": {},
   "source": [
    "# Функции предобработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6953b286-ce4a-4b88-9794-130ae0ab8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do-Nothing preprocessing function.\n",
    "def DON(input_data):\n",
    "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, r'<\\!\\[CDATA\\[', ' ')\n",
    "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed, r'\\]{1,}>', ' ')\n",
    "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'', ' ')\n",
    "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'', ' ')\n",
    "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed, r'\\n(\\t){0,2}', '')\n",
    "  output_data = tf.strings.regex_replace(tag_open_documents_removed, r'\\n(\\t){0,2}', ' ')\n",
    "  return output_data\n",
    "\n",
    "# Lowercasing preprocessing function.\n",
    "def LOW(input_data):\n",
    "  return tf.strings.lower(DON(input_data))\n",
    "\n",
    "# Removing Stop Words function.\n",
    "def RSW(input_data):\n",
    "  output_data = DON(input_data)\n",
    "\n",
    "  try:\n",
    "    input_string=output_data[0]\n",
    "\n",
    "  except:\n",
    "    input_string=output_data\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant(output_string)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  else:\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant([[output_string]])\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  return output_data\n",
    "\n",
    "# Porter Stemmer preprocessing function.\n",
    "def STM(input_data):\n",
    "  output_data = DON(input_data)\n",
    "  stemmer = PorterStemmer()\n",
    "\n",
    "  try:\n",
    "    input_string=output_data[0]\n",
    "\n",
    "  except:\n",
    "    input_string=output_data\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [stemmer.stem(word) for word in blob]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant(output_string)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  else:\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [stemmer.stem(word) for word in blob]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant([[output_string]])\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76e8555b-3e4f-4f6f-856f-de9f6807cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
    "#...5\n",
    "def LOW_RSW(input_data):\n",
    "  return RSW(LOW(input_data))\n",
    "\n",
    "# 6\n",
    "def LOW_STM(input_data):\n",
    "  return STM(LOW(input_data))\n",
    "\n",
    "# 7\n",
    "def RSW_LOW(input_data):\n",
    "  return LOW(RSW(input_data))\n",
    "\n",
    "# 8\n",
    "def RSW_STM(input_data):\n",
    "  return STM(RSW(input_data))\n",
    "\n",
    "# 9\n",
    "def STM_LOW(input_data):\n",
    "  return LOW(STM(input_data))\n",
    "\n",
    "# 10\n",
    "def STM_RSW(input_data):\n",
    "  return RSW(STM(input_data))\n",
    "\n",
    "# 11\n",
    "def LOW_STM_RSW(input_data):\n",
    "  return RSW(STM(LOW(input_data)))\n",
    "\n",
    "# 12\n",
    "def LOW_RSW_STM(input_data):\n",
    "  return STM(RSW(LOW(input_data)))\n",
    "\n",
    "# 13\n",
    "def STM_LOW_RSW(input_data):\n",
    "  return RSW(LOW(STM(input_data)))\n",
    "\n",
    "# 14\n",
    "def STM_RSW_LOW(input_data):\n",
    "  return LOW(RSW(STM(input_data)))\n",
    "\n",
    "# 15\n",
    "def RSW_LOW_STM(input_data):\n",
    "  return STM(LOW(RSW(input_data)))\n",
    "\n",
    "# 16\n",
    "def RSW_STM_LOW(input_data):\n",
    "  return LOW(STM(RSW(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a967d7f-c8da-4712-81f6-17b32f7c067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 0\n",
    "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
    "  # Set a large sequence length to find the longest sample in the training set.\n",
    "  sequence_length = 15000\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize=preprocessing_function,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=sequence_length,\n",
    "      encoding='ISO-8859-1')\n",
    "\n",
    "  train_text = training_set.map(lambda x, y: x)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "  model.add(vectorize_layer)\n",
    "\n",
    "  longest_sample_length=1\n",
    "\n",
    "  for element in training_set:\n",
    "    authorDocument=element[0]\n",
    "    label=element[1]\n",
    "\n",
    "    author_batch = tf.expand_dims(authorDocument, axis=0)\n",
    "    out = model(author_batch)\n",
    "    # Convert token list to numpy array.\n",
    "    token_list = out.numpy()[0]\n",
    "    token_list = np.trim_zeros(token_list,'b')\n",
    "    if longest_sample_length < len(token_list):\n",
    "      longest_sample_length = len(token_list)\n",
    "\n",
    "  print(\"Length of the longest sample is:\", longest_sample_length)\n",
    "\n",
    "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
    "  sequence_length = longest_sample_length\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize=preprocessing_function,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=sequence_length,\n",
    "      encoding='ISO-8859-1')\n",
    "\n",
    "  # Finally adapt the vectorize layer.\n",
    "  train_text = training_set.map(lambda x, y: x)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "  global max_features\n",
    "  max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
    "  return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13c78204-8b92-4386-846a-137461880caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = defaultdict(lambda: defaultdict(list))\n",
    "prepro_functions_dict_base = {\n",
    "    'DON':DON,\n",
    "    'LOW':LOW,\n",
    "    'RSW':RSW,\n",
    "    'STM':STM\n",
    "    }\n",
    "\n",
    "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
    "\n",
    "prepro_functions_dict_comb = {\n",
    "    # 1. Do nothing\n",
    "    'DON': DON,\n",
    "    # 2. Lowercasing\n",
    "    'LOW':LOW,\n",
    "    # 3. Removing Stopwords\n",
    "    'RSW':RSW,\n",
    "    # 4. Porter Stemming\n",
    "    'STM':STM,\n",
    "    # 5. LOW->RSW\n",
    "    'LOW_RSW':LOW_RSW,\n",
    "    # 6. LOW->STM\n",
    "    'LOW_STM':LOW_STM,\n",
    "    # 7. RSW->LOW\n",
    "    'RSW_LOW':RSW_LOW,\n",
    "    # 8. RSW->STM\n",
    "    'RSW_STM':RSW_STM,\n",
    "    # 9. STM->LOW\n",
    "    'STM_LOW':STM_LOW,\n",
    "    # 10. STM->RSW\n",
    "    'STM_RSW':STM_RSW,\n",
    "    # 11. LOW->STM->RSW\n",
    "    'LOW_STM_RSW':LOW_STM_RSW,\n",
    "    # 12. LOW->RSW->STM\n",
    "    'LOW_RSW_STM':LOW_RSW_STM,\n",
    "    # 13. STM->LOW->RSW\n",
    "    'STM_LOW_RSW':STM_LOW_RSW,\n",
    "    # 14. STM->RSW->LOW\n",
    "    'STM_RSW_LOW':STM_RSW_LOW,\n",
    "    # 15. RSW->LOW->STM\n",
    "    'RSW_LOW_STM':RSW_LOW_STM,\n",
    "    # 16. RSW->STM->LOW\n",
    "    'RSW_STM_LOW':RSW_STM_LOW\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82705342-1523-4b21-b220-1fdfd58a322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* * * * EVALUATION USING DON AS PREPROCESSING FUNCTION * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:05:51.728878: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-05-02 16:05:53.255188: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest sample is: 146\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b\"   I       t   h   o   u   g   h   t       t   h   i   s       w   a   s       t   h   e       a   n   n   o   u   n   c   e   m   e   n   t       o   f       a       J   a   n   e   t       J   a   c   k   s   o   n       r   e   m   i   x       a   t       f   i   r   s   t       a   n   d       I   '   m       k   i   n   d       o   f       d   i   s   a   p   p   o   i   n   t   e   d   .   \"]], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:05:53.602790: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.36\n",
      "XGBoost Accuracy on Test set -> 0.32\n",
      "RandomForest Accuracy on Test set -> 0.42\n",
      "DecisionTree Accuracy on Test set -> 0.26\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 146\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b\"   i       t   h   o   u   g   h   t       t   h   i   s       w   a   s       t   h   e       a   n   n   o   u   n   c   e   m   e   n   t       o   f       a       j   a   n   e   t       j   a   c   k   s   o   n       r   e   m   i   x       a   t       f   i   r   s   t       a   n   d       i   '   m       k   i   n   d       o   f       d   i   s   a   p   p   o   i   n   t   e   d   .   \"]], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:05:55.603567: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.32\n",
      "XGBoost Accuracy on Test set -> 0.38\n",
      "RandomForest Accuracy on Test set -> 0.38\n",
      "DecisionTree Accuracy on Test set -> 0.32\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 99\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'I h u g h h w h e n n u n c e e n f J n e J c k n r e x f r n I k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.34\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM AS PREPROCESSING FUNCTION * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:06:00.539668: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest sample is: 138\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'i t h o u g h t t h i s w a s t h e a n n o u n c e m e n t o f a j a n e t j a c k s o n r e m i x a t f i r s t a n d i m k i n d o f d i s a p p o i n t e d']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.4\n",
      "XGBoost Accuracy on Test set -> 0.38\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.26\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.24\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.4\n",
      "DecisionTree Accuracy on Test set -> 0.38\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 138\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'i t h o u g h t t h i s w a s t h e a n n o u n c e m e n t o f a j a n e t j a c k s o n r e m i x a t f i r s t a n d i m k i n d o f d i s a p p o i n t e d']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.4\n",
      "XGBoost Accuracy on Test set -> 0.38\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.26\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_LOW AS PREPROCESSING FUNCTION * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:06:12.770255: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest sample is: 99\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   i       h       u       g       h       h       w       h       e       n       n       u       n       c       e       e       n       f       j       n       e       j       c       k       n       r       e       x       f       r       n       i       k       n       f       p       p       n       e   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.44\n",
      "XGBoost Accuracy on Test set -> 0.44\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 99\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'i h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n i k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.44\n",
      "XGBoost Accuracy on Test set -> 0.44\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 138\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   i       t       h       o       u       g       h       t       t       h       i       s       w       a       s       t       h       e       a       n       n       o       u       n       c       e       m       e       n       t       o       f       a       j       a       n       e       t       j       a       c       k       s       o       n       r       e       m       i       x       a       t       f       i       r       s       t       a       n       d       i       m       k       i       n       d       o       f       d       i       s       a       p       p       o       i       n       t       e       d   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.4\n",
      "XGBoost Accuracy on Test set -> 0.36\n",
      "RandomForest Accuracy on Test set -> 0.32\n",
      "DecisionTree Accuracy on Test set -> 0.28\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.26\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.26\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.26\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 16:06:40.204529: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.26\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 93\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   h       u       g       h       h       w       h       e       n       n       u       n       c       e       e       n       f       j       n       e       j       c       k       n       r       e       x       f       r       n       k       n       f       p       p       n       e   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.26\n",
      "XGBoost Accuracy on Test set -> 0.42\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 99\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'i h u g h h w h e n n u n c e e n f j n e j c k n r e x f r n i k n f p p n e']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.44\n",
      "XGBoost Accuracy on Test set -> 0.44\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.36\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 99\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b\"I thought this was the announcement of a Janet Jackson remix at first and I'm kind of disappointed.\"]], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   i       h       u       g       h       h       w       h       e       n       n       u       n       c       e       e       n       f       j       n       e       j       c       k       n       r       e       x       f       r       n       i       k       n       f       p       p       n       e   ']], shape=(1, 1), dtype=string)\n",
      "AdaBoost Accuracy on Test set -> 0.44\n",
      "XGBoost Accuracy on Test set -> 0.44\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for key in prepro_functions_dict_comb:\n",
    "    print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_text, y_test)).shuffle(buffer_size=len(test_df), seed=1, reshuffle_each_iteration=False).batch(1).take(50)\n",
    "\n",
    "    # Preprocess training set to build a dictionary.\n",
    "    vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "    print(\"\\n\\n***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\")\n",
    "    # Print a raw and a preprocessed sample.\n",
    "    for element in train_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "\n",
    "      print(\"Sample considered is: \", author_batch)\n",
    "      print(\"Preprocessed: \", str(prepro_functions_dict_comb[key](author_batch.numpy())))\n",
    "      break\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "    model.add(vectorize_layer)\n",
    "\n",
    "    training_labels=[]\n",
    "    training_samples=[]\n",
    "\n",
    "    max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
    "\n",
    "    for element in train_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "        \n",
    "      text_vect_out = vectorize_layer(author_batch)\n",
    "\n",
    "      training_labels.append(label.numpy())\n",
    "      current_sample=np.zeros(max_features)\n",
    "      for current_token in text_vect_out[0][:].numpy():\n",
    "        current_sample[current_token]+=1\n",
    "      training_samples.append(current_sample)\n",
    "\n",
    "    training_labels=np.array(training_labels)\n",
    "    training_samples=np.array(training_samples)\n",
    "\n",
    "    test_labels=[]\n",
    "    test_samples=[]\n",
    "\n",
    "    for element in test_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "\n",
    "      text_vect_out = vectorize_layer(author_batch)\n",
    "\n",
    "      test_labels.append(label.numpy())\n",
    "      current_sample=np.zeros(max_features)\n",
    "      for current_token in text_vect_out[0][:].numpy():\n",
    "        current_sample[current_token]+=1\n",
    "      test_samples.append(current_sample)\n",
    "\n",
    "    test_labels=np.array(test_labels)\n",
    "    test_samples=np.array(test_samples)\n",
    "\n",
    "    models = {\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=0),\n",
    "        \"XGBoost\": XGBClassifier(random_state=0),\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=0),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=0)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(training_samples, training_labels.ravel())\n",
    "        acc = model.score(test_samples, test_labels)\n",
    "        model_results[key][name].append(acc)\n",
    "        print(f\"{name} Accuracy on Test set ->\", acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a141e6c-b4ef-4fe2-95d2-9652dd0b9801",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1e46a58-d968-429c-90a2-a5553b89ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPRO FUNCTION         | MODEL NAME        | ACCURANCY   \n",
      "------------------------------------------------------------\n",
      "DON                     | AdaBoost          | 0.3600\n",
      "DON                     | XGBoost           | 0.3200\n",
      "DON                     | RandomForest      | 0.4200\n",
      "DON                     | DecisionTree      | 0.2600\n",
      "------------------------------------------------------------\n",
      "LOW                     | AdaBoost          | 0.3200\n",
      "LOW                     | XGBoost           | 0.3800\n",
      "LOW                     | RandomForest      | 0.3800\n",
      "LOW                     | DecisionTree      | 0.3200\n",
      "------------------------------------------------------------\n",
      "RSW                     | AdaBoost          | 0.3400\n",
      "RSW                     | XGBoost           | 0.4200\n",
      "RSW                     | RandomForest      | 0.3600\n",
      "RSW                     | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "STM                     | AdaBoost          | 0.4000\n",
      "STM                     | XGBoost           | 0.3800\n",
      "STM                     | RandomForest      | 0.3400\n",
      "STM                     | DecisionTree      | 0.2600\n",
      "------------------------------------------------------------\n",
      "LOW_RSW                 | AdaBoost          | 0.2400\n",
      "LOW_RSW                 | XGBoost           | 0.4200\n",
      "LOW_RSW                 | RandomForest      | 0.4000\n",
      "LOW_RSW                 | DecisionTree      | 0.3800\n",
      "------------------------------------------------------------\n",
      "LOW_STM                 | AdaBoost          | 0.4000\n",
      "LOW_STM                 | XGBoost           | 0.3800\n",
      "LOW_STM                 | RandomForest      | 0.3400\n",
      "LOW_STM                 | DecisionTree      | 0.2600\n",
      "------------------------------------------------------------\n",
      "RSW_LOW                 | AdaBoost          | 0.4400\n",
      "RSW_LOW                 | XGBoost           | 0.4400\n",
      "RSW_LOW                 | RandomForest      | 0.3600\n",
      "RSW_LOW                 | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "RSW_STM                 | AdaBoost          | 0.4400\n",
      "RSW_STM                 | XGBoost           | 0.4400\n",
      "RSW_STM                 | RandomForest      | 0.3600\n",
      "RSW_STM                 | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "STM_LOW                 | AdaBoost          | 0.4000\n",
      "STM_LOW                 | XGBoost           | 0.3600\n",
      "STM_LOW                 | RandomForest      | 0.3200\n",
      "STM_LOW                 | DecisionTree      | 0.2800\n",
      "------------------------------------------------------------\n",
      "STM_RSW                 | AdaBoost          | 0.2600\n",
      "STM_RSW                 | XGBoost           | 0.4200\n",
      "STM_RSW                 | RandomForest      | 0.3400\n",
      "STM_RSW                 | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "LOW_STM_RSW             | AdaBoost          | 0.2600\n",
      "LOW_STM_RSW             | XGBoost           | 0.4200\n",
      "LOW_STM_RSW             | RandomForest      | 0.3400\n",
      "LOW_STM_RSW             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "LOW_RSW_STM             | AdaBoost          | 0.2600\n",
      "LOW_RSW_STM             | XGBoost           | 0.4200\n",
      "LOW_RSW_STM             | RandomForest      | 0.3400\n",
      "LOW_RSW_STM             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "STM_LOW_RSW             | AdaBoost          | 0.2600\n",
      "STM_LOW_RSW             | XGBoost           | 0.4200\n",
      "STM_LOW_RSW             | RandomForest      | 0.3400\n",
      "STM_LOW_RSW             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "STM_RSW_LOW             | AdaBoost          | 0.2600\n",
      "STM_RSW_LOW             | XGBoost           | 0.4200\n",
      "STM_RSW_LOW             | RandomForest      | 0.3400\n",
      "STM_RSW_LOW             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "RSW_LOW_STM             | AdaBoost          | 0.4400\n",
      "RSW_LOW_STM             | XGBoost           | 0.4400\n",
      "RSW_LOW_STM             | RandomForest      | 0.3600\n",
      "RSW_LOW_STM             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n",
      "RSW_STM_LOW             | AdaBoost          | 0.4400\n",
      "RSW_STM_LOW             | XGBoost           | 0.4400\n",
      "RSW_STM_LOW             | RandomForest      | 0.3600\n",
      "RSW_STM_LOW             | DecisionTree      | 0.3600\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_result_per_model = {}\n",
    "\n",
    "print(\"PREPRO FUNCTION         | MODEL NAME        | ACCURANCY   \")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for prepro_func in prepro_functions_dict_comb:\n",
    "    for model_name, scores in model_results[prepro_func].items():\n",
    "        acc = round(scores[-1], 4)\n",
    "        if model_name not in best_result_per_model or acc > best_result_per_model[model_name][1]:\n",
    "            best_result_per_model[model_name] = (prepro_func, acc)\n",
    "        print(f\"{prepro_func:23} | {model_name:17} | {acc:>6.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8067c5c2-1044-4907-a1ea-27a0947352fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NAME         | BEST PREPRO FUNCTION   | MAX ACCURACY\n",
      "------------------------------------------------------------\n",
      "AdaBoost           | RSW_LOW                | 0.4400\n",
      "XGBoost            | RSW_LOW                | 0.4400\n",
      "RandomForest       | DON                    | 0.4200\n",
      "DecisionTree       | LOW_RSW                | 0.3800\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL NAME         | BEST PREPRO FUNCTION   | MAX ACCURACY\")\n",
    "print(\"-\" * 60)\n",
    "for model_name, (best_func, acc) in best_result_per_model.items():\n",
    "    print(f\"{model_name:18} | {best_func:22} | {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58b43d-63a3-4f25-bad7-ea44b76905ec",
   "metadata": {},
   "source": [
    "# Сохраним результаты в отдельную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1978012c-eacb-4140-9a09-1515dbc68b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepo_path = \"../reports/preprocessing_combinations/tweet_eval_sentiment/\"\n",
    "os.makedirs(prepo_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0846dc3-19b0-4e79-a758-41253ee76651",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for prepo_func, model_scores in model_results.items():\n",
    "    for model_name, acc in model_scores.items():\n",
    "        rows.append({\n",
    "            \"prepo_func\": prepo_func,\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": round(acc[-1], 4)\n",
    "        })\n",
    "\n",
    "df_full = pd.DataFrame(rows)\n",
    "df_full.to_csv(f\"{prepo_path}full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50763841-fe2e-4f17-a375-45ae0663150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = pd.DataFrame([\n",
    "    {\"model\": model, \"best_prepo_func\": func, \"max_accuracy\": acc}\n",
    "    for model, (func, acc) in best_result_per_model.items()\n",
    "])\n",
    "df_best.to_csv(f\"{prepo_path}best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7241ed6b-816c-48df-8da7-621950bb404a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
