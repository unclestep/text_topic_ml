{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "340a1f23-3471-4ebc-9d48-68b45457c362",
   "metadata": {},
   "source": [
    "# **ccdv/arxiv-classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2247fca-a5f8-4752-acf1-146b524a7c7d",
   "metadata": {},
   "source": [
    "Данный код базируется на коде автора: https://github.com/marco-siino/text_preprocessing_impact/blob/main/20N_DS/LR_20N_TextPreProImpact_NB.ipynb. Научная статья разработчика указана в списке использованной литературы и специально дублируется здесь: https://www.sciencedirect.com/science/article/pii/S0306437923001783?ref=cra_js_challenge&fr=RR-1 \n",
    "\n",
    "В курсовой работе уже используются предложенные комбинации методов предобработки для Логистической регрессии, Наивного Байесовского классификатора и Метода опорных векторов. Для отсутствующих моделей: XGBoost, AdaBoost, RandomForest, DecisionTree - эксперимент воспроизводится с изменениями в процессе загрузки датасетов и некоторыми вытекающими правками в коде. Использование оригинального кода позволяет получить недостающие данные и сохранить полноту курсовой работы. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca026184-019b-4e83-bf68-8d095414c821",
   "metadata": {},
   "source": [
    "# Импорт модулей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f7f7965-e80c-44e8-94bf-0f753bc7db2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.random import seed\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "os.environ['TF_CUDNN_DETERMINISTIC']='true'\n",
    "os.environ['TF_DETERMINISTIC_OPS']='true'\n",
    "\n",
    "import textblob.download_corpora as dl\n",
    "dl.download_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309e2f2-e229-4c93-a9cd-8cf52d68624e",
   "metadata": {},
   "source": [
    "# Загружаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a371f96-4904-40c9-821b-57329873a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '../data/raw/ccdv_arxiv-classification/test.csv'\n",
    "train_dir = '../data/raw/ccdv_arxiv-classification/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f012ae72-6ee1-47d4-9929-dfb77b7a10a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(test_dir)\n",
    "train_df = pd.read_csv(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0e2863-5931-4d5d-ac7a-0e4783cc15f7",
   "metadata": {},
   "source": [
    "# Создаем обучающую выборку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a8f8ecb-3bd2-44c5-af20-5ce7d10a1b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_text = train_df[\"text\"].astype(str).values\n",
    "y_train = train_df[\"label\"].values\n",
    "X_test_text = test_df[\"text\"].astype(str).values\n",
    "y_test = test_df[\"label\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6693d80c-ce4b-4e7e-9e17-8d3962ce5ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test_text, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df0f9a3-0dfd-41bf-8738-2707c9012ef4",
   "metadata": {},
   "source": [
    "# Функции предобработки текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6953b286-ce4a-4b88-9794-130ae0ab8f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do-Nothing preprocessing function.\n",
    "def DON(input_data):\n",
    "  tag_open_CDATA_removed = tf.strings.regex_replace(input_data, r'<\\!\\[CDATA\\[', ' ')\n",
    "  tag_closed_CDATA_removed = tf.strings.regex_replace(tag_open_CDATA_removed, r'\\]{1,}>', ' ')\n",
    "  tag_author_lang_en_removed = tf.strings.regex_replace(tag_closed_CDATA_removed,'', ' ')\n",
    "  tag_closed_author_removed = tf.strings.regex_replace(tag_author_lang_en_removed,'', ' ')\n",
    "  tag_open_documents_removed = tf.strings.regex_replace(tag_closed_author_removed, r'\\n(\\t){0,2}', '')\n",
    "  output_data = tf.strings.regex_replace(tag_open_documents_removed, r'\\n(\\t){0,2}', ' ')\n",
    "  return output_data\n",
    "\n",
    "# Lowercasing preprocessing function.\n",
    "def LOW(input_data):\n",
    "  return tf.strings.lower(DON(input_data))\n",
    "\n",
    "# Removing Stop Words function.\n",
    "def RSW(input_data):\n",
    "  output_data = DON(input_data)\n",
    "\n",
    "  try:\n",
    "    input_string=output_data[0]\n",
    "\n",
    "  except:\n",
    "    input_string=output_data\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant(output_string)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  else:\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [word for word in blob if word not in stopwords.words('english')]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant([[output_string]])\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  return output_data\n",
    "\n",
    "# Porter Stemmer preprocessing function.\n",
    "def STM(input_data):\n",
    "  output_data = DON(input_data)\n",
    "  stemmer = PorterStemmer()\n",
    "\n",
    "  try:\n",
    "    input_string=output_data[0]\n",
    "\n",
    "  except:\n",
    "    input_string=output_data\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [stemmer.stem(word) for word in blob]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant(output_string)\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  else:\n",
    "\n",
    "    try:\n",
    "      input_string = input_string.numpy()\n",
    "\n",
    "    except:\n",
    "      return output_data\n",
    "\n",
    "    else:\n",
    "      input_string=(str(input_string))[2:-1]\n",
    "\n",
    "    blob = TextBlob(str(input_string)).words\n",
    "    outputlist = [stemmer.stem(word) for word in blob]\n",
    "    output_string = (' '.join(word for word in outputlist))\n",
    "    output_tensor=tf.constant([[output_string]])\n",
    "\n",
    "    return output_tensor\n",
    "\n",
    "  return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e8555b-3e4f-4f6f-856f-de9f6807cdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SECTION WITH PAIRS OF PREPRO FUNCTIONS. APPLICATION ORDER MATTERS (...IN FOLLOWING SECTIONS TOO).\n",
    "#...5\n",
    "def LOW_RSW(input_data):\n",
    "  return RSW(LOW(input_data))\n",
    "\n",
    "# 6\n",
    "def LOW_STM(input_data):\n",
    "  return STM(LOW(input_data))\n",
    "\n",
    "# 7\n",
    "def RSW_LOW(input_data):\n",
    "  return LOW(RSW(input_data))\n",
    "\n",
    "# 8\n",
    "def RSW_STM(input_data):\n",
    "  return STM(RSW(input_data))\n",
    "\n",
    "# 9\n",
    "def STM_LOW(input_data):\n",
    "  return LOW(STM(input_data))\n",
    "\n",
    "# 10\n",
    "def STM_RSW(input_data):\n",
    "  return RSW(STM(input_data))\n",
    "\n",
    "# 11\n",
    "def LOW_STM_RSW(input_data):\n",
    "  return RSW(STM(LOW(input_data)))\n",
    "\n",
    "# 12\n",
    "def LOW_RSW_STM(input_data):\n",
    "  return STM(RSW(LOW(input_data)))\n",
    "\n",
    "# 13\n",
    "def STM_LOW_RSW(input_data):\n",
    "  return RSW(LOW(STM(input_data)))\n",
    "\n",
    "# 14\n",
    "def STM_RSW_LOW(input_data):\n",
    "  return LOW(RSW(STM(input_data)))\n",
    "\n",
    "# 15\n",
    "def RSW_LOW_STM(input_data):\n",
    "  return STM(LOW(RSW(input_data)))\n",
    "\n",
    "# 16\n",
    "def RSW_STM_LOW(input_data):\n",
    "  return LOW(STM(RSW(input_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a967d7f-c8da-4712-81f6-17b32f7c067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 0\n",
    "def preprocess_and_adapt_ts(preprocessing_function,training_set):\n",
    "  # Set a large sequence length to find the longest sample in the training set.\n",
    "  sequence_length = 15000\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize=preprocessing_function,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=sequence_length,\n",
    "      encoding='ISO-8859-1')\n",
    "\n",
    "  train_text = training_set.map(lambda x, y: x)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "\n",
    "  model = tf.keras.models.Sequential()\n",
    "  model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "  model.add(vectorize_layer)\n",
    "\n",
    "  longest_sample_length=1\n",
    "\n",
    "  for element in training_set:\n",
    "    authorDocument=element[0]\n",
    "    label=element[1]\n",
    "\n",
    "    author_batch = tf.expand_dims(authorDocument, axis=0)\n",
    "    out = model(author_batch)\n",
    "    # Convert token list to numpy array.\n",
    "    token_list = out.numpy()[0]\n",
    "    token_list = np.trim_zeros(token_list,'b')\n",
    "    if longest_sample_length < len(token_list):\n",
    "      longest_sample_length = len(token_list)\n",
    "\n",
    "  print(\"Length of the longest sample is:\", longest_sample_length)\n",
    "\n",
    "  # After tokenization longest_sample_length covers all the document lenghts in our dataset.\n",
    "  sequence_length = longest_sample_length\n",
    "\n",
    "  vectorize_layer = TextVectorization(\n",
    "      standardize=preprocessing_function,\n",
    "      output_mode='int',\n",
    "      output_sequence_length=sequence_length,\n",
    "      encoding='ISO-8859-1')\n",
    "\n",
    "  # Finally adapt the vectorize layer.\n",
    "  train_text = training_set.map(lambda x, y: x)\n",
    "  vectorize_layer.adapt(train_text)\n",
    "  global max_features\n",
    "  max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
    "  return vectorize_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13c78204-8b92-4386-846a-137461880caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = defaultdict(lambda: defaultdict(list))\n",
    "prepro_functions_dict_base = {\n",
    "    'DON':DON,\n",
    "    'LOW':LOW,\n",
    "    'RSW':RSW,\n",
    "    'STM':STM\n",
    "    }\n",
    "\n",
    "# 3 prepro functions = 15 combs...+1 for do_nothing\n",
    "\n",
    "prepro_functions_dict_comb = {\n",
    "    # 1. Do nothing\n",
    "    'DON': DON,\n",
    "    # 2. Lowercasing\n",
    "    'LOW':LOW,\n",
    "    # 3. Removing Stopwords\n",
    "    'RSW':RSW,\n",
    "    # 4. Porter Stemming\n",
    "    'STM':STM,\n",
    "    # 5. LOW->RSW\n",
    "    'LOW_RSW':LOW_RSW,\n",
    "    # 6. LOW->STM\n",
    "    'LOW_STM':LOW_STM,\n",
    "    # 7. RSW->LOW\n",
    "    'RSW_LOW':RSW_LOW,\n",
    "    # 8. RSW->STM\n",
    "    'RSW_STM':RSW_STM,\n",
    "    # 9. STM->LOW\n",
    "    'STM_LOW':STM_LOW,\n",
    "    # 10. STM->RSW\n",
    "    'STM_RSW':STM_RSW,\n",
    "    # 11. LOW->STM->RSW\n",
    "    'LOW_STM_RSW':LOW_STM_RSW,\n",
    "    # 12. LOW->RSW->STM\n",
    "    'LOW_RSW_STM':LOW_RSW_STM,\n",
    "    # 13. STM->LOW->RSW\n",
    "    'STM_LOW_RSW':STM_LOW_RSW,\n",
    "    # 14. STM->RSW->LOW\n",
    "    'STM_RSW_LOW':STM_RSW_LOW,\n",
    "    # 15. RSW->LOW->STM\n",
    "    'RSW_LOW_STM':RSW_LOW_STM,\n",
    "    # 16. RSW->STM->LOW\n",
    "    'RSW_STM_LOW':RSW_STM_LOW\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82705342-1523-4b21-b220-1fdfd58a322b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "* * * * EVALUATION USING DON AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   L   a   b   e   l       D   i   s   t   r   i   b   u   t   i   o   n       L   e   a   r   n   i   n   g       F   o   r   e   s   t   s         W   e   i       S   h   e   n   1   ,   2       ,       K   a   i       Z   h   a   o   1       ,       Y   i   l   u       G   u   o   1       ,       A   l   a   n       Y   u   i   l   l   e   2      K   e   y       L   a   b   o   r   a   t   o   r   y       o   f       S   p   e   c   i   a   l   t   y       F   i   b   e   r       O   p   t   i   c   s       a   n   d       O   p   t   i   c   a   l       A   c   c   e   s   s       N   e   t   w   o   r   k   s   ,      S   h   a   n   g   h   a   i       I   n   s   t   i   t   u   t   e       f   o   r       A   d   v   a   n   c   e   d       C   o   m   m   u   n   i   c   a   t   i   o   n       a   n   d       D   a   t   a       S   c   i   e   n   c   e   ,      S   c   h   o   o   l       o   f       C   o   m   m   u   n   i   c   a   t   i   o   n       a   n   d       I   n   f   o   r   m   a   t   i   o   n       E   n   g   i   n   e   e   r   i   n   g   ,       S   h   a   n   g   h   a   i       U   n   i   v   e   r   s   i   t   y      2      D   e   p   a   r   t   m   e   n   t       o   f       C   o   m   p   u   t   e   r       S   c   i   e   n   c   e   ,       J   o   h   n   s       H   o   p   k   i   n   s       U   n   i   v   e   r   s   i   t   y         a   r   X   i   v   :   1   7   0   2   .   0   6   0   8   6   v   4       [   c   s   .   L   G   ]       1   6       O   c   t       2   0   1   7         1         {   s   h   e   n   w   e   i   1   2   3   1   ,   z   h   a   o   k   1   2   0   6   ,   g   y   l   .   l   u   a   n   0   ,   a   l   a   n   .   l   .   y   u   i   l   l   e   }   @   g   m   a   i   l   .   c   o   m         A   b   s   t   r   a   c   t      L   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   L   D   L   )       i   s       a       g   e   n   e   r   a   l       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k   ,       w   h   i   c   h       a   s   s   i   g   n   s      t   o       a   n       i   n   s   t   a   n   c   e       a       d   i   s   t   r   i   b   u   t   i   o   n       o   v   e   r       a       s   e   t       o   f       l   a   b   e   l   s       r   a   t   h   e   r       t   h   a   n       a       s   i   n   g   l   e       l   a   b   e   l       o   r       m   u   l   t   i   p   l   e      l   a   b   e   l   s   .       C   u   r   r   e   n   t       L   D   L       m   e   t   h   o   d   s       h   a   v   e       e   i   t   h   e   r       r   e   s   t   r   i   c   t   e   d       a   s   s   u   m   p   t   i   o   n   s       o   n       t   h   e       e   x   p   r   e   s   s   i   o   n      f   o   r   m       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       o   r       l   i   m   i   t   a   t   i   o   n   s       i   n       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   o      l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       T   h   i   s       p   a   p   e   r       p   r   e   s   e   n   t   s       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      l   e   a   r   n   i   n   g       f   o   r   e   s   t   s       (   L   D   L   F   s   )       -       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       b   a   s   e   d       o   n      d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   ,       w   h   i   c   h       h   a   v   e       s   e   v   e   r   a   l       a   d   v   a   n   t   a   g   e   s   :       1   )       D   e   c   i   s   i   o   n       t   r   e   e   s      h   a   v   e       t   h   e       p   o   t   e   n   t   i   a   l       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       b   y       a       m   i   x   t   u   r   e      o   f       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   .       2   )       T   h   e       l   e   a   r   n   i   n   g       o   f       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       c   a   n       b   e      c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   .       W   e       d   e   f   i   n   e       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n      f   o   r       a       f   o   r   e   s   t   ,       e   n   a   b   l   i   n   g       a   l   l       t   h   e       t   r   e   e   s       t   o       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   ,       a   n   d       s   h   o   w       t   h   a   t       a   n       u   p   d   a   t   e      f   u   n   c   t   i   o   n       f   o   r       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       w   h   i   c   h       g   u   a   r   a   n   t   e   e   s       a       s   t   r   i   c   t       d   e   c   r   e   a   s   e       o   f       t   h   e       l   o   s   s      f   u   n   c   t   i   o   n   ,       c   a   n       b   e       d   e   r   i   v   e   d       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   .       T   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       t   h   e       p   r   o   p   o   s   e   d      L   D   L   F   s       i   s       v   e   r   i   f   i   e   d       o   n       s   e   v   e   r   a   l       L   D   L       t   a   s   k   s       a   n   d       a       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       s   h   o   w   i   n   g      s   i   g   n   i   f   i   c   a   n   t       i   m   p   r   o   v   e   m   e   n   t   s       t   o       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       L   D   L       m   e   t   h   o   d   s   .         1         I   n   t   r   o   d   u   c   t   i   o   n         L   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   L   D   L   )       [   6   ,       1   1   ]       i   s       a       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k       t   o       d   e   a   l       w   i   t   h       p   r   o   b   l   e   m   s       o   f       l   a   b   e   l      a   m   b   i   g   u   i   t   y   .       U   n   l   i   k   e       s   i   n   g   l   e   -   l   a   b   e   l       l   e   a   r   n   i   n   g       (   S   L   L   )       a   n   d       m   u   l   t   i   -   l   a   b   e   l       l   e   a   r   n   i   n   g       (   M   L   L   )       [   2   6   ]   ,       w   h   i   c   h       a   s   s   u   m   e       a   n      i   n   s   t   a   n   c   e       i   s       a   s   s   i   g   n   e   d       t   o       a       s   i   n   g   l   e       l   a   b   e   l       o   r       m   u   l   t   i   p   l   e       l   a   b   e   l   s   ,       L   D   L       a   i   m   s       a   t       l   e   a   r   n   i   n   g       t   h   e       r   e   l   a   t   i   v   e       i   m   p   o   r   t   a   n   c   e      o   f       e   a   c   h       l   a   b   e   l       i   n   v   o   l   v   e   d       i   n       t   h   e       d   e   s   c   r   i   p   t   i   o   n       o   f       a   n       i   n   s   t   a   n   c   e   ,       i   .   e   .   ,       a       d   i   s   t   r   i   b   u   t   i   o   n       o   v   e   r       t   h   e       s   e   t       o   f       l   a   b   e   l   s   .       S   u   c   h      a       l   e   a   r   n   i   n   g       s   t   r   a   t   e   g   y       i   s       s   u   i   t   a   b   l   e       f   o   r       m   a   n   y       r   e   a   l   -   w   o   r   l   d       p   r   o   b   l   e   m   s   ,       w   h   i   c   h       h   a   v   e       l   a   b   e   l       a   m   b   i   g   u   i   t   y   .       A   n       e   x   a   m   p   l   e      i   s       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       [   8   ]   .       E   v   e   n       h   u   m   a   n   s       c   a   n   n   o   t       p   r   e   d   i   c   t       t   h   e       p   r   e   c   i   s   e       a   g   e       f   r   o   m       a       s   i   n   g   l   e       f   a   c   i   a   l       i   m   a   g   e   .      T   h   e   y       m   a   y       s   a   y       t   h   a   t       t   h   e       p   e   r   s   o   n       i   s       p   r   o   b   a   b   l   y       i   n       o   n   e       a   g   e       g   r   o   u   p       a   n   d       l   e   s   s       l   i   k   e   l   y       t   o       b   e       i   n       a   n   o   t   h   e   r   .       H   e   n   c   e       i   t       i   s      m   o   r   e       n   a   t   u   r   a   l       t   o       a   s   s   i   g   n       a       d   i   s   t   r   i   b   u   t   i   o   n       o   f       a   g   e       l   a   b   e   l   s       t   o       e   a   c   h       f   a   c   i   a   l       i   m   a   g   e       (   F   i   g   .       1   (   a   )   )       i   n   s   t   e   a   d       o   f       u   s   i   n   g       a      s   i   n   g   l   e       a   g   e       l   a   b   e   l   .       A   n   o   t   h   e   r       e   x   a   m   p   l   e       i   s       m   o   v   i   e       r   a   t   i   n   g       p   r   e   d   i   c   t   i   o   n       [   7   ]   .       M   a   n   y       f   a   m   o   u   s       m   o   v   i   e       r   e   v   i   e   w       w   e   b      s   i   t   e   s   ,       s   u   c   h       a   s       N   e   t   f   l   i   x   ,       I   M   D   b       a   n   d       D   o   u   b   a   n   ,       p   r   o   v   i   d   e       a       c   r   o   w   d       o   p   i   n   i   o   n       f   o   r       e   a   c   h       m   o   v   i   e       s   p   e   c   i   f   i   e   d       b   y       t   h   e      d   i   s   t   r   i   b   u   t   i   o   n       o   f       r   a   t   i   n   g   s       c   o   l   l   e   c   t   e   d       f   r   o   m       t   h   e   i   r       u   s   e   r   s       (   F   i   g   .       1   (   b   )   )   .       I   f       a       s   y   s   t   e   m       c   o   u   l   d       p   r   e   c   i   s   e   l   y       p   r   e   d   i   c   t       s   u   c   h       a      r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       e   v   e   r   y       m   o   v   i   e       b   e   f   o   r   e       i   t       i   s       r   e   l   e   a   s   e   d   ,       m   o   v   i   e       p   r   o   d   u   c   e   r   s       c   a   n       r   e   d   u   c   e       t   h   e   i   r       i   n   v   e   s   t   m   e   n   t      r   i   s   k       a   n   d       t   h   e       a   u   d   i   e   n   c   e       c   a   n       b   e   t   t   e   r       c   h   o   o   s   e       w   h   i   c   h       m   o   v   i   e   s       t   o       w   a   t   c   h   .      M   a   n   y       L   D   L       m   e   t   h   o   d   s       a   s   s   u   m   e       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       c   a   n       b   e       r   e   p   r   e   s   e   n   t   e   d       b   y       a       m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       [   2   ]      a   n   d       l   e   a   r   n       i   t       b   y       o   p   t   i   m   i   z   i   n   g       a   n       e   n   e   r   g   y       f   u   n   c   t   i   o   n       b   a   s   e   d       o   n       t   h   e       m   o   d   e   l       [   8   ,       1   1   ,       2   8   ,       6   ]   .       B   u   t   ,       t   h   e       e   x   p   o   n   e   n   t   i   a   l      p   a   r   t       o   f       t   h   i   s       m   o   d   e   l       r   e   s   t   r   i   c   t   s       t   h   e       g   e   n   e   r   a   l   i   t   y       o   f       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r   m   ,       e   .   g   .   ,       i   t       h   a   s       d   i   f   f   i   c   u   l   t   y       i   n       r   e   p   r   e   s   e   n   t   i   n   g      m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   .       S   o   m   e       o   t   h   e   r       L   D   L       m   e   t   h   o   d   s       e   x   t   e   n   d       t   h   e       e   x   i   s   t   i   n   g       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s   ,       e   .   g   ,       b   y      b   o   o   s   t   i   n   g       a   n   d       s   u   p   p   o   r   t       v   e   c   t   o   r       r   e   g   r   e   s   s   i   o   n   ,       t   o       d   e   a   l       w   i   t   h       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       [   7   ,       2   7   ]   ,       w   h   i   c   h       a   v   o   i   d       m   a   k   i   n   g      t   h   i   s       a   s   s   u   m   p   t   i   o   n   ,       b   u   t       h   a   v   e       l   i   m   i   t   a   t   i   o   n   s       i   n       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   h   e   y       d   o       n   o   t       l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s      i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .      3   1   s   t       C   o   n   f   e   r   e   n   c   e       o   n       N   e   u   r   a   l       I   n   f   o   r   m   a   t   i   o   n       P   r   o   c   e   s   s   i   n   g       S   y   s   t   e   m   s       (   N   I   P   S       2   0   1   7   )   ,       L   o   n   g       B   e   a   c   h   ,       C   A   ,       U   S   A   .         \\x0c   F   i   g   u   r   e       1   :       T   h   e       r   e   a   l   -   w   o   r   l   d       d   a   t   a       w   h   i   c   h       a   r   e       s   u   i   t   a   b   l   e       t   o       b   e       m   o   d   e   l   e   d       b   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       (   a   )      E   s   t   i   m   a   t   e   d       f   a   c   i   a   l       a   g   e   s       (   a       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   )   .       (   b   )       R   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       a       m   o   v   i   e      (   a       m   u   l   t   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   )   .         I   n       t   h   i   s       p   a   p   e   r   ,       w   e       p   r   e   s   e   n   t       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s       (   L   D   L   F   s   )       -       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       i   n   s   p   i   r   e   d       b   y       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       [   2   0   ]   .       E   x   t   e   n   d   i   n   g       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n      t   r   e   e   s       t   o       d   e   a   l       w   i   t   h       t   h   e       L   D   L       t   a   s   k       h   a   s       t   w   o       a   d   v   a   n   t   a   g   e   s   .       O   n   e       i   s       t   h   a   t       d   e   c   i   s   i   o   n       t   r   e   e   s       h   a   v   e       t   h   e       p   o   t   e   n   t   i   a   l      t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       b   y       m   i   x   t   u   r   e       o   f       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       w   h   i   c   h      a   v   o   i   d       m   a   k   i   n   g       s   t   r   o   n   g       a   s   s   u   m   p   t   i   o   n       o   n       t   h   e       f   o   r   m       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       T   h   e       s   e   c   o   n   d       i   s       t   h   a   t       t   h   e       s   p   l   i   t      n   o   d   e       p   a   r   a   m   e   t   e   r   s       i   n       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n   ,       w   h   i   c   h       e   n   a   b   l   e   s      a       c   o   m   b   i   n   a   t   i   o   n       o   f       t   r   e   e       l   e   a   r   n   i   n   g       a   n   d       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       W   e       d   e   f   i   n   e       a      d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       t   r   e   e       b   y       t   h   e       K   u   l   l   b   a   c   k   -   L   e   i   b   l   e   r       d   i   v   e   r   g   e   n   c   e       (   K   -   L   )       b   e   t   w   e   e   n       t   h   e      g   r   o   u   n   d       t   r   u   t   h       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       a   n   d       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       p   r   e   d   i   c   t   e   d       b   y       t   h   e       t   r   e   e   .       B   y       f   i   x   i   n   g       s   p   l   i   t       n   o   d   e   s   ,       w   e      s   h   o   w       t   h   a   t       t   h   e       o   p   t   i   m   i   z   a   t   i   o   n       o   f       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       t   o       m   i   n   i   m   i   z   e       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       o   f       t   h   e       t   r   e   e       c   a   n      b   e       a   d   d   r   e   s   s   e   d       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g       [   1   9   ,       2   9   ]   ,       i   n       w   h   i   c   h       t   h   e       o   r   i   g   i   n   a   l       l   o   s   s       f   u   n   c   t   i   o   n       t   o       b   e       m   i   n   i   m   i   z   e   d      g   e   t   s       i   t   e   r   a   t   i   v   e   l   y       r   e   p   l   a   c   e   d       b   y       a       d   e   c   r   e   a   s   i   n   g       s   e   q   u   e   n   c   e       o   f       u   p   p   e   r       b   o   u   n   d   s   .       F   o   l   l   o   w   i   n   g       t   h   i   s       o   p   t   i   m   i   z   a   t   i   o   n      s   t   r   a   t   e   g   y   ,       w   e       d   e   r   i   v   e       a       d   i   s   c   r   e   t   e       i   t   e   r   a   t   i   v   e       f   u   n   c   t   i   o   n       t   o       u   p   d   a   t   e       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   .       T   o       l   e   a   r   n       a       f   o   r   e   s   t   ,      w   e       a   v   e   r   a   g   e       t   h   e       l   o   s   s   e   s       o   f       a   l   l       t   h   e       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s       t   o       b   e       t   h   e       l   o   s   s       f   o   r       t   h   e       f   o   r   e   s   t       a   n   d       a   l   l   o   w       t   h   e       s   p   l   i   t       n   o   d   e   s      f   r   o   m       d   i   f   f   e   r   e   n   t       t   r   e   e   s       t   o       b   e       c   o   n   n   e   c   t   e   d       t   o       t   h   e       s   a   m   e       o   u   t   p   u   t       u   n   i   t       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   .       I   n       t   h   i   s      w   a   y   ,       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       o   f       a   l   l       t   h   e       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s       c   a   n       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   .       O   u   r       L   D   L   F   s       c   a   n       b   e      u   s   e   d       a   s       a       (   s   h   a   l   l   o   w   )       s   t   a   n   d   -   a   l   o   n   e       m   o   d   e   l   ,       a   n   d       c   a   n       a   l   s   o       b   e       i   n   t   e   g   r   a   t   e   d       w   i   t   h       a   n   y       d   e   e   p       n   e   t   w   o   r   k   s   ,       i   .   e   .   ,       t   h   e      f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       c   a   n       b   e       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n       a   n   d       a       d   e   e   p       n   e   t   w   o   r   k   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       F   i   g   .       2      i   l   l   u   s   t   r   a   t   e   s       a       s   k   e   t   c   h       c   h   a   r   t       o   f       o   u   r       L   D   L   F   s   ,       w   h   e   r   e       a       f   o   r   e   s   t       c   o   n   s   i   s   t   s       o   f       t   w   o       t   r   e   e   s       i   s       s   h   o   w   n   .      W   e       v   e   r   i   f   y       t   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       o   u   r       m   o   d   e   l       o   n       s   e   v   e   r   a   l       L   D   L       t   a   s   k   s   ,       s   u   c   h       a   s       c   r   o   w   d       o   p   i   n   i   o   n       p   r   e   d   i   c   t   i   o   n       o   n      m   o   v   i   e   s       a   n   d       d   i   s   e   a   s   e       p   r   e   d   i   c   t   i   o   n       b   a   s   e   d       o   n       h   u   m   a   n       g   e   n   e   s   ,       a   s       w   e   l   l       a   s       o   n   e       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       i   .   e   .   ,      f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n   ,       s   h   o   w   i   n   g       s   i   g   n   i   f   i   c   a   n   t       i   m   p   r   o   v   e   m   e   n   t   s       t   o       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       L   D   L       m   e   t   h   o   d   s   .       T   h   e      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       f   o   r       t   h   e   s   e       t   a   s   k   s       i   n   c   l   u   d   e       b   o   t   h       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   s       (   e   .   g   .   ,       t   h   e       a   g   e       d   i   s   t   r   i   b   u   t   i   o   n       i   n      F   i   g   .       1   (   a   )   )       a   n   d       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s       (   t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       o   n       a       m   o   v   i   e       i   n       F   i   g   .       1   (   b   )   )   .       T   h   e       s   u   p   e   r   i   o   r   i   t   y      o   f       o   u   r       m   o   d   e   l       o   n       b   o   t   h       o   f       t   h   e   m       v   e   r   i   f   i   e   s       i   t   s       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s         F   i   g   u   r   e       2   :       I   l   l   u   s   t   r   a   t   i   o   n       o   f       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   .       T   h   e       t   o   p       c   i   r   c   l   e   s       d   e   n   o   t   e       t   h   e       o   u   t   p   u   t       u   n   i   t   s      o   f       t   h   e       f   u   n   c   t   i   o   n       f       p   a   r   a   m   e   t   e   r   i   z   e   d       b   y       \\xce\\x98   ,       w   h   i   c   h       c   a   n       b   e       a       f   e   a   t   u   r   e       v   e   c   t   o   r       o   r       a       f   u   l   l   y   -   c   o   n   n   e   c   t   e   d       l   a   y   e   r       o   f      a       d   e   e   p       n   e   t   w   o   r   k   .       T   h   e       b   l   u   e       a   n   d       g   r   e   e   n       c   i   r   c   l   e   s       a   r   e       s   p   l   i   t       n   o   d   e   s       a   n   d       l   e   a   f       n   o   d   e   s   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       T   w   o       i   n   d   e   x      f   u   n   c   t   i   o   n       \\xcf\\x95   1       a   n   d       \\xcf\\x95   2       a   r   e       a   s   s   i   g   n   e   d       t   o       t   h   e   s   e       t   w   o       t   r   e   e   s       r   e   s   p   e   c   t   i   v   e   l   y   .       T   h   e       b   l   a   c   k       d   a   s   h       a   r   r   o   w   s       i   n   d   i   c   a   t   e       t   h   e      c   o   r   r   e   s   p   o   n   d   e   n   c   e       b   e   t   w   e   e   n       t   h   e       s   p   l   i   t       n   o   d   e   s       o   f       t   h   e   s   e       t   w   o       t   r   e   e   s       a   n   d       t   h   e       o   u   t   p   u   t       u   n   i   t   s       o   f       f   u   n   c   t   i   o   n       f       .       N   o   t   e      t   h   a   t   ,       o   n   e       o   u   t   p   u   t       u   n   i   t       m   a   y       c   o   r   r   e   s   p   o   n   d       t   o       t   h   e       s   p   l   i   t       n   o   d   e   s       b   e   l   o   n   g   i   n   g       t   o       d   i   f   f   e   r   e   n   t       t   r   e   e   s   .       E   a   c   h       t   r   e   e       h   a   s      i   n   d   e   p   e   n   d   e   n   t       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q       (   d   e   n   o   t   e   d       b   y       h   i   s   t   o   g   r   a   m   s       i   n       l   e   a   f       n   o   d   e   s   )   .       T   h   e       o   u   t   p   u   t       o   f       t   h   e       f   o   r   e   s   t      i   s       a       m   i   x   t   u   r   e       o   f       t   h   e       t   r   e   e       p   r   e   d   i   c   t   i   o   n   s   .       f       (   \\xc2\\xb7   ;       \\xce\\x98   )       a   n   d       q       a   r   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .         2         \\x0c   2         R   e   l   a   t   e   d       W   o   r   k         S   i   n   c   e       o   u   r       L   D   L       a   l   g   o   r   i   t   h   m       i   s       i   n   s   p   i   r   e   d       b   y       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   ,       i   t       i   s       n   e   c   e   s   s   a   r   y       t   o       f   i   r   s   t       r   e   v   i   e   w      s   o   m   e       t   y   p   i   c   a   l       t   e   c   h   n   i   q   u   e   s       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s   .       T   h   e   n   ,       w   e       d   i   s   c   u   s   s       c   u   r   r   e   n   t       L   D   L       m   e   t   h   o   d   s   .      D   e   c   i   s   i   o   n       t   r   e   e   s   .       R   a   n   d   o   m       f   o   r   e   s   t   s       o   r       r   a   n   d   o   m   i   z   e   d       d   e   c   i   s   i   o   n       t   r   e   e   s       [   1   6   ,       1   ,       3   ,       4   ]   ,       a   r   e       a       p   o   p   u   l   a   r       e   n   s   e   m   b   l   e      p   r   e   d   i   c   t   i   v   e       m   o   d   e   l       s   u   i   t   a   b   l   e       f   o   r       m   a   n   y       m   a   c   h   i   n   e       l   e   a   r   n   i   n   g       t   a   s   k   s   .       I   n       t   h   e       p   a   s   t   ,       l   e   a   r   n   i   n   g       o   f       a       d   e   c   i   s   i   o   n       t   r   e   e       w   a   s      b   a   s   e   d       o   n       h   e   u   r   i   s   t   i   c   s       s   u   c   h       a   s       a       g   r   e   e   d   y       a   l   g   o   r   i   t   h   m       w   h   e   r   e       l   o   c   a   l   l   y   -   o   p   t   i   m   a   l       h   a   r   d       d   e   c   i   s   i   o   n   s       a   r   e       m   a   d   e       a   t       e   a   c   h      s   p   l   i   t       n   o   d   e       [   1   ]   ,       a   n   d       t   h   u   s   ,       c   a   n   n   o   t       b   e       i   n   t   e   g   r   a   t   e   d       i   n   t   o       i   n       a       d   e   e   p       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k   ,       i   .   e   .   ,       b   e       c   o   m   b   i   n   e   d      w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .      T   h   e       n   e   w   l   y       p   r   o   p   o   s   e   d       d   e   e   p       n   e   u   r   a   l       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s       (   d   N   D   F   s   )       [   2   0   ]       o   v   e   r   c   o   m   e   s       t   h   i   s       p   r   o   b   l   e   m       b   y       i   n   t   r   o   d   u   c   i   n   g      a       s   o   f   t       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       f   u   n   c   t   i   o   n       a   t       t   h   e       s   p   l   i   t       n   o   d   e   s       a   n   d       a       g   l   o   b   a   l       l   o   s   s       f   u   n   c   t   i   o   n       d   e   f   i   n   e   d       o   n       a       t   r   e   e   .      T   h   i   s       e   n   s   u   r   e   s       t   h   a   t       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n       a   n   d       l   e   a   f       n   o   d   e      p   r   e   d   i   c   t   i   o   n   s       c   a   n       b   e       u   p   d   a   t   e   d       b   y       a       d   i   s   c   r   e   t   e       i   t   e   r   a   t   i   v   e       f   u   n   c   t   i   o   n   .      O   u   r       m   e   t   h   o   d       e   x   t   e   n   d   s       d   N   D   F   s       t   o       a   d   d   r   e   s   s       L   D   L       p   r   o   b   l   e   m   s   ,       b   u   t       t   h   i   s       e   x   t   e   n   s   i   o   n       i   s       n   o   n   -   t   r   i   v   i   a   l   ,       b   e   c   a   u   s   e      l   e   a   r   n   i   n   g       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       i   s       a       c   o   n   s   t   r   a   i   n   e   d       c   o   n   v   e   x       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m   .       A   l   t   h   o   u   g   h       a       s   t   e   p   -   s   i   z   e      f   r   e   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       w   a   s       g   i   v   e   n       i   n       d   N   D   F   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       i   t       w   a   s       o   n   l   y       p   r   o   v   e   d       t   o      c   o   n   v   e   r   g   e       f   o   r       a       c   l   a   s   s   i   f   i   c   a   t   i   o   n       l   o   s   s   .       C   o   n   s   e   q   u   e   n   t   l   y   ,       i   t       w   a   s       u   n   c   l   e   a   r       h   o   w       t   o       o   b   t   a   i   n       s   u   c   h       a   n       u   p   d   a   t   e       f   u   n   c   t   i   o   n      f   o   r       o   t   h   e   r       l   o   s   s   e   s   .       W   e       o   b   s   e   r   v   e   d   ,       h   o   w   e   v   e   r   ,       t   h   a   t       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       i   n       d   N   D   F   s       c   a   n       b   e       d   e   r   i   v   e   d       f   r   o   m      v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   ,       w   h   i   c   h       a   l   l   o   w   s       u   s       t   o       e   x   t   e   n   d       i   t       t   o       o   u   r       L   D   L       l   o   s   s   .       I   n       a   d   d   i   t   i   o   n   ,       t   h   e       s   t   r   a   t   e   g   i   e   s       u   s   e   d       i   n      L   D   L   F   s       a   n   d       d   N   D   F   s       t   o       l   e   a   r   n   i   n   g       t   h   e       e   n   s   e   m   b   l   e       o   f       m   u   l   t   i   p   l   e       t   r   e   e   s       (   f   o   r   e   s   t   s   )       a   r   e       d   i   f   f   e   r   e   n   t   :       1   )       w   e       e   x   p   l   i   c   i   t   l   y      d   e   f   i   n   e       a       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       f   o   r   e   s   t   s   ,       w   h   i   l   e       o   n   l   y       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       s   i   n   g   l   e       t   r   e   e       w   a   s       d   e   f   i   n   e   d       i   n       d   N   D   F   s   ;      2   )       w   e       a   l   l   o   w       t   h   e       s   p   l   i   t       n   o   d   e   s       f   r   o   m       d   i   f   f   e   r   e   n   t       t   r   e   e   s       t   o       b   e       c   o   n   n   e   c   t   e   d       t   o       t   h   e       s   a   m   e       o   u   t   p   u   t       u   n   i   t       o   f       t   h   e       f   e   a   t   u   r   e      l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   ,       w   h   i   l   e       d   N   D   F   s       d   i   d       n   o   t   ;       3   )       a   l   l       t   r   e   e   s       i   n       L   D   L   F   s       c   a   n       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   ,       w   h   i   l   e       t   r   e   e   s       i   n      d   N   D   F   s       w   e   r   e       l   e   a   r   n   e   d       a   l   t   e   r   n   a   t   i   v   e   l   y   .       T   h   e   s   e       c   h   a   n   g   e   s       i   n       t   h   e       e   n   s   e   m   b   l   e       l   e   a   r   n   i   n   g       a   r   e       i   m   p   o   r   t   a   n   t   ,       b   e   c   a   u   s   e       a   s      s   h   o   w   n       i   n       o   u   r       e   x   p   e   r   i   m   e   n   t   s       (   S   e   c   .       4   .   4   )   ,       L   D   L   F   s       c   a   n       g   e   t       b   e   t   t   e   r       r   e   s   u   l   t   s       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       b   u   t       b   y       u   s   i   n   g      t   h   e       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       p   r   o   p   o   s   e   d       i   n       d   N   D   F   s   ,       t   h   e       r   e   s   u   l   t   s       o   f       f   o   r   e   s   t   s       a   r   e       e   v   e   n       w   o   r   s   e       t   h   a   n       t   h   o   s   e       f   o   r       a       s   i   n   g   l   e      t   r   e   e   .      T   o       s   u   m       u   p   ,       w   .   r   .   t   .       d   N   D   F   s       [   2   0   ]   ,       t   h   e       c   o   n   t   r   i   b   u   t   i   o   n   s       o   f       L   D   L   F   s       a   r   e   :       f   i   r   s   t   ,       w   e       e   x   t   e   n   d       f   r   o   m       c   l   a   s   s   i   f   i   c   a   t   i   o   n       [   2   0   ]      t   o       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       b   y       p   r   o   p   o   s   i   n   g       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   o   r       t   h   e       f   o   r   e   s   t   s       a   n   d       d   e   r   i   v   e       t   h   e       g   r   a   d   i   e   n   t       t   o      l   e   a   r   n       s   p   l   i   t   s       n   o   d   e   s       w   .   r   .   t   .       t   h   i   s       l   o   s   s   ;       s   e   c   o   n   d   ,       w   e       d   e   r   i   v   e   d       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       f   o   r       l   e   a   f       n   o   d   e   s       b   y       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g       (   h   a   v   i   n   g       o   b   s   e   r   v   e   d       t   h   a   t       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       i   n       [   2   0   ]       w   a   s       a       s   p   e   c   i   a   l       c   a   s   e       o   f       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g   )   ;       l   a   s   t       b   u   t       n   o   t       t   h   e       l   e   a   s   t   ,       w   e       p   r   o   p   o   s   e       a   b   o   v   e       t   h   r   e   e       s   t   r   a   t   e   g   i   e   s       t   o       l   e   a   r   n   i   n   g       t   h   e       e   n   s   e   m   b   l   e       o   f      m   u   l   t   i   p   l   e       t   r   e   e   s   ,       w   h   i   c   h       a   r   e       d   i   f   f   e   r   e   n   t       f   r   o   m       [   2   0   ]   ,       b   u   t       w   e       s   h   o   w       a   r   e       e   f   f   e   c   t   i   v   e   .      L   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       A       n   u   m   b   e   r       o   f       s   p   e   c   i   a   l   i   z   e   d       a   l   g   o   r   i   t   h   m   s       h   a   v   e       b   e   e   n       p   r   o   p   o   s   e   d       t   o       a   d   d   r   e   s   s       t   h   e      L   D   L       t   a   s   k   ,       a   n   d       h   a   v   e       s   h   o   w   n       t   h   e   i   r       e   f   f   e   c   t   i   v   e   n   e   s   s       i   n       m   a   n   y       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   s   ,       s   u   c   h       a   s       f   a   c   i   a   l      a   g   e       e   s   t   i   m   a   t   i   o   n       [   8   ,       1   1   ,       2   8   ]   ,       e   x   p   r   e   s   s   i   o   n       r   e   c   o   g   n   i   t   i   o   n       [   3   0   ]       a   n   d       h   a   n   d       o   r   i   e   n   t   a   t   i   o   n       e   s   t   i   m   a   t   i   o   n       [   1   0   ]   .      G   e   n   g       e   t       a   l   .       [   8   ]       d   e   f   i   n   e   d       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       a   n       i   n   s   t   a   n   c   e       a   s       a       v   e   c   t   o   r       c   o   n   t   a   i   n   i   n   g       t   h   e       p   r   o   b   a   b   i   l   i   t   i   e   s      o   f       t   h   e       i   n   s   t   a   n   c   e       h   a   v   i   n   g       e   a   c   h       l   a   b   e   l   .       T   h   e   y       a   l   s   o       g   a   v   e       a       s   t   r   a   t   e   g   y       t   o       a   s   s   i   g   n       a       p   r   o   p   e   r       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      t   o       a   n       i   n   s   t   a   n   c   e       w   i   t   h       a       s   i   n   g   l   e       l   a   b   e   l   ,       i   .   e   .   ,       a   s   s   i   g   n   i   n   g       a       G   a   u   s   s   i   a   n       o   r       T   r   i   a   n   g   l   e       d   i   s   t   r   i   b   u   t   i   o   n       w   h   o   s   e       p   e   a   k      i   s       t   h   e       s   i   n   g   l   e       l   a   b   e   l   ,       a   n   d       p   r   o   p   o   s   e   d       a   n       a   l   g   o   r   i   t   h   m       c   a   l   l   e   d       I   I   S   -   L   L   D   ,       w   h   i   c   h       i   s       a   n       i   t   e   r   a   t   i   v   e       o   p   t   i   m   i   z   a   t   i   o   n      p   r   o   c   e   s   s       b   a   s   e   d       o   n       a       t   w   o   -   l   a   y   e   r       e   n   e   r   g   y       b   a   s   e   d       m   o   d   e   l   .       Y   a   n   g       e   t       a   l   .       [   2   8   ]       t   h   e   n       d   e   f   i   n   e   d       a       t   h   r   e   e   -   l   a   y   e   r       e   n   e   r   g   y      b   a   s   e   d       m   o   d   e   l   ,       c   a   l   l   e   d       S   C   E   -   L   D   L   ,       i   n       w   h   i   c   h       t   h   e       a   b   i   l   i   t   y       t   o       p   e   r   f   o   r   m       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       i   s       i   m   p   r   o   v   e   d       b   y      a   d   d   i   n   g       t   h   e       e   x   t   r   a       h   i   d   d   e   n       l   a   y   e   r       a   n   d       s   p   a   r   s   i   t   y       c   o   n   s   t   r   a   i   n   t   s       a   r   e       a   l   s   o       i   n   c   o   r   p   o   r   a   t   e   d       t   o       a   m   e   l   i   o   r   a   t   e       t   h   e       m   o   d   e   l   .      G   e   n   g       [   6   ]       d   e   v   e   l   o   p   e   d       a   n       a   c   c   e   l   e   r   a   t   e   d       v   e   r   s   i   o   n       o   f       I   I   S   -   L   L   D   ,       c   a   l   l   e   d       B   F   G   S   -   L   D   L   ,       b   y       u   s   i   n   g       q   u   a   s   i   -   N   e   w   t   o   n      o   p   t   i   m   i   z   a   t   i   o   n   .       A   l   l       t   h   e       a   b   o   v   e       L   D   L       m   e   t   h   o   d   s       a   s   s   u   m   e       t   h   a   t       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       c   a   n       b   e       r   e   p   r   e   s   e   n   t   e   d       b   y       a      m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       [   2   ]   ,       b   u   t       t   h   e       e   x   p   o   n   e   n   t   i   a   l       p   a   r   t       o   f       t   h   i   s       m   o   d   e   l       r   e   s   t   r   i   c   t   s       t   h   e       g   e   n   e   r   a   l   i   t   y       o   f       t   h   e      d   i   s   t   r   i   b   u   t   i   o   n       f   o   r   m   .      A   n   o   t   h   e   r       w   a   y       t   o       a   d   d   r   e   s   s       t   h   e       L   D   L       t   a   s   k   ,       i   s       t   o       e   x   t   e   n   d       e   x   i   s   t   i   n   g       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s       t   o       d   e   a   l       w   i   t   h       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n   s   .       G   e   n   g       a   n   d       H   o   u       [   7   ]       p   r   o   p   o   s   e   d       L   D   S   V   R   ,       a       L   D   L       m   e   t   h   o   d       b   y       e   x   t   e   n   d   i   n   g       s   u   p   p   o   r   t       v   e   c   t   o   r      r   e   g   r   e   s   s   o   r   ,       w   h   i   c   h       f   i   t       a       s   i   g   m   o   i   d       f   u   n   c   t   i   o   n       t   o       e   a   c   h       c   o   m   p   o   n   e   n   t       o   f       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       s   i   m   u   l   t   a   n   e   o   u   s   l   y       b   y       a      s   u   p   p   o   r   t       v   e   c   t   o   r       m   a   c   h   i   n   e   .       X   i   n   g       e   t       a   l   .       [   2   7   ]       t   h   e   n       e   x   t   e   n   d   e   d       b   o   o   s   t   i   n   g       t   o       a   d   d   r   e   s   s       t   h   e       L   D   L       t   a   s   k       b   y       a   d   d   i   t   i   v   e      w   e   i   g   h   t   e   d       r   e   g   r   e   s   s   o   r   s   .       T   h   e   y       s   h   o   w   e   d       t   h   a   t       u   s   i   n   g       t   h   e       v   e   c   t   o   r       t   r   e   e       m   o   d   e   l       a   s       t   h   e       w   e   a   k       r   e   g   r   e   s   s   o   r       c   a   n       l   e   a   d       t   o      b   e   t   t   e   r       p   e   r   f   o   r   m   a   n   c   e       a   n   d       n   a   m   e   d       t   h   i   s       m   e   t   h   o   d       A   O   S   O   -   L   D   L   L   o   g   i   t   B   o   o   s   t   .       A   s       t   h   e       l   e   a   r   n   i   n   g       o   f       t   h   i   s       t   r   e   e       m   o   d   e   l      i   s       b   a   s   e   d       o   n       l   o   c   a   l   l   y   -   o   p   t   i   m   a   l       h   a   r   d       d   a   t   a       p   a   r   t   i   t   i   o   n       f   u   n   c   t   i   o   n   s       a   t       e   a   c   h       s   p   l   i   t       n   o   d   e   ,       A   O   S   O   -   L   D   L   L   o   g   i   t   B   o   o   s   t       i   s      u   n   a   b   l   e       t   o       b   e       c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   .       E   x   t   e   n   d   i   n   g       c   u   r   r   e   n   t       d   e   e   p       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s       t   o      3         \\x0c   a   d   d   r   e   s   s       t   h   e       L   D   L       t   a   s   k       i   s       a   n       i   n   t   e   r   e   s   t   i   n   g       t   o   p   i   c   .       B   u   t   ,       t   h   e       e   x   i   s   t   i   n   g       s   u   c   h       a       m   e   t   h   o   d   ,       c   a   l   l   e   d       D   L   D   L       [   5   ]   ,       s   t   i   l   l      f   o   c   u   s   e   s       o   n       m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       b   a   s   e   d       L   D   L   .      O   u   r       m   e   t   h   o   d   ,       L   D   L   F   s   ,       e   x   t   e   n   d   s       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       t   o       a   d   d   r   e   s   s       L   D   L       t   a   s   k   s   ,       i   n       w   h   i   c   h       t   h   e       p   r   e   d   i   c   t   e   d      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       a       s   a   m   p   l   e       c   a   n       b   e       e   x   p   r   e   s   s   e   d       b   y       a       l   i   n   e   a   r       c   o   m   b   i   n   a   t   i   o   n       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s      o   f       t   h   e       t   r   a   i   n   i   n   g       d   a   t   a   ,       a   n   d       t   h   u   s       h   a   v   e       n   o       r   e   s   t   r   i   c   t   i   o   n   s       o   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       (   e   .   g   .   ,       n   o       r   e   q   u   i   r   e   m   e   n   t       o   f       t   h   e      m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l   )   .       I   n       a   d   d   i   t   i   o   n   ,       t   h   a   n   k   s       t   o       t   h   e       i   n   t   r   o   d   u   c   t   i   o   n       o   f       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       f   u   n   c   t   i   o   n   s   ,      L   D   L   F   s       c   a   n       b   e       c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   o       l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s       i   n       a   n       e   n   d   -   t   o   -   e   n   d      m   a   n   n   e   r   .         3         L   a   b   e   l       D   i   s   t   r   i   b   u   t   i   o   n       L   e   a   r   n   i   n   g       F   o   r   e   s   t   s         A       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s   .       W   e       f   i   r   s   t       i   n   t   r   o   d   u   c   e       h   o   w       t   o       l   e   a   r   n       a       s   i   n   g   l   e       d   e   c   i   s   i   o   n       t   r   e   e       b   y      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   ,       t   h   e   n       d   e   s   c   r   i   b   e       t   h   e       l   e   a   r   n   i   n   g       o   f       a       f   o   r   e   s   t   .      3   .   1         P   r   o   b   l   e   m       F   o   r   m   u   l   a   t   i   o   n         L   e   t       X       =       R   m       d   e   n   o   t   e       t   h   e       i   n   p   u   t       s   p   a   c   e       a   n   d       Y       =       {   y   1       ,       y   2       ,       .       .       .       ,       y   C       }       d   e   n   o   t   e       t   h   e       c   o   m   p   l   e   t   e       s   e   t       o   f       l   a   b   e   l   s   ,      w   h   e   r   e       C       i   s       t   h   e       n   u   m   b   e   r       o   f       p   o   s   s   i   b   l   e       l   a   b   e   l       v   a   l   u   e   s   .       W   e       c   o   n   s   i   d   e   r       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   L   D   L   )      p   r   o   b   l   e   m   ,       w   h   e   r   e       f   o   r       e   a   c   h       i   n   p   u   t       s   a   m   p   l   e       x       \\xe2\\x88\\x88       X       ,       t   h   e   r   e       i   s       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       d       =       (   d   x   y   1       ,       d   y   x   2       ,       .       .       .       ,       d   y   x   C       )   >       \\xe2\\x88\\x88      R   C       .       H   e   r   e       d   y   x   c       e   x   p   r   e   s   s   e   s       t   h   e       p   r   o   b   a   b   i   l   i   t   y       o   f       t   h   e       s   a   m   p   l   e       x       h   a   v   i   n   g       t   h   e       c   -   t   h       l   a   b   e   l       y   c       a   n   d       t   h   u   s       h   a   s       t   h   e      P   C      c   o   n   s   t   r   a   i   n   t   s       t   h   a   t       d   y   x   c       \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d       c   =   1       d   y   x   c       =       1   .       T   h   e       g   o   a   l       o   f       t   h   e       L   D   L       p   r   o   b   l   e   m       i   s       t   o       l   e   a   r   n       a       m   a   p   p   i   n   g      f   u   n   c   t   i   o   n       g       :       x       \\xe2\\x86\\x92       d       b   e   t   w   e   e   n       a   n       i   n   p   u   t       s   a   m   p   l   e       x       a   n   d       i   t   s       c   o   r   r   e   s   p   o   n   d   i   n   g       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       d   .      H   e   r   e   ,       w   e       w   a   n   t       t   o       l   e   a   r   n       t   h   e       m   a   p   p   i   n   g       f   u   n   c   t   i   o   n       g   (   x   )       b   y       a       d   e   c   i   s   i   o   n       t   r   e   e       b   a   s   e   d       m   o   d   e   l       T       .       A       d   e   c   i   s   i   o   n      t   r   e   e       c   o   n   s   i   s   t   s       o   f       a       s   e   t       o   f       s   p   l   i   t       n   o   d   e   s       N       a   n   d       a       s   e   t       o   f       l   e   a   f       n   o   d   e   s       L   .       E   a   c   h       s   p   l   i   t       n   o   d   e       n       \\xe2\\x88\\x88       N       d   e   f   i   n   e   s      a       s   p   l   i   t       f   u   n   c   t   i   o   n       s   n       (   \\xc2\\xb7   ;       \\xce\\x98   )       :       X       \\xe2\\x86\\x92       [   0   ,       1   ]       p   a   r   a   m   e   t   e   r   i   z   e   d       b   y       \\xce\\x98       t   o       d   e   t   e   r   m   i   n   e       w   h   e   t   h   e   r       a       s   a   m   p   l   e       i   s       s   e   n   t      t   o       t   h   e       l   e   f   t       o   r       r   i   g   h   t       s   u   b   t   r   e   e   .       E   a   c   h       l   e   a   f       n   o   d   e       `       \\xe2\\x88\\x88       L       h   o   l   d   s       a       d   i   s   t   r   i   b   u   t   i   o   n       q   `       =       (   q   `   1       ,       q   `   2       ,       .       .       .       ,       q   `   C       )   >      P   C      o   v   e   r       Y   ,       i   .   e   ,       q   `   c       \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d       c   =   1       q   `   c       =       1   .       T   o       b   u   i   l   d       a       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   ,       f   o   l   l   o   w   i   n   g       [   2   0   ]   ,      w   e       u   s   e       a       p   r   o   b   a   b   i   l   i   s   t   i   c       s   p   l   i   t       f   u   n   c   t   i   o   n       s   n       (   x   ;       \\xce\\x98   )       =       \\xcf\\x83   (   f   \\xcf\\x95   (   n   )       (   x   ;       \\xce\\x98   )   )   ,       w   h   e   r   e       \\xcf\\x83   (   \\xc2\\xb7   )       i   s       a       s   i   g   m   o   i   d       f   u   n   c   t   i   o   n   ,      \\xcf\\x95   (   \\xc2\\xb7   )       i   s       a   n       i   n   d   e   x       f   u   n   c   t   i   o   n       t   o       b   r   i   n   g       t   h   e       \\xcf\\x95   (   n   )   -   t   h       o   u   t   p   u   t       o   f       f   u   n   c   t   i   o   n       f       (   x   ;       \\xce\\x98   )       i   n       c   o   r   r   e   s   p   o   n   d   e   n   c   e       w   i   t   h      s   p   l   i   t       n   o   d   e       n   ,       a   n   d       f       :       x       \\xe2\\x86\\x92       R   M       i   s       a       r   e   a   l   -   v   a   l   u   e   d       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       d   e   p   e   n   d   i   n   g       o   n       t   h   e       s   a   m   p   l   e       x      a   n   d       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98   ,       a   n   d       c   a   n       t   a   k   e       a   n   y       f   o   r   m   .       F   o   r       a       s   i   m   p   l   e       f   o   r   m   ,       i   t       c   a   n       b   e       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n      o   f       x   ,       w   h   e   r   e       \\xce\\x98       i   s       t   h   e       t   r   a   n   s   f   o   r   m   a   t   i   o   n       m   a   t   r   i   x   ;       F   o   r       a       c   o   m   p   l   e   x       f   o   r   m   ,       i   t       c   a   n       b   e       a       d   e   e   p       n   e   t   w   o   r   k       t   o      p   e   r   f   o   r   m       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   ,       t   h   e   n       \\xce\\x98       i   s       t   h   e       n   e   t   w   o   r   k       p   a   r   a   m   e   t   e   r   .       T   h   e      c   o   r   r   e   s   p   o   n   d   e   n   c   e       b   e   t   w   e   e   n       t   h   e       s   p   l   i   t       n   o   d   e   s       a   n   d       t   h   e       o   u   t   p   u   t       u   n   i   t   s       o   f       f   u   n   c   t   i   o   n       f       ,       i   n   d   i   c   a   t   e   d       b   y       \\xcf\\x95   (   \\xc2\\xb7   )       t   h   a   t       i   s      r   a   n   d   o   m   l   y       g   e   n   e   r   a   t   e   d       b   e   f   o   r   e       t   r   e   e       l   e   a   r   n   i   n   g   ,       i   .   e   .   ,       w   h   i   c   h       o   u   t   p   u   t       u   n   i   t   s       f   r   o   m       \\xe2\\x80\\x9c   f       \\xe2\\x80\\x9d       a   r   e       u   s   e   d       f   o   r       c   o   n   s   t   r   u   c   t   i   n   g       a      t   r   e   e       i   s       d   e   t   e   r   m   i   n   e   d       r   a   n   d   o   m   l   y   .       A   n       e   x   a   m   p   l   e       t   o       d   e   m   o   n   s   t   r   a   t   e       \\xcf\\x95   (   \\xc2\\xb7   )       i   s       s   h   o   w   n       i   n       F   i   g   .       2   .       T   h   e   n   ,       t   h   e       p   r   o   b   a   b   i   l   i   t   y      o   f       t   h   e       s   a   m   p   l   e       x       f   a   l   l   i   n   g       i   n   t   o       l   e   a   f       n   o   d   e       `       i   s       g   i   v   e   n       b   y      Y      l      r      p   (   `   |   x   ;       \\xce\\x98   )       =      s   n       (   x   ;       \\xce\\x98   )   1   (   `   \\xe2\\x88\\x88   L   n       )       (   1       \\xe2\\x88\\x92       s   n       (   x   ;       \\xce\\x98   )   )   1   (   `   \\xe2\\x88\\x88   L   n       )       ,      (   1   )      n   \\xe2\\x88\\x88   N         w   h   e   r   e       1   (   \\xc2\\xb7   )       i   s       a   n       i   n   d   i   c   a   t   o   r       f   u   n   c   t   i   o   n       a   n   d       L   l   n       a   n   d       L   r   n       d   e   n   o   t   e       t   h   e       s   e   t   s       o   f       l   e   a   f       n   o   d   e   s       h   e   l   d       b   y       t   h   e       l   e   f   t       a   n   d      r   i   g   h   t       s   u   b   t   r   e   e   s       o   f       n   o   d   e       n   ,       T   n   l       a   n   d       T   n   r       ,       r   e   s   p   e   c   t   i   v   e   l   y   .       T   h   e       o   u   t   p   u   t       o   f       t   h   e       t   r   e   e       T       w   .   r   .   t   .       x   ,       i   .   e   .   ,       t   h   e       m   a   p   p   i   n   g      f   u   n   c   t   i   o   n       g   ,       i   s       d   e   f   i   n   e   d       b   y      X      g   (   x   ;       \\xce\\x98   ,       T       )       =      p   (   `   |   x   ;       \\xce\\x98   )   q   `       .      (   2   )      `   \\xe2\\x88\\x88   L         3   .   2         T   r   e   e       O   p   t   i   m   i   z   a   t   i   o   n         G   i   v   e   n       a       t   r   a   i   n   i   n   g       s   e   t       S       =       {   (   x   i       ,       d   i       )   }   N      i   =   1       ,       o   u   r       g   o   a   l       i   s       t   o       l   e   a   r   n       a       d   e   c   i   s   i   o   n       t   r   e   e       T       d   e   s   c   r   i   b   e   d       i   n       S   e   c   .       3   .   1      w   h   i   c   h       c   a   n       o   u   t   p   u   t       a       d   i   s   t   r   i   b   u   t   i   o   n       g   (   x   i       ;       \\xce\\x98   ,       T       )       s   i   m   i   l   a   r       t   o       d   i       f   o   r       e   a   c   h       s   a   m   p   l   e       x   i       .       T   o       t   h   i   s       e   n   d   ,       a      s   t   r   a   i   g   h   t   f   o   r   w   a   r   d       w   a   y       i   s       t   o       m   i   n   i   m   i   z   e       t   h   e       K   u   l   l   b   a   c   k   -   L   e   i   b   l   e   r       (   K   -   L   )       d   i   v   e   r   g   e   n   c   e       b   e   t   w   e   e   n       e   a   c   h       g   (   x   i       ;       \\xce\\x98   ,       T       )      a   n   d       d   i       ,       o   r       e   q   u   i   v   a   l   e   n   t   l   y       t   o       m   i   n   i   m   i   z   e       t   h   e       f   o   l   l   o   w   i   n   g       c   r   o   s   s   -   e   n   t   r   o   p   y       l   o   s   s   :      R   (   q   ,       \\xce\\x98   ;       S   )       =       \\xe2\\x88\\x92         N       C      N       C      \\x10   X      \\x11      1       X       X       y   c      1       X       X       y   c      d   x   i       l   o   g   (   g   c       (   x   i       ;       \\xce\\x98   ,       T       )   )       =       \\xe2\\x88\\x92      d   x   i       l   o   g      p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       ,       (   3   )      N       i   =   1       c   =   1      N       i   =   1       c   =   1      `   \\xe2\\x88\\x88   L         4         \\x0c   w   h   e   r   e       q       d   e   n   o   t   e       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       a   l   l       t   h   e       l   e   a   f       n   o   d   e   s       L       a   n   d       g   c       (   x   i       ;       \\xce\\x98   ,       T       )       i   s       t   h   e       c   -   t   h       o   u   t   p   u   t       u   n   i   t      o   f       g   (   x   i       ;       \\xce\\x98   ,       T       )   .       L   e   a   r   n   i   n   g       t   h   e       t   r   e   e       T       r   e   q   u   i   r   e   s       t   h   e       e   s   t   i   m   a   t   i   o   n       o   f       t   w   o       p   a   r   a   m   e   t   e   r   s   :       1   )       t   h   e       s   p   l   i   t       n   o   d   e      p   a   r   a   m   e   t   e   r       \\xce\\x98       a   n   d       2   )       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       q       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s   .       T   h   e       b   e   s   t       p   a   r   a   m   e   t   e   r   s       (   \\xce\\x98   \\xe2\\x88\\x97       ,       q   \\xe2\\x88\\x97       )       a   r   e      d   e   t   e   r   m   i   n   e   d       b   y      (   \\xce\\x98   \\xe2\\x88\\x97       ,       q   \\xe2\\x88\\x97       )       =       a   r   g       m   i   n       R   (   q   ,       \\xce\\x98   ;       S   )   .      (   4   )      \\xce\\x98   ,   q         T   o       s   o   l   v   e       E   q   n   .       4   ,       w   e       c   o   n   s   i   d   e   r       a   n       a   l   t   e   r   n   a   t   i   n   g       o   p   t   i   m   i   z   a   t   i   o   n       s   t   r   a   t   e   g   y   :       F   i   r   s   t   ,       w   e       f   i   x       q       a   n   d       o   p   t   i   m   i   z   e      \\xce\\x98   ;       T   h   e   n   ,       w   e       f   i   x       \\xce\\x98       a   n   d       o   p   t   i   m   i   z   e       q   .       T   h   e   s   e       t   w   o       l   e   a   r   n   i   n   g       s   t   e   p   s       a   r   e       a   l   t   e   r   n   a   t   i   v   e   l   y       p   e   r   f   o   r   m   e   d   ,       u   n   t   i   l      c   o   n   v   e   r   g   e   n   c   e       o   r       a       m   a   x   i   m   u   m       n   u   m   b   e   r       o   f       i   t   e   r   a   t   i   o   n   s       i   s       r   e   a   c   h   e   d       (   d   e   f   i   n   e   d       i   n       t   h   e       e   x   p   e   r   i   m   e   n   t   s   )   .      3   .   2   .   1         L   e   a   r   n   i   n   g       S   p   l   i   t       N   o   d   e   s         I   n       t   h   i   s       s   e   c   t   i   o   n   ,       w   e       d   e   s   c   r   i   b   e       h   o   w       t   o       l   e   a   r   n       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98       f   o   r       s   p   l   i   t       n   o   d   e   s   ,       w   h   e   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d      b   y       t   h   e       l   e   a   f       n   o   d   e   s       q       a   r   e       f   i   x   e   d   .       W   e       c   o   m   p   u   t   e       t   h   e       g   r   a   d   i   e   n   t       o   f       t   h   e       l   o   s   s       R   (   q   ,       \\xce\\x98   ;       S   )       w   .   r   .   t   .       \\xce\\x98       b   y       t   h   e       c   h   a   i   n      r   u   l   e   :      N      \\xe2\\x88\\x82   R   (   q   ,       \\xce\\x98   ;       S   )       X       X       \\xe2\\x88\\x82   R   (   q   ,       \\xce\\x98   ;       S   )       \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      =      ,      (   5   )      \\xe2\\x88\\x82   \\xce\\x98      \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x82   \\xce\\x98      i   =   1      n   \\xe2\\x88\\x88   N         w   h   e   r   e       o   n   l   y       t   h   e       f   i   r   s   t       t   e   r   m       d   e   p   e   n   d   s       o   n       t   h   e       t   r   e   e       a   n   d       t   h   e       s   e   c   o   n   d       t   e   r   m       d   e   p   e   n   d   s       o   n       t   h   e       s   p   e   c   i   f   i   c       t   y   p   e       o   f       t   h   e      f   u   n   c   t   i   o   n       f   \\xcf\\x95   (   n   )       .       T   h   e       f   i   r   s   t       t   e   r   m       i   s       g   i   v   e   n       b   y      C      \\x01       g   c       (   x   i       ;       \\xce\\x98   ,       T   n   l       )       \\x11      g   c       (   x   i       ;       \\xce\\x98   ,       T   n   r       )      1       X       y   c       \\x10      \\xe2\\x88\\x82   R   (   q   ,       \\xce\\x98   ;       S   )      =      d   x   i       s   n       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x92       1       \\xe2\\x88\\x92       s   n       (   x   i       ;       \\xce\\x98   )      ,       (   6   )      \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      N       c   =   1      g   c       (   x   i       ;       \\xce\\x98   ,       T       )      g   c       (   x   i       ;       \\xce\\x98   ,       T       )      P      P      w   h   e   r   e       g   c       (   x   i       ;       \\xce\\x98   ,       T   n   l       )       =       `   \\xe2\\x88\\x88   L   l   n       p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       a   n   d       g       c       (   x   i       ;       \\xce\\x98   ,       T   n   r       )       =       `   \\xe2\\x88\\x88   L   r   n       p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       .       N   o   t   e       t   h   a   t   ,      l   e   t       T   n       b   e       t   h   e       t   r   e   e       r   o   o   t   e   d       a   t       t   h   e       n   o   d   e       n   ,       t   h   e   n       w   e       h   a   v   e       g   c       (   x   i       ;       \\xce\\x98   ,       T   n       )       =       g   c       (   x   i       ;       \\xce\\x98   ,       T   n   l       )       +       g   c       (   x   i       ;       \\xce\\x98   ,       T   n   r       )   .      T   h   i   s       m   e   a   n   s       t   h   e       g   r   a   d   i   e   n   t       c   o   m   p   u   t   a   t   i   o   n       i   n       E   q   n   .       6       c   a   n       b   e       s   t   a   r   t   e   d       a   t       t   h   e       l   e   a   f       n   o   d   e   s       a   n   d       c   a   r   r   i   e   d       o   u   t       i   n       a      b   o   t   t   o   m       u   p       m   a   n   n   e   r   .       T   h   u   s   ,       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       s   t   a   n   d   a   r   d       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n   .         3   .   2   .   2         L   e   a   r   n   i   n   g       L   e   a   f       N   o   d   e   s         N   o   w   ,       f   i   x   i   n   g       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98   ,       w   e       s   h   o   w       h   o   w       t   o       l   e   a   r   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s       q   ,       w   h   i   c   h      i   s       a       c   o   n   s   t   r   a   i   n   e   d       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m   :      m   i   n       R   (   q   ,       \\xce\\x98   ;       S   )   ,       s   .   t   .   ,       \\xe2\\x88\\x80   `   ,      q         C      X         q   `   c       =       1   .         (   7   )         c   =   1         H   e   r   e   ,       w   e       p   r   o   p   o   s   e       t   o       a   d   d   r   e   s   s       t   h   i   s       c   o   n   s   t   r   a   i   n   e   d       c   o   n   v   e   x       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g       [   1   9   ,       2   9   ]   ,       w   h   i   c   h       l   e   a   d   s       t   o       a       s   t   e   p   -   s   i   z   e       f   r   e   e       a   n   d       f   a   s   t   -   c   o   n   v   e   r   g   e   d       u   p   d   a   t   e       r   u   l   e       f   o   r       q   .       I   n       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g   ,       a   n       o   r   i   g   i   n   a   l       o   b   j   e   c   t   i   v   e       f   u   n   c   t   i   o   n       t   o       b   e       m   i   n   i   m   i   z   e   d       g   e   t   s       r   e   p   l   a   c   e   d       b   y       i   t   s       b   o   u   n   d       i   n       a   n       i   t   e   r   a   t   i   v   e      m   a   n   n   e   r   .       A       u   p   p   e   r       b   o   u   n   d       f   o   r       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       R   (   q   ,       \\xce\\x98   ;       S   )       c   a   n       b   e       o   b   t   a   i   n   e   d       b   y       J   e   n   s   e   n   \\xe2\\x80\\x99   s       i   n   e   q   u   a   l   i   t   y   :      R   (   q   ,       \\xce\\x98   ;       S   )       =       \\xe2\\x88\\x92         N       C      \\x10   X      \\x11      1       X       X       y   c      d   x   i       l   o   g      p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c      N       i   =   1       c   =   1      `   \\xe2\\x88\\x88   L         \\xe2\\x89\\xa4   \\xe2\\x88\\x92      w   h   e   r   e       \\xce\\xbe   `       (   q   `   c       ,       x   i       )       =         1      N         N       X      C      X      i   =   1       c   =   1         p   (   `   |   x   i       ;   \\xce\\x98   )   q   `   c      g   c       (   x   i       ;   \\xce\\x98   ,   T       )         \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       =       \\xe2\\x88\\x92         d   y   x   c   i         X         \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )       l   o   g         `   \\xe2\\x88\\x88   L         \\x10       p   (   `   |   x       ;       \\xce\\x98   )   q       \\x11      i      `   c      ,      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )         (   8   )         .       W   e       d   e   f   i   n   e         N       C      \\x10       p   (   `   |   x       ;       \\xce\\x98   )   q       \\x11      1       X       X       y   c       X      i      `   c      d   x   i      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )       l   o   g      .      N       i   =   1       c   =   1      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )         (   9   )         `   \\xe2\\x88\\x88   L         T   h   e   n       \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       i   s       a   n       u   p   p   e   r       b   o   u   n   d       f   o   r       R   (   q   ,       \\xce\\x98   ;       S   )   ,       w   h   i   c   h       h   a   s       t   h   e       p   r   o   p   e   r   t   y       t   h   a   t       f   o   r       a   n   y       q       a   n   d       q   \\xcc\\x84   ,      \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       \\xe2\\x89\\xa5       R   (   q   ,       \\xce\\x98   ;       S   )   ,       a   n   d       \\xcf\\x86   (   q   ,       q   )       =       R   (   q   ,       \\xce\\x98   ;       S   )   .       A   s   s   u   m   e       t   h   a   t       w   e       a   r   e       a   t       a       p   o   i   n   t       q   (   t   )       c   o   r   r   e   s   p   o   n   d   i   n   g      t   o       t   h   e       t   -   t   h       i   t   e   r   a   t   i   o   n   ,       t   h   e   n       \\xcf\\x86   (   q   ,       q   (   t   )       )       i   s       a   n       u   p   p   e   r       b   o   u   n   d       f   o   r       R   (   q   ,       \\xce\\x98   ;       S   )   .       I   n       t   h   e       n   e   x   t       i   t   e   r   a   t   i   o   n   ,       q   (   t   +   1   )      i   s       c   h   o   s   e   n       s   u   c   h       t   h   a   t       \\xcf\\x86   (   q   (   t   +   1   )       ,       q   )       \\xe2\\x89\\xa4       R   (   q   (   t   )       ,       \\xce\\x98   ;       S   )   ,       w   h   i   c   h       i   m   p   l   i   e   s       R   (   q   (   t   +   1   )       ,       \\xce\\x98   ;       S   )       \\xe2\\x89\\xa4       R   (   q   (   t   )       ,       \\xce\\x98   ;       S   )   .      5         \\x0c   C   o   n   s   e   q   u   e   n   t   l   y   ,       w   e       c   a   n       m   i   n   i   m   i   z   e       \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       i   n   s   t   e   a   d       o   f       R   (   q   ,       \\xce\\x98   ;       S   )       a   f   t   e   r       e   n   s   u   r   i   n   g       t   h   a   t       R   (   q   (   t   )       ,       \\xce\\x98   ;       S   )       =      \\xcf\\x86   (   q   (   t   )       ,       q   \\xcc\\x84   )   ,       i   .   e   .   ,       q   \\xcc\\x84       =       q   (   t   )       .       S   o       w   e       h   a   v   e      q   (   t   +   1   )       =       a   r   g       m   i   n       \\xcf\\x86   (   q   ,       q   (   t   )       )   ,       s   .   t   .   ,       \\xe2\\x88\\x80   `   ,      q         C      X         q   `   c       =       1   ,         (   1   0   )         c   =   1         w   h   i   c   h       l   e   a   d   s       t   o       m   i   n   i   m   i   z   i   n   g       t   h   e       L   a   g   r   a   n   g   i   a   n       d   e   f   i   n   e   d       b   y      \\xcf\\x95   (   q   ,       q   (   t   )       )       =       \\xcf\\x86   (   q   ,       q   (   t   )       )       +         X         \\xce\\xbb   `       (         `   \\xe2\\x88\\x88   L         w   h   e   r   e       \\xce\\xbb   `       i   s       t   h   e       L   a   g   r   a   n   g   e       m   u   l   t   i   p   l   i   e   r   .       B   y       s   e   t   t   i   n   g      \\xce\\xbb   `       =      (   t   +   1   )         N   o   t   e       t   h   a   t   ,       q   `   c         (   t   +   1   )         \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d         d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s   .       T   h   e       s   t   a   r   t   i   n   g      (   0   )      d   i   s   t   r   i   b   u   t   i   o   n   :       q   `   c       =       C   1       .      3   .   3         q   `   c       \\xe2\\x88\\x92       1   )   ,         (   1   1   )         c   =   1         \\xe2\\x88\\x82   \\xcf\\x95   (   q   ,   q   (   t   )       )      \\xe2\\x88\\x82   q   `   c         N       C      1       X       X       y   c      (   t   )      (   t   +   1   )      d       \\xce\\xbe   `       (   q   `   c       ,       x   i       )       a   n   d       q   `   c      N       i   =   1       c   =   1       x   i         s   a   t   i   s   f   i   e   s       t   h   a   t       q   `   c         C      X         =       0   ,       w   e       h   a   v   e      P   N       y   c      (   t   )      d   x   i       \\xce\\xbe   `       (   q   `   c       ,       x   i       )      .      =       P   C       i   =   1      P   N       y   c      (   t   )      \\xce\\xbe      (   q      ,      x      )      d      x      `      i      i      c   =   1      i   =   1      `   c         (   1   2   )         (   t   +   1   )      =       1   .       E   q   n   .       1   2       i   s       t   h   e       u   p   d   a   t   e       s   c   h   e   m   e       f   o   r      c   =   1       q   `   c      (   0   )      p   o   i   n   t       q   `       c   a   n       b   e       s   i   m   p   l   y       i   n   i   t   i   a   l   i   z   e   d       b   y       t   h   e       u   n   i   f   o   r   m         P   C         L   e   a   r   n   i   n   g       a       F   o   r   e   s   t         A       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s       F       =       {   T   1       ,       .       .       .       ,       T   K       }   .       I   n       t   h   e       t   r   a   i   n   i   n   g       s   t   a   g   e   ,       a   l   l       t   r   e   e   s       i   n       t   h   e      f   o   r   e   s   t       F       u   s   e       t   h   e       s   a   m   e       p   a   r   a   m   e   t   e   r   s       \\xce\\x98       f   o   r       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       f       (   \\xc2\\xb7   ;       \\xce\\x98   )       (   b   u   t       c   o   r   r   e   s   p   o   n   d       t   o       d   i   f   f   e   r   e   n   t      o   u   t   p   u   t       u   n   i   t   s       o   f       f       a   s   s   i   g   n   e   d       b   y       \\xcf\\x95   ,       s   e   e       F   i   g   .       2   )   ,       b   u   t       e   a   c   h       t   r   e   e       h   a   s       i   n   d   e   p   e   n   d   e   n   t       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s      q   .       T   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       f   o   r   e   s   t       i   s       g   i   v   e   n       b   y       a   v   e   r   a   g   i   n   g       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n   s       f   o   r       a   l   l       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s   :      P   K      1      R   F       =       K      k   =   1       R   T   k       ,       w   h   e   r   e       R   T   k       i   s       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       t   r   e   e       T   k       d   e   f   i   n   e   d       b   y       E   q   n   .       3   .       T   o       l   e   a   r   n       \\xce\\x98       b   y      f   i   x   i   n   g       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q       o   f       a   l   l       t   h   e       t   r   e   e   s       i   n       t   h   e       f   o   r   e   s   t       F   ,       b   a   s   e   d       o   n       t   h   e       d   e   r   i   v   a   t   i   o   n       i   n       S   e   c   .       3   .   2      a   n   d       r   e   f   e   r   r   i   n   g       t   o       F   i   g   .       2   ,       w   e       h   a   v   e      N       K      \\xe2\\x88\\x82   f   \\xcf\\x95   k       (   n   )       (   x   i       ;       \\xce\\x98   )      1       X   X       X      \\xe2\\x88\\x82   R   F      \\xe2\\x88\\x82   R   T   k      =      ,      \\xe2\\x88\\x82   \\xce\\x98      K       i   =   1      \\xe2\\x88\\x82   f   \\xcf\\x95   k       (   n   )       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x82   \\xce\\x98         (   1   3   )         k   =   1       n   \\xe2\\x88\\x88   N   k         w   h   e   r   e       N   k       a   n   d       \\xcf\\x95   k       (   \\xc2\\xb7   )       a   r   e       t   h   e       s   p   l   i   t       n   o   d   e       s   e   t       a   n   d       t   h   e       i   n   d   e   x       f   u   n   c   t   i   o   n       o   f       T   k       ,       r   e   s   p   e   c   t   i   v   e   l   y   .       N   o   t   e       t   h   a   t   ,      t   h   e       i   n   d   e   x       f   u   n   c   t   i   o   n       \\xcf\\x95   k       (   \\xc2\\xb7   )       f   o   r       e   a   c   h       t   r   e   e       i   s       r   a   n   d   o   m   l   y       a   s   s   i   g   n   e   d       b   e   f   o   r   e       t   r   e   e       l   e   a   r   n   i   n   g   ,       a   n   d       t   h   u   s       s   p   l   i   t      n   o   d   e   s       c   o   r   r   e   s   p   o   n   d       t   o       a       s   u   b   s   e   t       o   f       o   u   t   p   u   t       u   n   i   t   s       o   f       f       .       T   h   i   s       s   t   r   a   t   e   g   y       i   s       s   i   m   i   l   a   r       t   o       t   h   e       r   a   n   d   o   m       s   u   b   s   p   a   c   e      m   e   t   h   o   d       [   1   7   ]   ,       w   h   i   c   h       i   n   c   r   e   a   s   e   s       t   h   e       r   a   n   d   o   m   n   e   s   s       i   n       t   r   a   i   n   i   n   g       t   o       r   e   d   u   c   e       t   h   e       r   i   s   k       o   f       o   v   e   r   f   i   t   t   i   n   g   .      A   s       f   o   r       q   ,       s   i   n   c   e       e   a   c   h       t   r   e   e       i   n       t   h   e       f   o   r   e   s   t       F       h   a   s       i   t   s       o   w   n       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q   ,       w   e       c   a   n       u   p   d   a   t   e       t   h   e   m      i   n   d   e   p   e   n   d   e   n   t   l   y       b   y       E   q   n   .       1   2   ,       g   i   v   e   n       b   y       \\xce\\x98   .       F   o   r       i   m   p   l   e   m   e   n   t   a   t   i   o   n   a   l       c   o   n   v   e   n   i   e   n   c   e   ,       w   e       d   o       n   o   t       c   o   n   d   u   c   t       t   h   i   s      u   p   d   a   t   e       s   c   h   e   m   e       o   n       t   h   e       w   h   o   l   e       d   a   t   a   s   e   t       S       b   u   t       o   n       a       s   e   t       o   f       m   i   n   i   -   b   a   t   c   h   e   s       B   .       T   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       o   f       a      L   D   L   F       i   s       s   h   o   w   n       i   n       A   l   g   o   r   i   t   h   m   .       1   .      A   l   g   o   r   i   t   h   m       1       T   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       o   f       a       L   D   L   F   .      R   e   q   u   i   r   e   :       S   :       t   r   a   i   n   i   n   g       s   e   t   ,       n   B       :       t   h   e       n   u   m   b   e   r       o   f       m   i   n   i   -   b   a   t   c   h   e   s       t   o       u   p   d   a   t   e       q      I   n   i   t   i   a   l   i   z   e       \\xce\\x98       r   a   n   d   o   m   l   y       a   n   d       q       u   n   i   f   o   r   m   l   y   ,       s   e   t       B       =       {   \\xe2\\x88\\x85   }      w   h   i   l   e       N   o   t       c   o   n   v   e   r   g   e       d   o      w   h   i   l   e       |   B   |       <       n   B       d   o      R   a   n   d   o   m   l   y       s   e   l   e   c   t       a       m   i   n   i   -   b   a   t   c   h       B       f   r   o   m       S      U   p   d   a   t   e   S      \\xce\\x98       b   y       c   o   m   p   u   t   i   n   g       g   r   a   d   i   e   n   t       (   E   q   n   .       1   3   )       o   n       B      B   =   B       B      e   n   d       w   h   i   l   e      U   p   d   a   t   e       q       b   y       i   t   e   r   a   t   i   n   g       E   q   n   .       1   2       o   n       B      B       =       {   \\xe2\\x88\\x85   }      e   n   d       w   h   i   l   e      I   n       t   h   e       t   e   s   t   i   n   g       s   t   a   g   e   ,       t   h   e       o   u   t   p   u   t       o   f       t   h   e       f   o   r   e   s   t       F       i   s       g   i   v   e   n       b   y       a   v   e   r   a   g   i   n   g       t   h   e       p   r   e   d   i   c   t   i   o   n   s       f   r   o   m       a   l   l       t   h   e      P   K      1      i   n   d   i   v   i   d   u   a   l       t   r   e   e   s   :       g   (   x   ;       \\xce\\x98   ,       F   )       =       K      k   =   1       g   (   x   ;       \\xce\\x98   ,       T   k       )   .      6         \\x0c   4         E   x   p   e   r   i   m   e   n   t   a   l       R   e   s   u   l   t   s         O   u   r       r   e   a   l   i   z   a   t   i   o   n       o   f       L   D   L   F   s       i   s       b   a   s   e   d       o   n       \\xe2\\x80\\x9c   C   a   f   f   e   \\xe2\\x80\\x9d       [   1   8   ]   .       I   t       i   s       m   o   d   u   l   a   r       a   n   d       i   m   p   l   e   m   e   n   t   e   d       a   s       a       s   t   a   n   d   a   r   d      n   e   u   r   a   l       n   e   t   w   o   r   k       l   a   y   e   r   .       W   e       c   a   n       e   i   t   h   e   r       u   s   e       i   t       a   s       a       s   h   a   l   l   o   w       s   t   a   n   d   -   a   l   o   n   e       m   o   d   e   l       (   s   L   D   L   F   s   )       o   r       i   n   t   e   g   r   a   t   e       i   t      w   i   t   h       a   n   y       d   e   e   p       n   e   t   w   o   r   k   s       (   d   L   D   L   F   s   )   .       W   e       e   v   a   l   u   a   t   e       s   L   D   L   F   s       o   n       d   i   f   f   e   r   e   n   t       L   D   L       t   a   s   k   s       a   n   d       c   o   m   p   a   r   e       i   t       w   i   t   h      o   t   h   e   r       s   t   a   n   d   -   a   l   o   n   e       L   D   L       m   e   t   h   o   d   s   .       A   s       d   L   D   L   F   s       c   a   n       b   e       l   e   a   r   n   e   d       f   r   o   m       r   a   w       i   m   a   g   e       d   a   t   a       i   n       a   n       e   n   d   -   t   o   -   e   n   d      m   a   n   n   e   r   ,       w   e       v   e   r   i   f   y       d   L   D   L   F   s       o   n       a       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       i   .   e   .   ,       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n   .       T   h   e       d   e   f   a   u   l   t      s   e   t   t   i   n   g   s       f   o   r       t   h   e       p   a   r   a   m   e   t   e   r   s       o   f       o   u   r       f   o   r   e   s   t   s       a   r   e   :       t   r   e   e       n   u   m   b   e   r       (   5   )   ,       t   r   e   e       d   e   p   t   h       (   7   )   ,       o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f      t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       (   6   4   )   ,       i   t   e   r   a   t   i   o   n       t   i   m   e   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       (   2   0   )   ,       t   h   e       n   u   m   b   e   r       o   f      m   i   n   i   -   b   a   t   c   h   e   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       (   1   0   0   )   ,       m   a   x   i   m   u   m       i   t   e   r   a   t   i   o   n       (   2   5   0   0   0   )   .      4   .   1         C   o   m   p   a   r   i   s   o   n       o   f       s   L   D   L   F   s       t   o       S   t   a   n   d   -   a   l   o   n   e       L   D   L       M   e   t   h   o   d   s         W   e       c   o   m   p   a   r   e       o   u   r       s   h   a   l   l   o   w       m   o   d   e   l       s   L   D   L   F   s       w   i   t   h       o   t   h   e   r       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       s   t   a   n   d   -   a   l   o   n   e       L   D   L       m   e   t   h   o   d   s   .      F   o   r       s   L   D   L   F   s   ,       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       f       (   x   ,       \\xce\\x98   )       i   s       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n       o   f       x   ,       i   .   e   .   ,       t   h   e       i   -   t   h      o   u   t   p   u   t       u   n   i   t       f   i       (   x   ,       \\xce\\xb8   i       )       =       \\xce\\xb8   i   >       x   ,       w   h   e   r   e       \\xce\\xb8   i       i   s       t   h   e       i   -   t   h       c   o   l   u   m   n       o   f       t   h   e       t   r   a   n   s   f   o   r   m   a   t   i   o   n       m   a   t   r   i   x       \\xce\\x98   .       W   e      u   s   e   d       3       p   o   p   u   l   a   r       L   D   L       d   a   t   a   s   e   t   s       i   n       [   6   ]   ,       M   o   v   i   e   ,       H   u   m   a   n       G   e   n   e       a   n   d       N   a   t   u   r   a   l       S   c   e   n   e   1       .       T   h   e       s   a   m   p   l   e   s      i   n       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   r   e       r   e   p   r   e   s   e   n   t   e   d       b   y       n   u   m   e   r   i   c   a   l       d   e   s   c   r   i   p   t   o   r   s   ,       a   n   d       t   h   e       g   r   o   u   n   d       t   r   u   t   h   s       f   o   r       t   h   e   m       a   r   e      t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       m   o   v   i   e   s   ,       t   h   e       d   i   s   e   a   s   e   s       d   i   s   t   r   i   b   u   t   i   o   n   s       r   e   l   a   t   e   d       t   o       h   u   m   a   n      g   e   n   e   s       a   n   d       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       o   n       s   c   e   n   e   s   ,       s   u   c   h       a   s       p   l   a   n   t   ,       s   k   y       a   n   d       c   l   o   u   d   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       T   h   e       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n   s       o   f       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   r   e       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   ,       s   u   c   h       a   s       t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       s   h   o   w   n       i   n      F   i   g   .       1   (   b   )   .       F   o   l   l   o   w   i   n   g       [   7   ,       2   7   ]   ,       w   e       u   s   e       6       m   e   a   s   u   r   e   s       t   o       e   v   a   l   u   a   t   e       t   h   e       p   e   r   f   o   r   m   a   n   c   e   s       o   f       L   D   L       m   e   t   h   o   d   s   ,      w   h   i   c   h       c   o   m   p   u   t   e       t   h   e       a   v   e   r   a   g   e       s   i   m   i   l   a   r   i   t   y   /   d   i   s   t   a   n   c   e       b   e   t   w   e   e   n       t   h   e       p   r   e   d   i   c   t   e   d       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s       a   n   d       t   h   e       r   e   a   l      r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s   ,       i   n   c   l   u   d   i   n   g       4       d   i   s   t   a   n   c   e       m   e   a   s   u   r   e   s       (   K   -   L   ,       E   u   c   l   i   d   e   a   n   ,       S   \\xcf\\x86   r   e   n   s   e   n   ,       S   q   u   a   r   e   d       \\xcf\\x87   2       )       a   n   d       t   w   o      s   i   m   i   l   a   r   i   t   y       m   e   a   s   u   r   e   s       (   F   i   d   e   l   i   t   y   ,       I   n   t   e   r   s   e   c   t   i   o   n   )   .      W   e       e   v   a   l   u   a   t   e       o   u   r       s   h   a   l   l   o   w       m   o   d   e   l       s   L   D   L   F   s       o   n       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   n   d       c   o   m   p   a   r   e       i   t       w   i   t   h       o   t   h   e   r       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t      s   t   a   n   d   -   a   l   o   n   e       L   D   L       m   e   t   h   o   d   s   .       T   h   e       r   e   s   u   l   t   s       o   f       s   L   D   L   F   s       a   n   d       t   h   e       c   o   m   p   e   t   i   t   o   r   s       a   r   e       s   u   m   m   a   r   i   z   e   d       i   n       T   a   b   l   e       1   .      F   o   r       M   o   v   i   e       w   e       q   u   o   t   e       t   h   e       r   e   s   u   l   t   s       r   e   p   o   r   t   e   d       i   n       [   2   7   ]   ,       a   s       t   h   e       c   o   d   e       o   f       [   2   7   ]       i   s       n   o   t       p   u   b   l   i   c   l   y       a   v   a   i   l   a   b   l   e   .       F   o   r       t   h   e      r   e   s   u   l   t   s       o   f       t   h   e       o   t   h   e   r   s       t   w   o   ,       w   e       r   u   n       c   o   d   e       t   h   a   t       t   h   e       a   u   t   h   o   r   s       h   a   d       m   a   d   e       a   v   a   i   l   a   b   l   e   .       I   n       a   l   l       c   a   s   e   ,       f   o   l   l   o   w   i   n   g       [   2   7   ,       6   ]   ,      w   e       s   p   l   i   t       e   a   c   h       d   a   t   a   s   e   t       i   n   t   o       1   0       f   i   x   e   d       f   o   l   d   s       a   n   d       d   o       s   t   a   n   d   a   r   d       t   e   n   -   f   o   l   d       c   r   o   s   s       v   a   l   i   d   a   t   i   o   n   ,       w   h   i   c   h       r   e   p   r   e   s   e   n   t   s      t   h   e       r   e   s   u   l   t       b   y       \\xe2\\x80\\x9c   m   e   a   n   \\xc2\\xb1   s   t   a   n   d   a   r   d       d   e   v   i   a   t   i   o   n   \\xe2\\x80\\x9d       a   n   d       m   a   t   t   e   r   s       l   e   s   s       h   o   w       t   r   a   i   n   i   n   g       a   n   d       t   e   s   t   i   n   g       d   a   t   a       g   e   t       d   i   v   i   d   e   d   .      A   s       c   a   n       b   e       s   e   e   n       f   r   o   m       T   a   b   l   e       1   ,       s   L   D   L   F   s       p   e   r   f   o   r   m       b   e   s   t       o   n       a   l   l       o   f       t   h   e       s   i   x       m   e   a   s   u   r   e   s   .      T   a   b   l   e       1   :       C   o   m   p   a   r   i   s   o   n       r   e   s   u   l   t   s       o   n       t   h   r   e   e       L   D   L       d   a   t   a   s   e   t   s       [   6   ]   .       \\xe2\\x80\\x9c   \\xe2\\x86\\x91   \\xe2\\x80\\x9d       a   n   d       \\xe2\\x80\\x9c   \\xe2\\x86\\x93   \\xe2\\x80\\x9d       i   n   d   i   c   a   t   e       t   h   e       l   a   r   g   e   r       a   n   d       t   h   e       s   m   a   l   l   e   r      t   h   e       b   e   t   t   e   r   ,       r   e   s   p   e   c   t   i   v   e   l   y   .      D   a   t   a   s   e   t         M   e   t   h   o   d         K   -   L       \\xe2\\x86\\x93         E   u   c   l   i   d   e   a   n       \\xe2\\x86\\x93         S   \\xcf\\x86   r   e   n   s   e   n       \\xe2\\x86\\x93         S   q   u   a   r   e   d       \\xcf\\x87   2       \\xe2\\x86\\x93         F   i   d   e   l   i   t   y       \\xe2\\x86\\x91         I   n   t   e   r   s   e   c   t   i   o   n       \\xe2\\x86\\x91         M   o   v   i   e         s   L   D   L   F       (   o   u   r   s   )      A   O   S   O   -   L   D   L   o   g   i   t   B   o   o   s   t       [   2   7   ]      L   D   L   o   g   i   t   B   o   o   s   t       [   2   7   ]      L   D   S   V   R       [   7   ]      B   F   G   S   -   L   D   L       [   6   ]      I   I   S   -   L   D   L       [   1   1   ]         0   .   0   7   3   \\xc2\\xb1   0   .   0   0   5      0   .   0   8   6   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   0   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   2   \\xc2\\xb1   0   .   0   0   5      0   .   0   9   9   \\xc2\\xb1   0   .   0   0   4      0   .   1   2   9   \\xc2\\xb1   0   .   0   0   7         0   .   1   3   3   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   5   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   9   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   8   \\xc2\\xb1   0   .   0   0   4      0   .   1   6   7   \\xc2\\xb1   0   .   0   0   4      0   .   1   8   7   \\xc2\\xb1   0   .   0   0   4         0   .   1   3   0   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   2   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   5   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   6   \\xc2\\xb1   0   .   0   0   4      0   .   1   6   4   \\xc2\\xb1   0   .   0   0   3      0   .   1   8   3   \\xc2\\xb1   0   .   0   0   4         0   .   0   7   0   \\xc2\\xb1   0   .   0   0   4      0   .   0   8   4   \\xc2\\xb1   0   .   0   0   3      0   .   0   8   8   \\xc2\\xb1   0   .   0   0   3      0   .   0   8   8   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   6   \\xc2\\xb1   0   .   0   0   4      0   .   1   2   0   \\xc2\\xb1   0   .   0   0   5         0   .   9   8   1   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   8   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   7   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   7   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   4   \\xc2\\xb1   0   .   0   0   1      0   .   9   6   7   \\xc2\\xb1   0   .   0   0   1         0   .   8   7   0   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   8   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   5   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   4   \\xc2\\xb1   0   .   0   0   4      0   .   8   3   6   \\xc2\\xb1   0   .   0   0   3      0   .   8   1   7   \\xc2\\xb1   0   .   0   0   4         s   L   D   L   F       (   o   u   r   s   )      L   D   S   V   R       [   7   ]      B   F   G   S   -   L   D   L       [   6   ]      I   I   S   -   L   D   L       [   1   1   ]         0   .   2   2   8   \\xc2\\xb1   0   .   0   0   6      0   .   2   4   5   \\xc2\\xb1   0   .   0   1   9      0   .   2   3   1   \\xc2\\xb1   0   .   0   2   1      0   .   2   3   9   \\xc2\\xb1   0   .   0   1   8         0   .   0   8   5   \\xc2\\xb1   0   .   0   0   2      0   .   0   9   9   \\xc2\\xb1   0   .   0   0   5      0   .   0   7   6   \\xc2\\xb1   0   .   0   0   6      0   .   0   8   9   \\xc2\\xb1   0   .   0   0   6         0   .   2   1   2   \\xc2\\xb1   0   .   0   0   2      0   .   2   2   9   \\xc2\\xb1   0   .   0   1   5      0   .   2   3   1   \\xc2\\xb1   0   .   0   1   2      0   .   2   5   3   \\xc2\\xb1   0   .   0   0   9         0   .   1   7   9   \\xc2\\xb1   0   .   0   0   4      0   .   1   8   9   \\xc2\\xb1   0   .   0   2   1      0   .   2   1   1   \\xc2\\xb1   0   .   0   1   8      0   .   2   0   5   \\xc2\\xb1   0   .   0   1   2         0   .   9   4   8   \\xc2\\xb1   0   .   0   0   1      0   .   9   4   0   \\xc2\\xb1   0   .   0   0   6      0   .   9   3   8   \\xc2\\xb1   0   .   0   0   8      0   .   9   4   4   \\xc2\\xb1   0   .   0   0   3         0   .   7   8   8   \\xc2\\xb1   0   .   0   0   2      0   .   7   7   1   \\xc2\\xb1   0   .   0   1   5      0   .   7   6   9   \\xc2\\xb1   0   .   0   1   2      0   .   7   4   7   \\xc2\\xb1   0   .   0   0   9         s   L   D   L   F       (   o   u   r   s   )      L   D   S   V   R       [   7   ]      B   F   G   S   -   L   D   L       [   6   ]      I   I   S   -   L   D   L       [   1   1   ]         0   .   5   3   4   \\xc2\\xb1   0   .   0   1   3      0   .   8   5   2   \\xc2\\xb1   0   .   0   2   3      0   .   8   5   6   \\xc2\\xb1   0   .   0   6   1      0   .   8   7   9   \\xc2\\xb1   0   .   0   2   3         0   .   3   1   7   \\xc2\\xb1   0   .   0   1   4      0   .   5   1   1   \\xc2\\xb1   0   .   0   2   1      0   .   4   7   5   \\xc2\\xb1   0   .   0   2   9      0   .   4   5   8   \\xc2\\xb1   0   .   0   1   4         0   .   3   3   6   \\xc2\\xb1   0   .   0   1   0      0   .   4   9   2   \\xc2\\xb1   0   .   0   1   6      0   .   5   0   8   \\xc2\\xb1   0   .   0   2   6      0   .   5   3   9   \\xc2\\xb1   0   .   0   1   1         0   .   4   4   8   \\xc2\\xb1   0   .   0   1   7      0   .   5   9   5   \\xc2\\xb1   0   .   0   2   6      0   .   7   1   6   \\xc2\\xb1   0   .   0   4   1      0   .   7   9   2   \\xc2\\xb1   0   .   0   1   9         0   .   8   2   4   \\xc2\\xb1   0   .   0   0   8      0   .   8   1   3   \\xc2\\xb1   0   .   0   0   8      0   .   7   2   2   \\xc2\\xb1   0   .   0   2   1      0   .   6   8   6   \\xc2\\xb1   0   .   0   0   9         0   .   6   6   4   \\xc2\\xb1   0   .   0   1   0      0   .   5   0   9   \\xc2\\xb1   0   .   0   1   6      0   .   4   9   2   \\xc2\\xb1   0   .   0   2   6      0   .   4   6   1   \\xc2\\xb1   0   .   0   1   1         H   u   m   a   n       G   e   n   e         N   a   t   u   r   a   l       S   c   e   n   e         4   .   2         E   v   a   l   u   a   t   i   o   n       o   f       d   L   D   L   F   s       o   n       F   a   c   i   a   l       A   g   e       E   s   t   i   m   a   t   i   o   n         I   n       s   o   m   e       l   i   t   e   r   a   t   u   r   e       [   8   ,       1   1   ,       2   8   ,       1   5   ,       5   ]   ,       a   g   e       e   s   t   i   m   a   t   i   o   n       i   s       f   o   r   m   u   l   a   t   e   d       a   s       a       L   D   L       p   r   o   b   l   e   m   .       W   e       c   o   n   d   u   c   t      f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       e   x   p   e   r   i   m   e   n   t   s       o   n       M   o   r   p   h       [   2   4   ]   ,       w   h   i   c   h       c   o   n   t   a   i   n   s       m   o   r   e       t   h   a   n       5   0   ,   0   0   0       f   a   c   i   a   l       i   m   a   g   e   s      f   r   o   m       a   b   o   u   t       1   3   ,   0   0   0       p   e   o   p   l   e       o   f       d   i   f   f   e   r   e   n   t       r   a   c   e   s   .       E   a   c   h       f   a   c   i   a   l       i   m   a   g   e       i   s       a   n   n   o   t   a   t   e   d       w   i   t   h       a       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e   .      T   o       g   e   n   e   r   a   t   e       a   n       a   g   e       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       e   a   c   h       f   a   c   e       i   m   a   g   e   ,       w   e       f   o   l   l   o   w       t   h   e       s   a   m   e       s   t   r   a   t   e   g   y       u   s   e   d       i   n       [   8   ,       2   8   ,       5   ]   ,      w   h   i   c   h       u   s   e   s       a       G   a   u   s   s   i   a   n       d   i   s   t   r   i   b   u   t   i   o   n       w   h   o   s   e       m   e   a   n       i   s       t   h   e       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e       o   f       t   h   e       f   a   c   e       i   m   a   g   e       (   F   i   g   .       1   (   a   )   )   .      T   h   e       p   r   e   d   i   c   t   e   d       a   g   e       f   o   r       a       f   a   c   e       i   m   a   g   e       i   s       s   i   m   p   l   y       t   h   e       a   g   e       h   a   v   i   n   g       t   h   e       h   i   g   h   e   s   t       p   r   o   b   a   b   i   l   i   t   y       i   n       t   h   e       p   r   e   d   i   c   t   e   d      1         W   e       d   o   w   n   l   o   a   d       t   h   e   s   e       d   a   t   a   s   e   t   s       f   r   o   m       h   t   t   p   :   /   /   c   s   e   .   s   e   u   .   e   d   u   .   c   n   /   p   e   o   p   l   e   /   x   g   e   n   g   /   L   D   L   /   i   n   d   e   x   .   h   t   m   .         7         \\x0c   l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   .       T   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       i   s       e   v   a   l   u   a   t   e   d       b   y       t   h   e       m   e   a   n       a   b   s   o   l   u   t   e       e   r   r   o   r       (   M   A   E   )      b   e   t   w   e   e   n       p   r   e   d   i   c   t   e   d       a   g   e   s       a   n   d       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e   s   .       A   s       t   h   e       c   u   r   r   e   n   t       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       r   e   s   u   l   t       o   n       M   o   r   p   h      i   s       o   b   t   a   i   n       b   y       f   i   n   e   -   t   u   n   i   n   g       D   L   D   L       [   5   ]       o   n       V   G   G   -   F   a   c   e       [   2   3   ]   ,       w   e       a   l   s   o       b   u   i   l   d       a       d   L   D   L   F       o   n       V   G   G   -   F   a   c   e   ,       b   y      r   e   p   l   a   c   i   n   g       t   h   e       s   o   f   t   m   a   x       l   a   y   e   r       i   n       V   G   G   N   e   t       b   y       a       L   D   L   F   .       F   o   l   l   o   w   i   n   g       [   5   ]   ,       w   e       d   o       s   t   a   n   d   a   r   d       1   0       t   e   n   -   f   o   l   d       c   r   o   s   s      v   a   l   i   d   a   t   i   o   n       a   n   d       t   h   e       r   e   s   u   l   t   s       a   r   e       s   u   m   m   a   r   i   z   e   d       i   n       T   a   b   l   e   .       2   ,       w   h   i   c   h       s   h   o   w   s       d   L   D   L   F       a   c   h   i   e   v   e       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t      p   e   r   f   o   r   m   a   n   c   e       o   n       M   o   r   p   h   .       N   o   t   e       t   h   a   t   ,       t   h   e       s   i   g   n   i   f   i   c   a   n   t       p   e   r   f   o   r   m   a   n   c   e       g   a   i   n       b   e   t   w   e   e   n       d   e   e   p       L   D   L       m   o   d   e   l   s      (   D   L   D   L       a   n   d       d   L   D   L   F   )       a   n   d       n   o   n   -   d   e   e   p       L   D   L       m   o   d   e   l   s       (   I   I   S   -   L   D   L   ,       C   P   N   N   ,       B   F   G   S   -   L   D   L   )       a   n   d       t   h   e       s   u   p   e   r   i   o   r   i   t   y      o   f       d   L   D   L   F       c   o   m   p   a   r   e   d       w   i   t   h       D   L   D   L       v   e   r   i   f   i   e   s       t   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       e   n   d   -   t   o   -   e   n   d       l   e   a   r   n   i   n   g       a   n   d       o   u   r       t   r   e   e   -   b   a   s   e   d      m   o   d   e   l       f   o   r       L   D   L   ,       r   e   s   p   e   c   t   i   v   e   l   y   .      T   a   b   l   e       2   :       M   A   E       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       c   o   m   p   a   r   i   s   o   n       o   n       M   o   r   p   h       [   2   4   ]   .      M   e   t   h   o   d         I   I   S   -   L   D   L       [   1   1   ]         C   P   N   N       [   1   1   ]         B   F   G   S   -   L   D   L       [   6   ]         D   L   D   L   +   V   G   G   -   F   a   c   e       [   5   ]         d   L   D   L   F   +   V   G   G   -   F   a   c   e       (   o   u   r   s   )         M   A   E         5   .   6   7   \\xc2\\xb1   0   .   1   5         4   .   8   7   \\xc2\\xb1   0   .   3   1         3   .   9   4   \\xc2\\xb1   0   .   0   5         2   .   4   2   \\xc2\\xb1   0   .   0   1         2   .   2   4   \\xc2\\xb1   0   .   0   2         A   s       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       o   f       g   e   n   d   e   r       a   n   d       e   t   h   n   i   c   i   t   y       i   s       v   e   r   y       u   n   b   a   l   a   n   c   e   d       i   n       M   o   r   p   h   ,       m   a   n   y       a   g   e       e   s   t   i   m   a   t   i   o   n       m   e   t   h   o   d   s       [   1   3   ,       1   4   ,       1   5   ]       a   r   e       e   v   a   l   u   a   t   e   d       o   n       a       s   u   b   s   e   t       o   f       M   o   r   p   h   ,       c   a   l   l   e   d       M   o   r   p   h   _   S   u   b       f   o   r       s   h   o   r   t   ,       w   h   i   c   h       c   o   n   s   i   s   t   s       o   f      2   0   ,   1   6   0       s   e   l   e   c   t   e   d       f   a   c   i   a   l       i   m   a   g   e   s       t   o       a   v   o   i   d       t   h   e       i   n   f   l   u   e   n   c   e       o   f       u   n   b   a   l   a   n   c   e   d       d   i   s   t   r   i   b   u   t   i   o   n   .       T   h   e       b   e   s   t       p   e   r   f   o   r   m   a   n   c   e      r   e   p   o   r   t   e   d       o   n       M   o   r   p   h   _   S   u   b       i   s       g   i   v   e   n       b   y       D   2   L   D   L       [   1   5   ]   ,       a       d   a   t   a   -   d   e   p   e   n   d   e   n   t       L   D   L       m   e   t   h   o   d   .       A   s       D   2   L   D   L       u   s   e   d      t   h   e       o   u   t   p   u   t       o   f       t   h   e       \\xe2\\x80\\x9c   f   c   7   \\xe2\\x80\\x9d       l   a   y   e   r       i   n       A   l   e   x   N   e   t       [   2   1   ]       a   s       t   h   e       f   a   c   e       i   m   a   g   e       f   e   a   t   u   r   e   s   ,       h   e   r   e       w   e       i   n   t   e   g   r   a   t   e       a       L   D   L   F      w   i   t   h       A   l   e   x   N   e   t   .       F   o   l   l   o   w   i   n   g       t   h   e       e   x   p   e   r   i   m   e   n   t       s   e   t   t   i   n   g       u   s   e   d       i   n       D   2   L   D   L   ,       w   e       e   v   a   l   u   a   t   e       o   u   r       d   L   D   L   F       a   n   d       t   h   e      c   o   m   p   e   t   i   t   o   r   s   ,       i   n   c   l   u   d   i   n   g       b   o   t   h       S   L   L       a   n   d       L   D   L       b   a   s   e   d       m   e   t   h   o   d   s   ,       u   n   d   e   r       s   i   x       d   i   f   f   e   r   e   n   t       t   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o   s       (   1   0   %      t   o       6   0   %   )   .       A   l   l       o   f       t   h   e       c   o   m   p   e   t   i   t   o   r   s       a   r   e       t   r   a   i   n   e   d       o   n       t   h   e       s   a   m   e       d   e   e   p       f   e   a   t   u   r   e   s       u   s   e   d       b   y       D   2   L   D   L   .       A   s       c   a   n       b   e      s   e   e   n       f   r   o   m       T   a   b   l   e       3   ,       o   u   r       d   L   D   L   F   s       s   i   g   n   i   f   i   c   a   n   t   l   y       o   u   t   p   e   r   f   o   r   m       o   t   h   e   r   s       f   o   r       a   l   l       t   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o   s   .      N   o   t   e       t   h   a   t   ,       t   h   e       g   e   n   e   r   a   t   e   d       a   g   e       d   i   s   t   r   i   -       F   i   g   u   r   e       3   :       M   A   E       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       c   o   m   p   a   r   i   s   o   n       o   n      b   u   t   i   o   n   s       a   r   e       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   s       M   o   r   p   h   _   S   u   b   .      a   n   d       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       u   s   e   d       i   n      T   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o      M   e   t   h   o   d      S   e   c   .       4   .   1       a   r   e       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   .      1   0   %      2   0   %      3   0   %      4   0   %      5   0   %      6   0   %      T   h   e       p   r   o   p   o   s   e   d       m   e   t   h   o   d       L   D   L   F   s       a   c   h   i   e   v   e      A   A   S       [   2   2   ]      4   .   9   0   8   1      4   .   7   6   1   6      4   .   6   5   0   7      4   .   5   5   5   3      4   .   4   6   9   0      4   .   4   0   6   1      t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       r   e   s   u   l   t   s       o   n       b   o   t   h       o   f      L   A   R   R       [   1   2   ]      4   .   7   5   0   1      4   .   6   1   1   2      4   .   5   1   3   1      4   .   4   2   7   3      4   .   3   5   0   0      4   .   2   9   4   9      I   I   S   -   A   L   D   L       [   9   ]      4   .   1   7   9   1      4   .   1   6   8   3      4   .   1   2   2   8      4   .   1   1   0   7      4   .   1   0   2   4      4   .   0   9   0   2      t   h   e   m   ,       w   h   i   c   h       v   e   r   i   f   i   e   s       t   h   a   t       o   u   r       m   o   d   e   l      D   2   L   D   L       [   1   5   ]      4   .   1   0   8   0      3   .   9   8   5   7      3   .   9   2   0   4      3   .   8   7   1   2      3   .   8   5   6   0      3   .   8   3   8   5      h   a   s       t   h   e       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l      d   L   D   L   F       (   o   u   r   s   )      3   .   8   4   9   5      3   .   6   2   2   0      3   .   3   9   9   1      3   .   2   4   0   1      3   .   1   9   1   7      3   .   1   2   2   4      f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .      4   .   3         T   i   m   e       C   o   m   p   l   e   x   i   t   y         L   e   t       h       a   n   d       s   B       b   e       t   h   e       t   r   e   e       d   e   p   t   h       a   n   d       t   h   e      b   a   t   c   h       s   i   z   e   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       E   a   c   h       t   r   e   e       h   a   s       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1       s   p   l   i   t       n   o   d   e   s       a   n   d       2   h   \\xe2\\x88\\x92   1       l   e   a   f       n   o   d   e   s   .       L   e   t       D       =       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1   .      F   o   r       o   n   e       t   r   e   e       a   n   d       o   n   e       s   a   m   p   l   e   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a       f   o   r   w   a   r   d       p   a   s   s       a   n   d       a       b   a   c   k   w   a   r   d       p   a   s   s       a   r   e       O   (   D       +      D       +       1   \\xc3\\x97   C   )       =       O   (   D   \\xc3\\x97   C   )       a   n   d       O   (   D       +       1   \\xc3\\x97   C       +       D   \\xc3\\x97   C   )       =       O   (   D   \\xc3\\x97   C   )   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       S   o       f   o   r       K       t   r   e   e   s       a   n   d      n   B       b   a   t   c   h   e   s   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a       f   o   r   w   a   r   d       a   n   d       b   a   c   k   w   a   r   d       p   a   s   s       i   s       O   (   D   \\xc3\\x97   C   \\xc3\\x97   K   \\xc3\\x97   n   B       \\xc3\\x97   s   B       )   .       T   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a   n       i   t   e   r   a   t   i   o   n       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e   s       a   r   e       O   (   n   B       \\xc3\\x97   s   B       \\xc3\\x97   K   \\xc3\\x97   C   \\xc3\\x97   D       +       1   )       =       O   (   D   \\xc3\\x97   C   \\xc3\\x97   K   \\xc3\\x97   n   B       \\xc3\\x97   s   B       )   .      T   h   u   s   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       f   o   r       t   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       (   o   n   e       e   p   o   c   h   ,       n   B       b   a   t   c   h   e   s   )       a   n   d       t   h   e       t   e   s   t   i   n   g       p   r   o   c   e   d   u   r   e      (   o   n   e       s   a   m   p   l   e   )       a   r   e       O   (   D   \\xc3\\x97   C   \\xc3\\x97   K   \\xc3\\x97   n   B       \\xc3\\x97   s   B       )       a   n   d       O   (   D   \\xc3\\x97   C   \\xc3\\x97   K   )   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       L   D   L   F   s       a   r   e       e   f   f   i   c   i   e   n   t   :       O   n      M   o   r   p   h   _   S   u   b       (   1   2   6   3   6       t   r   a   i   n   i   n   g       i   m   a   g   e   s   ,       8   4   2   4       t   e   s   t   i   n   g       i   m   a   g   e   s   )   ,       o   u   r       m   o   d   e   l       o   n   l   y       t   a   k   e   s       5   2   5   0   s       f   o   r       t   r   a   i   n   i   n   g      (   2   5   0   0   0       i   t   e   r   a   t   i   o   n   s   )       a   n   d       8   s       f   o   r       t   e   s   t   i   n   g       a   l   l       8   4   2   4       i   m   a   g   e   s   .      4   .   4         P   a   r   a   m   e   t   e   r       D   i   s   c   u   s   s   i   o   n         N   o   w       w   e       d   i   s   c   u   s   s       t   h   e       i   n   f   l   u   e   n   c   e       o   f       p   a   r   a   m   e   t   e   r       s   e   t   t   i   n   g   s       o   n       p   e   r   f   o   r   m   a   n   c   e   .       W   e       r   e   p   o   r   t       t   h   e       r   e   s   u   l   t   s       o   f       r   a   t   i   n   g      p   r   e   d   i   c   t   i   o   n       o   n       M   o   v   i   e       (   m   e   a   s   u   r   e   d       b   y       K   -   L   )       a   n   d       a   g   e       e   s   t   i   m   a   t   i   o   n       o   n       M   o   r   p   h   _   S   u   b       w   i   t   h       6   0   %       t   r   a   i   n   i   n   g       s   e   t      r   a   t   i   o       (   m   e   a   s   u   r   e   d       b   y       M   A   E   )       f   o   r       d   i   f   f   e   r   e   n   t       p   a   r   a   m   e   t   e   r       s   e   t   t   i   n   g   s       i   n       t   h   i   s       s   e   c   t   i   o   n   .      T   r   e   e       n   u   m   b   e   r   .       A   s       a       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       m   o   d   e   l   ,       i   t       i   s       n   e   c   e   s   s   a   r   y       t   o       i   n   v   e   s   t   i   g   a   t   e       h   o   w       p   e   r   f   o   r   m   a   n   c   e   s      c   h   a   n   g   e       b   y       v   a   r   y   i   n   g       t   h   e       t   r   e   e       n   u   m   b   e   r       u   s   e   d       i   n       a       f   o   r   e   s   t   .       N   o   t   e       t   h   a   t   ,       a   s       w   e       d   i   s   c   u   s   s   e   d       i   n       S   e   c   .       2   ,       t   h   e      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       t   o       l   e   a   r   n       a       f   o   r   e   s   t       p   r   o   p   o   s   e   d       i   n       d   N   D   F   s       [   2   0   ]       i   s       d   i   f   f   e   r   e   n   t       f   r   o   m       o   u   r   s   .       T   h   e   r   e   f   o   r   e   ,       i   t       i   s      n   e   c   e   s   s   a   r   y       t   o       s   e   e       w   h   i   c   h       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       i   s       b   e   t   t   e   r       t   o       l   e   a   r   n       a       f   o   r   e   s   t   .       T   o   w   a   r   d   s       t   h   i   s       e   n   d   ,       w   e       r   e   p   l   a   c   e       o   u   r      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       i   n       d   L   D   L   F   s       b   y       t   h   e       o   n   e       u   s   e   d       i   n       d   N   D   F   s   ,       a   n   d       n   a   m   e       t   h   i   s       m   e   t   h   o   d       d   N   D   F   s   -   L   D   L   .       T   h   e      c   o   r   r   e   s   p   o   n   d   i   n   g       s   h   a   l   l   o   w       m   o   d   e   l       i   s       n   a   m   e   d       b   y       s   N   D   F   s   -   L   D   L   .       W   e       f   i   x       o   t   h   e   r       p   a   r   a   m   e   t   e   r   s   ,       i   .   e   .   ,       t   r   e   e       d   e   p   t   h       a   n   d      8         \\x0c   o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   ,       a   s       t   h   e       d   e   f   a   u   l   t       s   e   t   t   i   n   g   .       A   s       s   h   o   w   n       i   n       F   i   g   .       4       (   a   )   ,       o   u   r      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       c   a   n       i   m   p   r   o   v   e       t   h   e       p   e   r   f   o   r   m   a   n   c   e       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       w   h   i   l   e       t   h   e       o   n   e       u   s   e   d       i   n       d   N   D   F   s      e   v   e   n       l   e   a   d   s       t   o       a       w   o   r   s   e       p   e   r   f   o   r   m   a   n   c   e       t   h   a   n       o   n   e       f   o   r       a       s   i   n   g   l   e       t   r   e   e   .      O   b   s   e   r   v   e   d       f   r   o   m       F   i   g   .       4   ,       t   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       L   D   L   F   s       c   a   n       b   e       i   m   p   r   o   v   e   d       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       b   u   t       t   h   e      i   m   p   r   o   v   e   m   e   n   t       b   e   c   o   m   e   s       i   n   c   r   e   a   s   i   n   g   l   y       s   m   a   l   l   e   r       a   n   d       s   m   a   l   l   e   r   .       T   h   e   r   e   f   o   r   e   ,       u   s   i   n   g       m   u   c   h       l   a   r   g   e   r       e   n   s   e   m   b   l   e   s      d   o   e   s       n   o   t       y   i   e   l   d       a       b   i   g       i   m   p   r   o   v   e   m   e   n   t       (   O   n       M   o   v   i   e   ,       t   h   e       n   u   m   b   e   r       o   f       t   r   e   e   s       K       =       1   0   0   :       K   -   L       =       0   .   0   7   0       v   s       K       =       2   0   :      K   -   L       =       0   .   0   7   1   )   .       N   o   t   e       t   h   a   t   ,       n   o   t       a   l   l       r   a   n   d   o   m       f   o   r   e   s   t   s       b   a   s   e   d       m   e   t   h   o   d   s       u   s   e       a       l   a   r   g   e       n   u   m   b   e   r       o   f       t   r   e   e   s   ,       e   .   g   .   ,      S   h   o   t   t   o   n       e   t       a   l   .       [   2   5   ]       o   b   t   a   i   n   e   d       v   e   r   y       g   o   o   d       p   o   s   e       e   s   t   i   m   a   t   i   o   n       r   e   s   u   l   t   s       f   r   o   m       d   e   p   t   h       i   m   a   g   e   s       b   y       o   n   l   y       3       d   e   c   i   s   i   o   n      t   r   e   e   s   .      T   r   e   e       d   e   p   t   h   .       T   r   e   e       d   e   p   t   h       i   s       a   n   o   t   h   e   r       i   m   p   o   r   t   a   n   t       p   a   r   a   m   e   t   e   r       f   o   r       d   e   c   i   s   i   o   n       t   r   e   e   s   .       I   n       L   D   L   F   s   ,       t   h   e   r   e       i   s       a   n      i   m   p   l   i   c   i   t       c   o   n   s   t   r   a   i   n   t       b   e   t   w   e   e   n       t   r   e   e       d   e   p   t   h       h       a   n   d       o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       \\xcf\\x84       :      \\xcf\\x84       \\xe2\\x89\\xa5       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1   .       T   o       d   i   s   c   u   s   s       t   h   e       i   n   f   l   u   e   n   c   e       o   f       t   r   e   e       d   e   p   t   h       t   o       t   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       d   L   D   L   F   s   ,       w   e       s   e   t       \\xcf\\x84       =       2   h   \\xe2\\x88\\x92   1      a   n   d       f   i   x       t   r   e   e       n   u   m   b   e   r       K       =       1   ,       a   n   d       t   h   e       p   e   r   f   o   r   m   a   n   c   e       c   h   a   n   g   e       b   y       v   a   r   y   i   n   g       t   r   e   e       d   e   p   t   h       i   s       s   h   o   w   n       i   n       F   i   g   .       4       (   b   )   .      W   e       s   e   e       t   h   a   t       t   h   e       p   e   r   f   o   r   m   a   n   c   e       f   i   r   s   t       i   m   p   r   o   v   e   s       t   h   e   n       d   e   c   r   e   a   s   e   s       w   i   t   h       t   h   e       i   n   c   r   e   a   s   e       o   f       t   h   e       t   r   e   e       d   e   p   t   h   .       T   h   e      r   e   a   s   o   n       i   s       a   s       t   h   e       t   r   e   e       d   e   p   t   h       i   n   c   r   e   a   s   e   s   ,       t   h   e       d   i   m   e   n   s   i   o   n       o   f       l   e   a   r   n   e   d       f   e   a   t   u   r   e   s       i   n   c   r   e   a   s   e   s       e   x   p   o   n   e   n   t   i   a   l   l   y   ,       w   h   i   c   h      g   r   e   a   t   l   y       i   n   c   r   e   a   s   e   s       t   h   e       t   r   a   i   n   i   n   g       d   i   f   f   i   c   u   l   t   y   .       S   o       u   s   i   n   g       m   u   c   h       l   a   r   g   e   r       d   e   p   t   h   s       m   a   y       l   e   a   d       t   o       b   a   d       p   e   r   f   o   r   m   a   n   c   e      (   O   n       M   o   v   i   e   ,       t   r   e   e       d   e   p   t   h       h       =       1   8   :       K   -   L       =       0   .   1   1   6   2       v   s       h       =       9   :       K   -   L       =       0   .   0   8   3   1   )   .         F   i   g   u   r   e       4   :       T   h   e       p   e   r   f   o   r   m   a   n   c   e       c   h   a   n   g   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       o   n       M   o   r   p   h   _   S   u   b       a   n   d       r   a   t   i   n   g       p   r   e   d   i   c   t   i   o   n       o   n       M   o   v   i   e      b   y       v   a   r   y   i   n   g       (   a   )       t   r   e   e       n   u   m   b   e   r       a   n   d       (   b   )       t   r   e   e       d   e   p   t   h   .       O   u   r       a   p   p   r   o   a   c   h       (   d   L   D   L   F   s   /   s   L   D   L   F   s   )       c   a   n       i   m   p   r   o   v   e       t   h   e      p   e   r   f   o   r   m   a   n   c   e       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       w   h   i   l   e       u   s   i   n   g       t   h   e       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       p   r   o   p   o   s   e   d       i   n       d   N   D   F   s       (   d   N   D   F   s   L   D   L   /   s   N   D   F   s   -   L   D   L   )       e   v   e   n       l   e   a   d   s       t   o       a       w   o   r   s   e       p   e   r   f   o   r   m   a   n   c   e       t   h   a   n       o   n   e       f   o   r       a       s   i   n   g   l   e       t   r   e   e   .         5         C   o   n   c   l   u   s   i   o   n         W   e       p   r   e   s   e   n   t       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s   ,       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       i   n   s   p   i   r   e   d       b   y      d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   .       W   e       d   e   f   i   n   e   d       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       t   h   e       f   o   r   e   s   t   s       a   n   d       f   o   u   n   d      t   h   a   t       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       c   a   n       b   e       o   p   t   i   m   i   z   e   d       v   i   a       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   ,       w   h   i   c   h       e   n   a   b   l   e   s       a   l   l       t   h   e       t   r   e   e   s      a   n   d       t   h   e       f   e   a   t   u   r   e       t   h   e   y       u   s   e       t   o       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       E   x   p   e   r   i   m   e   n   t   a   l       r   e   s   u   l   t   s       s   h   o   w   e   d      t   h   e       s   u   p   e   r   i   o   r   i   t   y       o   f       o   u   r       a   l   g   o   r   i   t   h   m       f   o   r       s   e   v   e   r   a   l       L   D   L       t   a   s   k   s       a   n   d       a       r   e   l   a   t   e   d       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       a   n   d      v   e   r   i   f   i   e   d       o   u   r       m   o   d   e   l       h   a   s       t   h   e       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .      A   c   k   n   o   w   l   e   d   g   e   m   e   n   t   .       T   h   i   s       w   o   r   k       w   a   s       s   u   p   p   o   r   t   e   d       i   n       p   a   r   t       b   y       t   h   e       N   a   t   i   o   n   a   l       N   a   t   u   r   a   l       S   c   i   e   n   c   e       F   o   u   n   d   a   t   i   o   n       o   f      C   h   i   n   a       N   o   .       6   1   6   7   2   3   3   6   ,       i   n       p   a   r   t       b   y       \\xe2\\x80\\x9c   C   h   e   n       G   u   a   n   g   \\xe2\\x80\\x9d       p   r   o   j   e   c   t       s   u   p   p   o   r   t   e   d       b   y       S   h   a   n   g   h   a   i       M   u   n   i   c   i   p   a   l       E   d   u   c   a   t   i   o   n      C   o   m   m   i   s   s   i   o   n       a   n   d       S   h   a   n   g   h   a   i       E   d   u   c   a   t   i   o   n       D   e   v   e   l   o   p   m   e   n   t       F   o   u   n   d   a   t   i   o   n       N   o   .       1   5   C   G   4   3       a   n   d       i   n       p   a   r   t       b   y       O   N   R      N   0   0   0   1   4   -   1   5   -   1   -   2   3   5   6   .         R   e   f   e   r   e   n   c   e   s      [   1   ]       Y   .       A   m   i   t       a   n   d       D   .       G   e   m   a   n   .       S   h   a   p   e       q   u   a   n   t   i   z   a   t   i   o   n       a   n   d       r   e   c   o   g   n   i   t   i   o   n       w   i   t   h       r   a   n   d   o   m   i   z   e   d       t   r   e   e   s   .       N   e   u   r   a   l       C   o   m   p   u   t   a   t   i   o   n   ,      9   (   7   )   :   1   5   4   5   \\xe2\\x80\\x93   1   5   8   8   ,       1   9   9   7   .      [   2   ]       A   .       L   .       B   e   r   g   e   r   ,       S   .       D   .       P   i   e   t   r   a   ,       a   n   d       V   .       J   .       D   .       P   i   e   t   r   a   .       A       m   a   x   i   m   u   m       e   n   t   r   o   p   y       a   p   p   r   o   a   c   h       t   o       n   a   t   u   r   a   l       l   a   n   g   u   a   g   e       p   r   o   c   e   s   s   i   n   g   .      C   o   m   p   u   t   a   t   i   o   n   a   l       L   i   n   g   u   i   s   t   i   c   s   ,       2   2   (   1   )   :   3   9   \\xe2\\x80\\x93   7   1   ,       1   9   9   6   .      [   3   ]       L   .       B   r   e   i   m   a   n   .       R   a   n   d   o   m       f   o   r   e   s   t   s   .       M   a   c   h   i   n   e       L   e   a   r   n   i   n   g   ,       4   5   (   1   )   :   5   \\xe2\\x80\\x93   3   2   ,       2   0   0   1   .      [   4   ]       A   .       C   r   i   m   i   n   i   s   i       a   n   d       J   .       S   h   o   t   t   o   n   .       D   e   c   i   s   i   o   n       F   o   r   e   s   t   s       f   o   r       C   o   m   p   u   t   e   r       V   i   s   i   o   n       a   n   d       M   e   d   i   c   a   l       I   m   a   g   e       A   n   a   l   y   s   i   s   .       S   p   r   i   n   g   e   r   ,      2   0   1   3   .      [   5   ]       B   .   -   B   .       G   a   o   ,       C   .       X   i   n   g   ,       C   .   -   W   .       X   i   e   ,       J   .       W   u   ,       a   n   d       X   .       G   e   n   g   .       D   e   e   p       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       w   i   t   h       l   a   b   e   l       a   m   b   i   g   u   i   t   y   .      a   r   X   i   v   :   1   6   1   1   .   0   1   7   3   1   ,       2   0   1   7   .      [   6   ]       X   .       G   e   n   g   .       L   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       I   E   E   E       T   r   a   n   s   .       K   n   o   w   l   .       D   a   t   a       E   n   g   .   ,       2   8   (   7   )   :   1   7   3   4   \\xe2\\x80\\x93   1   7   4   8   ,       2   0   1   6   .         9         \\x0c   [   7   ]       X   .       G   e   n   g       a   n   d       P   .       H   o   u   .       P   r   e   -   r   e   l   e   a   s   e       p   r   e   d   i   c   t   i   o   n       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       m   o   v   i   e   s       b   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       I   n      P   r   o   .       I   J   C   A   I   ,       p   a   g   e   s       3   5   1   1   \\xe2\\x80\\x93   3   5   1   7   ,       2   0   1   5   .      [   8   ]       X   .       G   e   n   g   ,       K   .       S   m   i   t   h   -   M   i   l   e   s   ,       a   n   d       Z   .       Z   h   o   u   .       F   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       l   e   a   r   n   i   n   g       f   r   o   m       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       I   n       P   r   o   c   .      A   A   A   I   ,       2   0   1   0   .      [   9   ]       X   .       G   e   n   g   ,       Q   .       W   a   n   g   ,       a   n   d       Y   .       X   i   a   .       F   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       a   d   a   p   t   i   v   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       I   n       P   r   o   c   .      I   C   P   R   ,       p   a   g   e   s       4   4   6   5   \\xe2\\x80\\x93   4   4   7   0   ,       2   0   1   4   .      [   1   0   ]       X   .       G   e   n   g       a   n   d       Y   .       X   i   a   .       H   e   a   d       p   o   s   e       e   s   t   i   m   a   t   i   o   n       b   a   s   e   d       o   n       m   u   l   t   i   v   a   r   i   a   t   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   .       I   n       P   r   o   c   .       C   V   P   R   ,       p   a   g   e   s      1   8   3   7   \\xe2\\x80\\x93   1   8   4   2   ,       2   0   1   4   .      [   1   1   ]       X   .       G   e   n   g   ,       C   .       Y   i   n   ,       a   n   d       Z   .       Z   h   o   u   .       F   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       l   e   a   r   n   i   n   g       f   r   o   m       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       I   E   E   E       T   r   a   n   s   .      P   a   t   t   e   r   n       A   n   a   l   .       M   a   c   h   .       I   n   t   e   l   l   .   ,       3   5   (   1   0   )   :   2   4   0   1   \\xe2\\x80\\x93   2   4   1   2   ,       2   0   1   3   .      [   1   2   ]       G   .       G   u   o   ,       Y   .       F   u   ,       C   .       R   .       D   y   e   r   ,       a   n   d       T   .       S   .       H   u   a   n   g   .       I   m   a   g   e   -   b   a   s   e   d       h   u   m   a   n       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       m   a   n   i   f   o   l   d       l   e   a   r   n   i   n   g       a   n   d      l   o   c   a   l   l   y       a   d   j   u   s   t   e   d       r   o   b   u   s   t       r   e   g   r   e   s   s   i   o   n   .       I   E   E   E       T   r   a   n   s   .       I   m   a   g   e       P   r   o   c   e   s   s   i   n   g   ,       1   7   (   7   )   :   1   1   7   8   \\xe2\\x80\\x93   1   1   8   8   ,       2   0   0   8   .      [   1   3   ]       G   .       G   u   o       a   n   d       G   .       M   u   .       H   u   m   a   n       a   g   e       e   s   t   i   m   a   t   i   o   n   :       W   h   a   t       i   s       t   h   e       i   n   f   l   u   e   n   c   e       a   c   r   o   s   s       r   a   c   e       a   n   d       g   e   n   d   e   r   ?       I   n       C   V   P   R      W   o   r   k   s   h   o   p   s   ,       p   a   g   e   s       7   1   \\xe2\\x80\\x93   7   8   ,       2   0   1   0   .      [   1   4   ]       G   .       G   u   o       a   n   d       C   .       Z   h   a   n   g   .       A       s   t   u   d   y       o   n       c   r   o   s   s   -   p   o   p   u   l   a   t   i   o   n       a   g   e       e   s   t   i   m   a   t   i   o   n   .       I   n       P   r   o   c   .       C   V   P   R   ,       p   a   g   e   s       4   2   5   7   \\xe2\\x80\\x93   4   2   6   3   ,      2   0   1   4   .      [   1   5   ]       Z   .       H   e   ,       X   .       L   i   ,       Z   .       Z   h   a   n   g   ,       F   .       W   u   ,       X   .       G   e   n   g   ,       Y   .       Z   h   a   n   g   ,       M   .   -   H   .       Y   a   n   g   ,       a   n   d       Y   .       Z   h   u   a   n   g   .       D   a   t   a   -   d   e   p   e   n   d   e   n   t       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r       a   g   e       e   s   t   i   m   a   t   i   o   n   .       I   E   E   E       T   r   a   n   s   .       o   n       I   m   a   g   e       P   r   o   c   e   s   s   i   n   g   ,       2   0   1   7   .      [   1   6   ]       T   .       K   .       H   o   .       R   a   n   d   o   m       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       I   n       P   r   o   c   .       I   C   D   A   R   ,       p   a   g   e   s       2   7   8   \\xe2\\x80\\x93   2   8   2   ,       1   9   9   5   .      [   1   7   ]       T   .       K   .       H   o   .       T   h   e       r   a   n   d   o   m       s   u   b   s   p   a   c   e       m   e   t   h   o   d       f   o   r       c   o   n   s   t   r   u   c   t   i   n   g       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       I   E   E   E       T   r   a   n   s   .       P   a   t   t   e   r   n       A   n   a   l   .       M   a   c   h   .      I   n   t   e   l   l   .   ,       2   0   (   8   )   :   8   3   2   \\xe2\\x80\\x93   8   4   4   ,       1   9   9   8   .      [   1   8   ]       Y   .       J   i   a   ,       E   .       S   h   e   l   h   a   m   e   r   ,       J   .       D   o   n   a   h   u   e   ,       S   .       K   a   r   a   y   e   v   ,       J   .       L   o   n   g   ,       R   .       G   i   r   s   h   i   c   k   ,       S   .       G   u   a   d   a   r   r   a   m   a   ,       a   n   d       T   .       D   a   r   r   e   l   l   .       C   a   f   f   e   :      C   o   n   v   o   l   u   t   i   o   n   a   l       a   r   c   h   i   t   e   c   t   u   r   e       f   o   r       f   a   s   t       f   e   a   t   u   r   e       e   m   b   e   d   d   i   n   g   .       a   r   X   i   v       p   r   e   p   r   i   n   t       a   r   X   i   v   :   1   4   0   8   .   5   0   9   3   ,       2   0   1   4   .      [   1   9   ]       M   .       I   .       J   o   r   d   a   n   ,       Z   .       G   h   a   h   r   a   m   a   n   i   ,       T   .       S   .       J   a   a   k   k   o   l   a   ,       a   n   d       L   .       K   .       S   a   u   l   .       A   n       i   n   t   r   o   d   u   c   t   i   o   n       t   o       v   a   r   i   a   t   i   o   n   a   l       m   e   t   h   o   d   s       f   o   r      g   r   a   p   h   i   c   a   l       m   o   d   e   l   s   .       M   a   c   h   i   n   e       L   e   a   r   n   i   n   g   ,       3   7   (   2   )   :   1   8   3   \\xe2\\x80\\x93   2   3   3   ,       1   9   9   9   .      [   2   0   ]       P   .       K   o   n   t   s   c   h   i   e   d   e   r   ,       M   .       F   i   t   e   r   a   u   ,       A   .       C   r   i   m   i   n   i   s   i   ,       a   n   d       S   .       R   .       B   u   l   \\xc3\\xb2   .       D   e   e   p       n   e   u   r   a   l       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       I   n       P   r   o   c   .       I   C   C   V   ,      p   a   g   e   s       1   4   6   7   \\xe2\\x80\\x93   1   4   7   5   ,       2   0   1   5   .      [   2   1   ]       A   .       K   r   i   z   h   e   v   s   k   y   ,       I   .       S   u   t   s   k   e   v   e   r   ,       a   n   d       G   .       E   .       H   i   n   t   o   n   .       I   m   a   g   e   n   e   t       c   l   a   s   s   i   f   i   c   a   t   i   o   n       w   i   t   h       d   e   e   p       c   o   n   v   o   l   u   t   i   o   n   a   l       n   e   u   r   a   l      n   e   t   w   o   r   k   s   .       I   n       P   r   o   c   .       N   I   P   S   ,       p   a   g   e   s       1   1   0   6   \\xe2\\x80\\x93   1   1   1   4   ,       2   0   1   2   .      [   2   2   ]       A   .       L   a   n   i   t   i   s   ,       C   .       D   r   a   g   a   n   o   v   a   ,       a   n   d       C   .       C   h   r   i   s   t   o   d   o   u   l   o   u   .       C   o   m   p   a   r   i   n   g       d   i   f   f   e   r   e   n   t       c   l   a   s   s   i   f   i   e   r   s       f   o   r       a   u   t   o   m   a   t   i   c       a   g   e      e   s   t   i   m   a   t   i   o   n   .       I   E   E   E       T   r   a   n   s   .       o   n       C   y   b   e   r   n   e   t   i   c   s   ,   ,       3   4   (   1   )   :   6   2   1   \\xe2\\x80\\x93   6   2   8   ,       2   0   0   4   .      [   2   3   ]       O   .       M   .       P   a   r   k   h   i   ,       A   .       V   e   d   a   l   d   i   ,       a   n   d       A   .       Z   i   s   s   e   r   m   a   n   .       D   e   e   p       f   a   c   e       r   e   c   o   g   n   i   t   i   o   n   .       I   n       P   r   o   c   .       B   M   V   C   ,       p   a   g   e   s       4   1   .   1   \\xe2\\x80\\x93   4   1   .   1   2   ,      2   0   1   5   .      [   2   4   ]       K   .       R   i   c   a   n   e   k       a   n   d       T   .       T   e   s   a   f   a   y   e   .       M   O   R   P   H   :       A       l   o   n   g   i   t   u   d   i   n   a   l       i   m   a   g   e       d   a   t   a   b   a   s   e       o   f       n   o   r   m   a   l       a   d   u   l   t       a   g   e   -   p   r   o   g   r   e   s   s   i   o   n   .       I   n      P   r   o   c   .       F   G   ,       p   a   g   e   s       3   4   1   \\xe2\\x80\\x93   3   4   5   ,       2   0   0   6   .      [   2   5   ]       J   .       S   h   o   t   t   o   n   ,       A   .       W   .       F   i   t   z   g   i   b   b   o   n   ,       M   .       C   o   o   k   ,       T   .       S   h   a   r   p   ,       M   .       F   i   n   o   c   c   h   i   o   ,       R   .       M   o   o   r   e   ,       A   .       K   i   p   m   a   n   ,       a   n   d       A   .       B   l   a   k   e   .      R   e   a   l   -   t   i   m   e       h   u   m   a   n       p   o   s   e       r   e   c   o   g   n   i   t   i   o   n       i   n       p   a   r   t   s       f   r   o   m       s   i   n   g   l   e       d   e   p   t   h       i   m   a   g   e   s   .       I   n       P   r   o   c   .       C   V   P   R   ,       p   a   g   e   s       1   2   9   7   \\xe2\\x80\\x93   1   3   0   4   ,      2   0   1   1   .      [   2   6   ]       G   .       T   s   o   u   m   a   k   a   s       a   n   d       I   .       K   a   t   a   k   i   s   .       M   u   l   t   i   -   l   a   b   e   l       c   l   a   s   s   i   f   i   c   a   t   i   o   n   :       A   n       o   v   e   r   v   i   e   w   .       I   n   t   e   r   n   a   t   i   o   n   a   l       J   o   u   r   n   a   l       o   f       D   a   t   a      W   a   r   e   h   o   u   s   i   n   g       a   n   d       M   i   n   i   n   g   ,       3   (   3   )   :   1   \\xe2\\x80\\x93   1   3   ,       2   0   0   7   .      [   2   7   ]       C   .       X   i   n   g   ,       X   .       G   e   n   g   ,       a   n   d       H   .       X   u   e   .       L   o   g   i   s   t   i   c       b   o   o   s   t   i   n   g       r   e   g   r   e   s   s   i   o   n       f   o   r       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       I   n       P   r   o   c   .       C   V   P   R   ,      p   a   g   e   s       4   4   8   9   \\xe2\\x80\\x93   4   4   9   7   ,       2   0   1   6   .      [   2   8   ]       X   .       Y   a   n   g   ,       X   .       G   e   n   g   ,       a   n   d       D   .       Z   h   o   u   .       S   p   a   r   s   i   t   y       c   o   n   d   i   t   i   o   n   a   l       e   n   e   r   g   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r       a   g   e       e   s   t   i   m   a   t   i   o   n   .      I   n       P   r   o   c   .       I   J   C   A   I   ,       p   a   g   e   s       2   2   5   9   \\xe2\\x80\\x93   2   2   6   5   ,       2   0   1   6   .      [   2   9   ]       A   .       L   .       Y   u   i   l   l   e       a   n   d       A   .       R   a   n   g   a   r   a   j   a   n   .       T   h   e       c   o   n   c   a   v   e   -   c   o   n   v   e   x       p   r   o   c   e   d   u   r   e   .       N   e   u   r   a   l       C   o   m   p   u   t   a   t   i   o   n   ,       1   5   (   4   )   :   9   1   5   \\xe2\\x80\\x93   9   3   6   ,      2   0   0   3   .      [   3   0   ]       Y   .       Z   h   o   u   ,       H   .       X   u   e   ,       a   n   d       X   .       G   e   n   g   .       E   m   o   t   i   o   n       d   i   s   t   r   i   b   u   t   i   o   n       r   e   c   o   g   n   i   t   i   o   n       f   r   o   m       f   a   c   i   a   l       e   x   p   r   e   s   s   i   o   n   s   .       I   n       P   r   o   c   .       M   M   ,      p   a   g   e   s       1   2   4   7   \\xe2\\x80\\x93   1   2   5   0   ,       2   0   1   5   .         1   0         \\x0c   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.14\n",
      "XGBoost Accuracy on Test set -> 0.22\n",
      "RandomForest Accuracy on Test set -> 0.3\n",
      "DecisionTree Accuracy on Test set -> 0.18\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s         w   e   i       s   h   e   n   1   ,   2       ,       k   a   i       z   h   a   o   1       ,       y   i   l   u       g   u   o   1       ,       a   l   a   n       y   u   i   l   l   e   2      k   e   y       l   a   b   o   r   a   t   o   r   y       o   f       s   p   e   c   i   a   l   t   y       f   i   b   e   r       o   p   t   i   c   s       a   n   d       o   p   t   i   c   a   l       a   c   c   e   s   s       n   e   t   w   o   r   k   s   ,      s   h   a   n   g   h   a   i       i   n   s   t   i   t   u   t   e       f   o   r       a   d   v   a   n   c   e   d       c   o   m   m   u   n   i   c   a   t   i   o   n       a   n   d       d   a   t   a       s   c   i   e   n   c   e   ,      s   c   h   o   o   l       o   f       c   o   m   m   u   n   i   c   a   t   i   o   n       a   n   d       i   n   f   o   r   m   a   t   i   o   n       e   n   g   i   n   e   e   r   i   n   g   ,       s   h   a   n   g   h   a   i       u   n   i   v   e   r   s   i   t   y      2      d   e   p   a   r   t   m   e   n   t       o   f       c   o   m   p   u   t   e   r       s   c   i   e   n   c   e   ,       j   o   h   n   s       h   o   p   k   i   n   s       u   n   i   v   e   r   s   i   t   y         a   r   x   i   v   :   1   7   0   2   .   0   6   0   8   6   v   4       [   c   s   .   l   g   ]       1   6       o   c   t       2   0   1   7         1         {   s   h   e   n   w   e   i   1   2   3   1   ,   z   h   a   o   k   1   2   0   6   ,   g   y   l   .   l   u   a   n   0   ,   a   l   a   n   .   l   .   y   u   i   l   l   e   }   @   g   m   a   i   l   .   c   o   m         a   b   s   t   r   a   c   t      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   l   d   l   )       i   s       a       g   e   n   e   r   a   l       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k   ,       w   h   i   c   h       a   s   s   i   g   n   s      t   o       a   n       i   n   s   t   a   n   c   e       a       d   i   s   t   r   i   b   u   t   i   o   n       o   v   e   r       a       s   e   t       o   f       l   a   b   e   l   s       r   a   t   h   e   r       t   h   a   n       a       s   i   n   g   l   e       l   a   b   e   l       o   r       m   u   l   t   i   p   l   e      l   a   b   e   l   s   .       c   u   r   r   e   n   t       l   d   l       m   e   t   h   o   d   s       h   a   v   e       e   i   t   h   e   r       r   e   s   t   r   i   c   t   e   d       a   s   s   u   m   p   t   i   o   n   s       o   n       t   h   e       e   x   p   r   e   s   s   i   o   n      f   o   r   m       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       o   r       l   i   m   i   t   a   t   i   o   n   s       i   n       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   o      l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       t   h   i   s       p   a   p   e   r       p   r   e   s   e   n   t   s       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      l   e   a   r   n   i   n   g       f   o   r   e   s   t   s       (   l   d   l   f   s   )       -       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       b   a   s   e   d       o   n      d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   ,       w   h   i   c   h       h   a   v   e       s   e   v   e   r   a   l       a   d   v   a   n   t   a   g   e   s   :       1   )       d   e   c   i   s   i   o   n       t   r   e   e   s      h   a   v   e       t   h   e       p   o   t   e   n   t   i   a   l       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       b   y       a       m   i   x   t   u   r   e      o   f       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   .       2   )       t   h   e       l   e   a   r   n   i   n   g       o   f       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       c   a   n       b   e      c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   .       w   e       d   e   f   i   n   e       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n      f   o   r       a       f   o   r   e   s   t   ,       e   n   a   b   l   i   n   g       a   l   l       t   h   e       t   r   e   e   s       t   o       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   ,       a   n   d       s   h   o   w       t   h   a   t       a   n       u   p   d   a   t   e      f   u   n   c   t   i   o   n       f   o   r       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       w   h   i   c   h       g   u   a   r   a   n   t   e   e   s       a       s   t   r   i   c   t       d   e   c   r   e   a   s   e       o   f       t   h   e       l   o   s   s      f   u   n   c   t   i   o   n   ,       c   a   n       b   e       d   e   r   i   v   e   d       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   .       t   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       t   h   e       p   r   o   p   o   s   e   d      l   d   l   f   s       i   s       v   e   r   i   f   i   e   d       o   n       s   e   v   e   r   a   l       l   d   l       t   a   s   k   s       a   n   d       a       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       s   h   o   w   i   n   g      s   i   g   n   i   f   i   c   a   n   t       i   m   p   r   o   v   e   m   e   n   t   s       t   o       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       l   d   l       m   e   t   h   o   d   s   .         1         i   n   t   r   o   d   u   c   t   i   o   n         l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   l   d   l   )       [   6   ,       1   1   ]       i   s       a       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k       t   o       d   e   a   l       w   i   t   h       p   r   o   b   l   e   m   s       o   f       l   a   b   e   l      a   m   b   i   g   u   i   t   y   .       u   n   l   i   k   e       s   i   n   g   l   e   -   l   a   b   e   l       l   e   a   r   n   i   n   g       (   s   l   l   )       a   n   d       m   u   l   t   i   -   l   a   b   e   l       l   e   a   r   n   i   n   g       (   m   l   l   )       [   2   6   ]   ,       w   h   i   c   h       a   s   s   u   m   e       a   n      i   n   s   t   a   n   c   e       i   s       a   s   s   i   g   n   e   d       t   o       a       s   i   n   g   l   e       l   a   b   e   l       o   r       m   u   l   t   i   p   l   e       l   a   b   e   l   s   ,       l   d   l       a   i   m   s       a   t       l   e   a   r   n   i   n   g       t   h   e       r   e   l   a   t   i   v   e       i   m   p   o   r   t   a   n   c   e      o   f       e   a   c   h       l   a   b   e   l       i   n   v   o   l   v   e   d       i   n       t   h   e       d   e   s   c   r   i   p   t   i   o   n       o   f       a   n       i   n   s   t   a   n   c   e   ,       i   .   e   .   ,       a       d   i   s   t   r   i   b   u   t   i   o   n       o   v   e   r       t   h   e       s   e   t       o   f       l   a   b   e   l   s   .       s   u   c   h      a       l   e   a   r   n   i   n   g       s   t   r   a   t   e   g   y       i   s       s   u   i   t   a   b   l   e       f   o   r       m   a   n   y       r   e   a   l   -   w   o   r   l   d       p   r   o   b   l   e   m   s   ,       w   h   i   c   h       h   a   v   e       l   a   b   e   l       a   m   b   i   g   u   i   t   y   .       a   n       e   x   a   m   p   l   e      i   s       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       [   8   ]   .       e   v   e   n       h   u   m   a   n   s       c   a   n   n   o   t       p   r   e   d   i   c   t       t   h   e       p   r   e   c   i   s   e       a   g   e       f   r   o   m       a       s   i   n   g   l   e       f   a   c   i   a   l       i   m   a   g   e   .      t   h   e   y       m   a   y       s   a   y       t   h   a   t       t   h   e       p   e   r   s   o   n       i   s       p   r   o   b   a   b   l   y       i   n       o   n   e       a   g   e       g   r   o   u   p       a   n   d       l   e   s   s       l   i   k   e   l   y       t   o       b   e       i   n       a   n   o   t   h   e   r   .       h   e   n   c   e       i   t       i   s      m   o   r   e       n   a   t   u   r   a   l       t   o       a   s   s   i   g   n       a       d   i   s   t   r   i   b   u   t   i   o   n       o   f       a   g   e       l   a   b   e   l   s       t   o       e   a   c   h       f   a   c   i   a   l       i   m   a   g   e       (   f   i   g   .       1   (   a   )   )       i   n   s   t   e   a   d       o   f       u   s   i   n   g       a      s   i   n   g   l   e       a   g   e       l   a   b   e   l   .       a   n   o   t   h   e   r       e   x   a   m   p   l   e       i   s       m   o   v   i   e       r   a   t   i   n   g       p   r   e   d   i   c   t   i   o   n       [   7   ]   .       m   a   n   y       f   a   m   o   u   s       m   o   v   i   e       r   e   v   i   e   w       w   e   b      s   i   t   e   s   ,       s   u   c   h       a   s       n   e   t   f   l   i   x   ,       i   m   d   b       a   n   d       d   o   u   b   a   n   ,       p   r   o   v   i   d   e       a       c   r   o   w   d       o   p   i   n   i   o   n       f   o   r       e   a   c   h       m   o   v   i   e       s   p   e   c   i   f   i   e   d       b   y       t   h   e      d   i   s   t   r   i   b   u   t   i   o   n       o   f       r   a   t   i   n   g   s       c   o   l   l   e   c   t   e   d       f   r   o   m       t   h   e   i   r       u   s   e   r   s       (   f   i   g   .       1   (   b   )   )   .       i   f       a       s   y   s   t   e   m       c   o   u   l   d       p   r   e   c   i   s   e   l   y       p   r   e   d   i   c   t       s   u   c   h       a      r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       e   v   e   r   y       m   o   v   i   e       b   e   f   o   r   e       i   t       i   s       r   e   l   e   a   s   e   d   ,       m   o   v   i   e       p   r   o   d   u   c   e   r   s       c   a   n       r   e   d   u   c   e       t   h   e   i   r       i   n   v   e   s   t   m   e   n   t      r   i   s   k       a   n   d       t   h   e       a   u   d   i   e   n   c   e       c   a   n       b   e   t   t   e   r       c   h   o   o   s   e       w   h   i   c   h       m   o   v   i   e   s       t   o       w   a   t   c   h   .      m   a   n   y       l   d   l       m   e   t   h   o   d   s       a   s   s   u   m   e       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       c   a   n       b   e       r   e   p   r   e   s   e   n   t   e   d       b   y       a       m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       [   2   ]      a   n   d       l   e   a   r   n       i   t       b   y       o   p   t   i   m   i   z   i   n   g       a   n       e   n   e   r   g   y       f   u   n   c   t   i   o   n       b   a   s   e   d       o   n       t   h   e       m   o   d   e   l       [   8   ,       1   1   ,       2   8   ,       6   ]   .       b   u   t   ,       t   h   e       e   x   p   o   n   e   n   t   i   a   l      p   a   r   t       o   f       t   h   i   s       m   o   d   e   l       r   e   s   t   r   i   c   t   s       t   h   e       g   e   n   e   r   a   l   i   t   y       o   f       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r   m   ,       e   .   g   .   ,       i   t       h   a   s       d   i   f   f   i   c   u   l   t   y       i   n       r   e   p   r   e   s   e   n   t   i   n   g      m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   .       s   o   m   e       o   t   h   e   r       l   d   l       m   e   t   h   o   d   s       e   x   t   e   n   d       t   h   e       e   x   i   s   t   i   n   g       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s   ,       e   .   g   ,       b   y      b   o   o   s   t   i   n   g       a   n   d       s   u   p   p   o   r   t       v   e   c   t   o   r       r   e   g   r   e   s   s   i   o   n   ,       t   o       d   e   a   l       w   i   t   h       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       [   7   ,       2   7   ]   ,       w   h   i   c   h       a   v   o   i   d       m   a   k   i   n   g      t   h   i   s       a   s   s   u   m   p   t   i   o   n   ,       b   u   t       h   a   v   e       l   i   m   i   t   a   t   i   o   n   s       i   n       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   h   e   y       d   o       n   o   t       l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s      i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .      3   1   s   t       c   o   n   f   e   r   e   n   c   e       o   n       n   e   u   r   a   l       i   n   f   o   r   m   a   t   i   o   n       p   r   o   c   e   s   s   i   n   g       s   y   s   t   e   m   s       (   n   i   p   s       2   0   1   7   )   ,       l   o   n   g       b   e   a   c   h   ,       c   a   ,       u   s   a   .         \\x0c   f   i   g   u   r   e       1   :       t   h   e       r   e   a   l   -   w   o   r   l   d       d   a   t   a       w   h   i   c   h       a   r   e       s   u   i   t   a   b   l   e       t   o       b   e       m   o   d   e   l   e   d       b   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       (   a   )      e   s   t   i   m   a   t   e   d       f   a   c   i   a   l       a   g   e   s       (   a       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   )   .       (   b   )       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       a       m   o   v   i   e      (   a       m   u   l   t   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   )   .         i   n       t   h   i   s       p   a   p   e   r   ,       w   e       p   r   e   s   e   n   t       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s       (   l   d   l   f   s   )       -       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       i   n   s   p   i   r   e   d       b   y       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       [   2   0   ]   .       e   x   t   e   n   d   i   n   g       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n      t   r   e   e   s       t   o       d   e   a   l       w   i   t   h       t   h   e       l   d   l       t   a   s   k       h   a   s       t   w   o       a   d   v   a   n   t   a   g   e   s   .       o   n   e       i   s       t   h   a   t       d   e   c   i   s   i   o   n       t   r   e   e   s       h   a   v   e       t   h   e       p   o   t   e   n   t   i   a   l      t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       b   y       m   i   x   t   u   r   e       o   f       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       w   h   i   c   h      a   v   o   i   d       m   a   k   i   n   g       s   t   r   o   n   g       a   s   s   u   m   p   t   i   o   n       o   n       t   h   e       f   o   r   m       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       t   h   e       s   e   c   o   n   d       i   s       t   h   a   t       t   h   e       s   p   l   i   t      n   o   d   e       p   a   r   a   m   e   t   e   r   s       i   n       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n   ,       w   h   i   c   h       e   n   a   b   l   e   s      a       c   o   m   b   i   n   a   t   i   o   n       o   f       t   r   e   e       l   e   a   r   n   i   n   g       a   n   d       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       w   e       d   e   f   i   n   e       a      d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       t   r   e   e       b   y       t   h   e       k   u   l   l   b   a   c   k   -   l   e   i   b   l   e   r       d   i   v   e   r   g   e   n   c   e       (   k   -   l   )       b   e   t   w   e   e   n       t   h   e      g   r   o   u   n   d       t   r   u   t   h       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       a   n   d       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       p   r   e   d   i   c   t   e   d       b   y       t   h   e       t   r   e   e   .       b   y       f   i   x   i   n   g       s   p   l   i   t       n   o   d   e   s   ,       w   e      s   h   o   w       t   h   a   t       t   h   e       o   p   t   i   m   i   z   a   t   i   o   n       o   f       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       t   o       m   i   n   i   m   i   z   e       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       o   f       t   h   e       t   r   e   e       c   a   n      b   e       a   d   d   r   e   s   s   e   d       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g       [   1   9   ,       2   9   ]   ,       i   n       w   h   i   c   h       t   h   e       o   r   i   g   i   n   a   l       l   o   s   s       f   u   n   c   t   i   o   n       t   o       b   e       m   i   n   i   m   i   z   e   d      g   e   t   s       i   t   e   r   a   t   i   v   e   l   y       r   e   p   l   a   c   e   d       b   y       a       d   e   c   r   e   a   s   i   n   g       s   e   q   u   e   n   c   e       o   f       u   p   p   e   r       b   o   u   n   d   s   .       f   o   l   l   o   w   i   n   g       t   h   i   s       o   p   t   i   m   i   z   a   t   i   o   n      s   t   r   a   t   e   g   y   ,       w   e       d   e   r   i   v   e       a       d   i   s   c   r   e   t   e       i   t   e   r   a   t   i   v   e       f   u   n   c   t   i   o   n       t   o       u   p   d   a   t   e       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   .       t   o       l   e   a   r   n       a       f   o   r   e   s   t   ,      w   e       a   v   e   r   a   g   e       t   h   e       l   o   s   s   e   s       o   f       a   l   l       t   h   e       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s       t   o       b   e       t   h   e       l   o   s   s       f   o   r       t   h   e       f   o   r   e   s   t       a   n   d       a   l   l   o   w       t   h   e       s   p   l   i   t       n   o   d   e   s      f   r   o   m       d   i   f   f   e   r   e   n   t       t   r   e   e   s       t   o       b   e       c   o   n   n   e   c   t   e   d       t   o       t   h   e       s   a   m   e       o   u   t   p   u   t       u   n   i   t       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   .       i   n       t   h   i   s      w   a   y   ,       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       o   f       a   l   l       t   h   e       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s       c   a   n       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   .       o   u   r       l   d   l   f   s       c   a   n       b   e      u   s   e   d       a   s       a       (   s   h   a   l   l   o   w   )       s   t   a   n   d   -   a   l   o   n   e       m   o   d   e   l   ,       a   n   d       c   a   n       a   l   s   o       b   e       i   n   t   e   g   r   a   t   e   d       w   i   t   h       a   n   y       d   e   e   p       n   e   t   w   o   r   k   s   ,       i   .   e   .   ,       t   h   e      f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       c   a   n       b   e       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n       a   n   d       a       d   e   e   p       n   e   t   w   o   r   k   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       f   i   g   .       2      i   l   l   u   s   t   r   a   t   e   s       a       s   k   e   t   c   h       c   h   a   r   t       o   f       o   u   r       l   d   l   f   s   ,       w   h   e   r   e       a       f   o   r   e   s   t       c   o   n   s   i   s   t   s       o   f       t   w   o       t   r   e   e   s       i   s       s   h   o   w   n   .      w   e       v   e   r   i   f   y       t   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       o   u   r       m   o   d   e   l       o   n       s   e   v   e   r   a   l       l   d   l       t   a   s   k   s   ,       s   u   c   h       a   s       c   r   o   w   d       o   p   i   n   i   o   n       p   r   e   d   i   c   t   i   o   n       o   n      m   o   v   i   e   s       a   n   d       d   i   s   e   a   s   e       p   r   e   d   i   c   t   i   o   n       b   a   s   e   d       o   n       h   u   m   a   n       g   e   n   e   s   ,       a   s       w   e   l   l       a   s       o   n   e       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       i   .   e   .   ,      f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n   ,       s   h   o   w   i   n   g       s   i   g   n   i   f   i   c   a   n   t       i   m   p   r   o   v   e   m   e   n   t   s       t   o       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       l   d   l       m   e   t   h   o   d   s   .       t   h   e      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       f   o   r       t   h   e   s   e       t   a   s   k   s       i   n   c   l   u   d   e       b   o   t   h       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   s       (   e   .   g   .   ,       t   h   e       a   g   e       d   i   s   t   r   i   b   u   t   i   o   n       i   n      f   i   g   .       1   (   a   )   )       a   n   d       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s       (   t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       o   n       a       m   o   v   i   e       i   n       f   i   g   .       1   (   b   )   )   .       t   h   e       s   u   p   e   r   i   o   r   i   t   y      o   f       o   u   r       m   o   d   e   l       o   n       b   o   t   h       o   f       t   h   e   m       v   e   r   i   f   i   e   s       i   t   s       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s         f   i   g   u   r   e       2   :       i   l   l   u   s   t   r   a   t   i   o   n       o   f       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   .       t   h   e       t   o   p       c   i   r   c   l   e   s       d   e   n   o   t   e       t   h   e       o   u   t   p   u   t       u   n   i   t   s      o   f       t   h   e       f   u   n   c   t   i   o   n       f       p   a   r   a   m   e   t   e   r   i   z   e   d       b   y       \\xce\\x98   ,       w   h   i   c   h       c   a   n       b   e       a       f   e   a   t   u   r   e       v   e   c   t   o   r       o   r       a       f   u   l   l   y   -   c   o   n   n   e   c   t   e   d       l   a   y   e   r       o   f      a       d   e   e   p       n   e   t   w   o   r   k   .       t   h   e       b   l   u   e       a   n   d       g   r   e   e   n       c   i   r   c   l   e   s       a   r   e       s   p   l   i   t       n   o   d   e   s       a   n   d       l   e   a   f       n   o   d   e   s   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       t   w   o       i   n   d   e   x      f   u   n   c   t   i   o   n       \\xcf\\x95   1       a   n   d       \\xcf\\x95   2       a   r   e       a   s   s   i   g   n   e   d       t   o       t   h   e   s   e       t   w   o       t   r   e   e   s       r   e   s   p   e   c   t   i   v   e   l   y   .       t   h   e       b   l   a   c   k       d   a   s   h       a   r   r   o   w   s       i   n   d   i   c   a   t   e       t   h   e      c   o   r   r   e   s   p   o   n   d   e   n   c   e       b   e   t   w   e   e   n       t   h   e       s   p   l   i   t       n   o   d   e   s       o   f       t   h   e   s   e       t   w   o       t   r   e   e   s       a   n   d       t   h   e       o   u   t   p   u   t       u   n   i   t   s       o   f       f   u   n   c   t   i   o   n       f       .       n   o   t   e      t   h   a   t   ,       o   n   e       o   u   t   p   u   t       u   n   i   t       m   a   y       c   o   r   r   e   s   p   o   n   d       t   o       t   h   e       s   p   l   i   t       n   o   d   e   s       b   e   l   o   n   g   i   n   g       t   o       d   i   f   f   e   r   e   n   t       t   r   e   e   s   .       e   a   c   h       t   r   e   e       h   a   s      i   n   d   e   p   e   n   d   e   n   t       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q       (   d   e   n   o   t   e   d       b   y       h   i   s   t   o   g   r   a   m   s       i   n       l   e   a   f       n   o   d   e   s   )   .       t   h   e       o   u   t   p   u   t       o   f       t   h   e       f   o   r   e   s   t      i   s       a       m   i   x   t   u   r   e       o   f       t   h   e       t   r   e   e       p   r   e   d   i   c   t   i   o   n   s   .       f       (   \\xc2\\xb7   ;       \\xce\\x98   )       a   n   d       q       a   r   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .         2         \\x0c   2         r   e   l   a   t   e   d       w   o   r   k         s   i   n   c   e       o   u   r       l   d   l       a   l   g   o   r   i   t   h   m       i   s       i   n   s   p   i   r   e   d       b   y       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   ,       i   t       i   s       n   e   c   e   s   s   a   r   y       t   o       f   i   r   s   t       r   e   v   i   e   w      s   o   m   e       t   y   p   i   c   a   l       t   e   c   h   n   i   q   u   e   s       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s   .       t   h   e   n   ,       w   e       d   i   s   c   u   s   s       c   u   r   r   e   n   t       l   d   l       m   e   t   h   o   d   s   .      d   e   c   i   s   i   o   n       t   r   e   e   s   .       r   a   n   d   o   m       f   o   r   e   s   t   s       o   r       r   a   n   d   o   m   i   z   e   d       d   e   c   i   s   i   o   n       t   r   e   e   s       [   1   6   ,       1   ,       3   ,       4   ]   ,       a   r   e       a       p   o   p   u   l   a   r       e   n   s   e   m   b   l   e      p   r   e   d   i   c   t   i   v   e       m   o   d   e   l       s   u   i   t   a   b   l   e       f   o   r       m   a   n   y       m   a   c   h   i   n   e       l   e   a   r   n   i   n   g       t   a   s   k   s   .       i   n       t   h   e       p   a   s   t   ,       l   e   a   r   n   i   n   g       o   f       a       d   e   c   i   s   i   o   n       t   r   e   e       w   a   s      b   a   s   e   d       o   n       h   e   u   r   i   s   t   i   c   s       s   u   c   h       a   s       a       g   r   e   e   d   y       a   l   g   o   r   i   t   h   m       w   h   e   r   e       l   o   c   a   l   l   y   -   o   p   t   i   m   a   l       h   a   r   d       d   e   c   i   s   i   o   n   s       a   r   e       m   a   d   e       a   t       e   a   c   h      s   p   l   i   t       n   o   d   e       [   1   ]   ,       a   n   d       t   h   u   s   ,       c   a   n   n   o   t       b   e       i   n   t   e   g   r   a   t   e   d       i   n   t   o       i   n       a       d   e   e   p       l   e   a   r   n   i   n   g       f   r   a   m   e   w   o   r   k   ,       i   .   e   .   ,       b   e       c   o   m   b   i   n   e   d      w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .      t   h   e       n   e   w   l   y       p   r   o   p   o   s   e   d       d   e   e   p       n   e   u   r   a   l       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s       (   d   n   d   f   s   )       [   2   0   ]       o   v   e   r   c   o   m   e   s       t   h   i   s       p   r   o   b   l   e   m       b   y       i   n   t   r   o   d   u   c   i   n   g      a       s   o   f   t       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       f   u   n   c   t   i   o   n       a   t       t   h   e       s   p   l   i   t       n   o   d   e   s       a   n   d       a       g   l   o   b   a   l       l   o   s   s       f   u   n   c   t   i   o   n       d   e   f   i   n   e   d       o   n       a       t   r   e   e   .      t   h   i   s       e   n   s   u   r   e   s       t   h   a   t       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n       a   n   d       l   e   a   f       n   o   d   e      p   r   e   d   i   c   t   i   o   n   s       c   a   n       b   e       u   p   d   a   t   e   d       b   y       a       d   i   s   c   r   e   t   e       i   t   e   r   a   t   i   v   e       f   u   n   c   t   i   o   n   .      o   u   r       m   e   t   h   o   d       e   x   t   e   n   d   s       d   n   d   f   s       t   o       a   d   d   r   e   s   s       l   d   l       p   r   o   b   l   e   m   s   ,       b   u   t       t   h   i   s       e   x   t   e   n   s   i   o   n       i   s       n   o   n   -   t   r   i   v   i   a   l   ,       b   e   c   a   u   s   e      l   e   a   r   n   i   n   g       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       i   s       a       c   o   n   s   t   r   a   i   n   e   d       c   o   n   v   e   x       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m   .       a   l   t   h   o   u   g   h       a       s   t   e   p   -   s   i   z   e      f   r   e   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       w   a   s       g   i   v   e   n       i   n       d   n   d   f   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s   ,       i   t       w   a   s       o   n   l   y       p   r   o   v   e   d       t   o      c   o   n   v   e   r   g   e       f   o   r       a       c   l   a   s   s   i   f   i   c   a   t   i   o   n       l   o   s   s   .       c   o   n   s   e   q   u   e   n   t   l   y   ,       i   t       w   a   s       u   n   c   l   e   a   r       h   o   w       t   o       o   b   t   a   i   n       s   u   c   h       a   n       u   p   d   a   t   e       f   u   n   c   t   i   o   n      f   o   r       o   t   h   e   r       l   o   s   s   e   s   .       w   e       o   b   s   e   r   v   e   d   ,       h   o   w   e   v   e   r   ,       t   h   a   t       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       i   n       d   n   d   f   s       c   a   n       b   e       d   e   r   i   v   e   d       f   r   o   m      v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   ,       w   h   i   c   h       a   l   l   o   w   s       u   s       t   o       e   x   t   e   n   d       i   t       t   o       o   u   r       l   d   l       l   o   s   s   .       i   n       a   d   d   i   t   i   o   n   ,       t   h   e       s   t   r   a   t   e   g   i   e   s       u   s   e   d       i   n      l   d   l   f   s       a   n   d       d   n   d   f   s       t   o       l   e   a   r   n   i   n   g       t   h   e       e   n   s   e   m   b   l   e       o   f       m   u   l   t   i   p   l   e       t   r   e   e   s       (   f   o   r   e   s   t   s   )       a   r   e       d   i   f   f   e   r   e   n   t   :       1   )       w   e       e   x   p   l   i   c   i   t   l   y      d   e   f   i   n   e       a       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       f   o   r   e   s   t   s   ,       w   h   i   l   e       o   n   l   y       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       s   i   n   g   l   e       t   r   e   e       w   a   s       d   e   f   i   n   e   d       i   n       d   n   d   f   s   ;      2   )       w   e       a   l   l   o   w       t   h   e       s   p   l   i   t       n   o   d   e   s       f   r   o   m       d   i   f   f   e   r   e   n   t       t   r   e   e   s       t   o       b   e       c   o   n   n   e   c   t   e   d       t   o       t   h   e       s   a   m   e       o   u   t   p   u   t       u   n   i   t       o   f       t   h   e       f   e   a   t   u   r   e      l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   ,       w   h   i   l   e       d   n   d   f   s       d   i   d       n   o   t   ;       3   )       a   l   l       t   r   e   e   s       i   n       l   d   l   f   s       c   a   n       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y   ,       w   h   i   l   e       t   r   e   e   s       i   n      d   n   d   f   s       w   e   r   e       l   e   a   r   n   e   d       a   l   t   e   r   n   a   t   i   v   e   l   y   .       t   h   e   s   e       c   h   a   n   g   e   s       i   n       t   h   e       e   n   s   e   m   b   l   e       l   e   a   r   n   i   n   g       a   r   e       i   m   p   o   r   t   a   n   t   ,       b   e   c   a   u   s   e       a   s      s   h   o   w   n       i   n       o   u   r       e   x   p   e   r   i   m   e   n   t   s       (   s   e   c   .       4   .   4   )   ,       l   d   l   f   s       c   a   n       g   e   t       b   e   t   t   e   r       r   e   s   u   l   t   s       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       b   u   t       b   y       u   s   i   n   g      t   h   e       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       p   r   o   p   o   s   e   d       i   n       d   n   d   f   s   ,       t   h   e       r   e   s   u   l   t   s       o   f       f   o   r   e   s   t   s       a   r   e       e   v   e   n       w   o   r   s   e       t   h   a   n       t   h   o   s   e       f   o   r       a       s   i   n   g   l   e      t   r   e   e   .      t   o       s   u   m       u   p   ,       w   .   r   .   t   .       d   n   d   f   s       [   2   0   ]   ,       t   h   e       c   o   n   t   r   i   b   u   t   i   o   n   s       o   f       l   d   l   f   s       a   r   e   :       f   i   r   s   t   ,       w   e       e   x   t   e   n   d       f   r   o   m       c   l   a   s   s   i   f   i   c   a   t   i   o   n       [   2   0   ]      t   o       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       b   y       p   r   o   p   o   s   i   n   g       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   o   r       t   h   e       f   o   r   e   s   t   s       a   n   d       d   e   r   i   v   e       t   h   e       g   r   a   d   i   e   n   t       t   o      l   e   a   r   n       s   p   l   i   t   s       n   o   d   e   s       w   .   r   .   t   .       t   h   i   s       l   o   s   s   ;       s   e   c   o   n   d   ,       w   e       d   e   r   i   v   e   d       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       f   o   r       l   e   a   f       n   o   d   e   s       b   y       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g       (   h   a   v   i   n   g       o   b   s   e   r   v   e   d       t   h   a   t       t   h   e       u   p   d   a   t   e       f   u   n   c   t   i   o   n       i   n       [   2   0   ]       w   a   s       a       s   p   e   c   i   a   l       c   a   s   e       o   f       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g   )   ;       l   a   s   t       b   u   t       n   o   t       t   h   e       l   e   a   s   t   ,       w   e       p   r   o   p   o   s   e       a   b   o   v   e       t   h   r   e   e       s   t   r   a   t   e   g   i   e   s       t   o       l   e   a   r   n   i   n   g       t   h   e       e   n   s   e   m   b   l   e       o   f      m   u   l   t   i   p   l   e       t   r   e   e   s   ,       w   h   i   c   h       a   r   e       d   i   f   f   e   r   e   n   t       f   r   o   m       [   2   0   ]   ,       b   u   t       w   e       s   h   o   w       a   r   e       e   f   f   e   c   t   i   v   e   .      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       a       n   u   m   b   e   r       o   f       s   p   e   c   i   a   l   i   z   e   d       a   l   g   o   r   i   t   h   m   s       h   a   v   e       b   e   e   n       p   r   o   p   o   s   e   d       t   o       a   d   d   r   e   s   s       t   h   e      l   d   l       t   a   s   k   ,       a   n   d       h   a   v   e       s   h   o   w   n       t   h   e   i   r       e   f   f   e   c   t   i   v   e   n   e   s   s       i   n       m   a   n   y       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   s   ,       s   u   c   h       a   s       f   a   c   i   a   l      a   g   e       e   s   t   i   m   a   t   i   o   n       [   8   ,       1   1   ,       2   8   ]   ,       e   x   p   r   e   s   s   i   o   n       r   e   c   o   g   n   i   t   i   o   n       [   3   0   ]       a   n   d       h   a   n   d       o   r   i   e   n   t   a   t   i   o   n       e   s   t   i   m   a   t   i   o   n       [   1   0   ]   .      g   e   n   g       e   t       a   l   .       [   8   ]       d   e   f   i   n   e   d       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       a   n       i   n   s   t   a   n   c   e       a   s       a       v   e   c   t   o   r       c   o   n   t   a   i   n   i   n   g       t   h   e       p   r   o   b   a   b   i   l   i   t   i   e   s      o   f       t   h   e       i   n   s   t   a   n   c   e       h   a   v   i   n   g       e   a   c   h       l   a   b   e   l   .       t   h   e   y       a   l   s   o       g   a   v   e       a       s   t   r   a   t   e   g   y       t   o       a   s   s   i   g   n       a       p   r   o   p   e   r       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n      t   o       a   n       i   n   s   t   a   n   c   e       w   i   t   h       a       s   i   n   g   l   e       l   a   b   e   l   ,       i   .   e   .   ,       a   s   s   i   g   n   i   n   g       a       g   a   u   s   s   i   a   n       o   r       t   r   i   a   n   g   l   e       d   i   s   t   r   i   b   u   t   i   o   n       w   h   o   s   e       p   e   a   k      i   s       t   h   e       s   i   n   g   l   e       l   a   b   e   l   ,       a   n   d       p   r   o   p   o   s   e   d       a   n       a   l   g   o   r   i   t   h   m       c   a   l   l   e   d       i   i   s   -   l   l   d   ,       w   h   i   c   h       i   s       a   n       i   t   e   r   a   t   i   v   e       o   p   t   i   m   i   z   a   t   i   o   n      p   r   o   c   e   s   s       b   a   s   e   d       o   n       a       t   w   o   -   l   a   y   e   r       e   n   e   r   g   y       b   a   s   e   d       m   o   d   e   l   .       y   a   n   g       e   t       a   l   .       [   2   8   ]       t   h   e   n       d   e   f   i   n   e   d       a       t   h   r   e   e   -   l   a   y   e   r       e   n   e   r   g   y      b   a   s   e   d       m   o   d   e   l   ,       c   a   l   l   e   d       s   c   e   -   l   d   l   ,       i   n       w   h   i   c   h       t   h   e       a   b   i   l   i   t   y       t   o       p   e   r   f   o   r   m       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       i   s       i   m   p   r   o   v   e   d       b   y      a   d   d   i   n   g       t   h   e       e   x   t   r   a       h   i   d   d   e   n       l   a   y   e   r       a   n   d       s   p   a   r   s   i   t   y       c   o   n   s   t   r   a   i   n   t   s       a   r   e       a   l   s   o       i   n   c   o   r   p   o   r   a   t   e   d       t   o       a   m   e   l   i   o   r   a   t   e       t   h   e       m   o   d   e   l   .      g   e   n   g       [   6   ]       d   e   v   e   l   o   p   e   d       a   n       a   c   c   e   l   e   r   a   t   e   d       v   e   r   s   i   o   n       o   f       i   i   s   -   l   l   d   ,       c   a   l   l   e   d       b   f   g   s   -   l   d   l   ,       b   y       u   s   i   n   g       q   u   a   s   i   -   n   e   w   t   o   n      o   p   t   i   m   i   z   a   t   i   o   n   .       a   l   l       t   h   e       a   b   o   v   e       l   d   l       m   e   t   h   o   d   s       a   s   s   u   m   e       t   h   a   t       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       c   a   n       b   e       r   e   p   r   e   s   e   n   t   e   d       b   y       a      m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       [   2   ]   ,       b   u   t       t   h   e       e   x   p   o   n   e   n   t   i   a   l       p   a   r   t       o   f       t   h   i   s       m   o   d   e   l       r   e   s   t   r   i   c   t   s       t   h   e       g   e   n   e   r   a   l   i   t   y       o   f       t   h   e      d   i   s   t   r   i   b   u   t   i   o   n       f   o   r   m   .      a   n   o   t   h   e   r       w   a   y       t   o       a   d   d   r   e   s   s       t   h   e       l   d   l       t   a   s   k   ,       i   s       t   o       e   x   t   e   n   d       e   x   i   s   t   i   n   g       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s       t   o       d   e   a   l       w   i   t   h       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n   s   .       g   e   n   g       a   n   d       h   o   u       [   7   ]       p   r   o   p   o   s   e   d       l   d   s   v   r   ,       a       l   d   l       m   e   t   h   o   d       b   y       e   x   t   e   n   d   i   n   g       s   u   p   p   o   r   t       v   e   c   t   o   r      r   e   g   r   e   s   s   o   r   ,       w   h   i   c   h       f   i   t       a       s   i   g   m   o   i   d       f   u   n   c   t   i   o   n       t   o       e   a   c   h       c   o   m   p   o   n   e   n   t       o   f       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       s   i   m   u   l   t   a   n   e   o   u   s   l   y       b   y       a      s   u   p   p   o   r   t       v   e   c   t   o   r       m   a   c   h   i   n   e   .       x   i   n   g       e   t       a   l   .       [   2   7   ]       t   h   e   n       e   x   t   e   n   d   e   d       b   o   o   s   t   i   n   g       t   o       a   d   d   r   e   s   s       t   h   e       l   d   l       t   a   s   k       b   y       a   d   d   i   t   i   v   e      w   e   i   g   h   t   e   d       r   e   g   r   e   s   s   o   r   s   .       t   h   e   y       s   h   o   w   e   d       t   h   a   t       u   s   i   n   g       t   h   e       v   e   c   t   o   r       t   r   e   e       m   o   d   e   l       a   s       t   h   e       w   e   a   k       r   e   g   r   e   s   s   o   r       c   a   n       l   e   a   d       t   o      b   e   t   t   e   r       p   e   r   f   o   r   m   a   n   c   e       a   n   d       n   a   m   e   d       t   h   i   s       m   e   t   h   o   d       a   o   s   o   -   l   d   l   l   o   g   i   t   b   o   o   s   t   .       a   s       t   h   e       l   e   a   r   n   i   n   g       o   f       t   h   i   s       t   r   e   e       m   o   d   e   l      i   s       b   a   s   e   d       o   n       l   o   c   a   l   l   y   -   o   p   t   i   m   a   l       h   a   r   d       d   a   t   a       p   a   r   t   i   t   i   o   n       f   u   n   c   t   i   o   n   s       a   t       e   a   c   h       s   p   l   i   t       n   o   d   e   ,       a   o   s   o   -   l   d   l   l   o   g   i   t   b   o   o   s   t       i   s      u   n   a   b   l   e       t   o       b   e       c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   .       e   x   t   e   n   d   i   n   g       c   u   r   r   e   n   t       d   e   e   p       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m   s       t   o      3         \\x0c   a   d   d   r   e   s   s       t   h   e       l   d   l       t   a   s   k       i   s       a   n       i   n   t   e   r   e   s   t   i   n   g       t   o   p   i   c   .       b   u   t   ,       t   h   e       e   x   i   s   t   i   n   g       s   u   c   h       a       m   e   t   h   o   d   ,       c   a   l   l   e   d       d   l   d   l       [   5   ]   ,       s   t   i   l   l      f   o   c   u   s   e   s       o   n       m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l       b   a   s   e   d       l   d   l   .      o   u   r       m   e   t   h   o   d   ,       l   d   l   f   s   ,       e   x   t   e   n   d   s       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s       t   o       a   d   d   r   e   s   s       l   d   l       t   a   s   k   s   ,       i   n       w   h   i   c   h       t   h   e       p   r   e   d   i   c   t   e   d      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       a       s   a   m   p   l   e       c   a   n       b   e       e   x   p   r   e   s   s   e   d       b   y       a       l   i   n   e   a   r       c   o   m   b   i   n   a   t   i   o   n       o   f       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s      o   f       t   h   e       t   r   a   i   n   i   n   g       d   a   t   a   ,       a   n   d       t   h   u   s       h   a   v   e       n   o       r   e   s   t   r   i   c   t   i   o   n   s       o   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       (   e   .   g   .   ,       n   o       r   e   q   u   i   r   e   m   e   n   t       o   f       t   h   e      m   a   x   i   m   u   m       e   n   t   r   o   p   y       m   o   d   e   l   )   .       i   n       a   d   d   i   t   i   o   n   ,       t   h   a   n   k   s       t   o       t   h   e       i   n   t   r   o   d   u   c   t   i   o   n       o   f       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       f   u   n   c   t   i   o   n   s   ,      l   d   l   f   s       c   a   n       b   e       c   o   m   b   i   n   e   d       w   i   t   h       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g   ,       e   .   g   .   ,       t   o       l   e   a   r   n       d   e   e   p       f   e   a   t   u   r   e   s       i   n       a   n       e   n   d   -   t   o   -   e   n   d      m   a   n   n   e   r   .         3         l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s         a       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s   .       w   e       f   i   r   s   t       i   n   t   r   o   d   u   c   e       h   o   w       t   o       l   e   a   r   n       a       s   i   n   g   l   e       d   e   c   i   s   i   o   n       t   r   e   e       b   y      l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   ,       t   h   e   n       d   e   s   c   r   i   b   e       t   h   e       l   e   a   r   n   i   n   g       o   f       a       f   o   r   e   s   t   .      3   .   1         p   r   o   b   l   e   m       f   o   r   m   u   l   a   t   i   o   n         l   e   t       x       =       r   m       d   e   n   o   t   e       t   h   e       i   n   p   u   t       s   p   a   c   e       a   n   d       y       =       {   y   1       ,       y   2       ,       .       .       .       ,       y   c       }       d   e   n   o   t   e       t   h   e       c   o   m   p   l   e   t   e       s   e   t       o   f       l   a   b   e   l   s   ,      w   h   e   r   e       c       i   s       t   h   e       n   u   m   b   e   r       o   f       p   o   s   s   i   b   l   e       l   a   b   e   l       v   a   l   u   e   s   .       w   e       c   o   n   s   i   d   e   r       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       (   l   d   l   )      p   r   o   b   l   e   m   ,       w   h   e   r   e       f   o   r       e   a   c   h       i   n   p   u   t       s   a   m   p   l   e       x       \\xe2\\x88\\x88       x       ,       t   h   e   r   e       i   s       a       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       d       =       (   d   x   y   1       ,       d   y   x   2       ,       .       .       .       ,       d   y   x   c       )   >       \\xe2\\x88\\x88      r   c       .       h   e   r   e       d   y   x   c       e   x   p   r   e   s   s   e   s       t   h   e       p   r   o   b   a   b   i   l   i   t   y       o   f       t   h   e       s   a   m   p   l   e       x       h   a   v   i   n   g       t   h   e       c   -   t   h       l   a   b   e   l       y   c       a   n   d       t   h   u   s       h   a   s       t   h   e      p   c      c   o   n   s   t   r   a   i   n   t   s       t   h   a   t       d   y   x   c       \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d       c   =   1       d   y   x   c       =       1   .       t   h   e       g   o   a   l       o   f       t   h   e       l   d   l       p   r   o   b   l   e   m       i   s       t   o       l   e   a   r   n       a       m   a   p   p   i   n   g      f   u   n   c   t   i   o   n       g       :       x       \\xe2\\x86\\x92       d       b   e   t   w   e   e   n       a   n       i   n   p   u   t       s   a   m   p   l   e       x       a   n   d       i   t   s       c   o   r   r   e   s   p   o   n   d   i   n   g       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       d   .      h   e   r   e   ,       w   e       w   a   n   t       t   o       l   e   a   r   n       t   h   e       m   a   p   p   i   n   g       f   u   n   c   t   i   o   n       g   (   x   )       b   y       a       d   e   c   i   s   i   o   n       t   r   e   e       b   a   s   e   d       m   o   d   e   l       t       .       a       d   e   c   i   s   i   o   n      t   r   e   e       c   o   n   s   i   s   t   s       o   f       a       s   e   t       o   f       s   p   l   i   t       n   o   d   e   s       n       a   n   d       a       s   e   t       o   f       l   e   a   f       n   o   d   e   s       l   .       e   a   c   h       s   p   l   i   t       n   o   d   e       n       \\xe2\\x88\\x88       n       d   e   f   i   n   e   s      a       s   p   l   i   t       f   u   n   c   t   i   o   n       s   n       (   \\xc2\\xb7   ;       \\xce\\x98   )       :       x       \\xe2\\x86\\x92       [   0   ,       1   ]       p   a   r   a   m   e   t   e   r   i   z   e   d       b   y       \\xce\\x98       t   o       d   e   t   e   r   m   i   n   e       w   h   e   t   h   e   r       a       s   a   m   p   l   e       i   s       s   e   n   t      t   o       t   h   e       l   e   f   t       o   r       r   i   g   h   t       s   u   b   t   r   e   e   .       e   a   c   h       l   e   a   f       n   o   d   e       `       \\xe2\\x88\\x88       l       h   o   l   d   s       a       d   i   s   t   r   i   b   u   t   i   o   n       q   `       =       (   q   `   1       ,       q   `   2       ,       .       .       .       ,       q   `   c       )   >      p   c      o   v   e   r       y   ,       i   .   e   ,       q   `   c       \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d       c   =   1       q   `   c       =       1   .       t   o       b   u   i   l   d       a       d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   ,       f   o   l   l   o   w   i   n   g       [   2   0   ]   ,      w   e       u   s   e       a       p   r   o   b   a   b   i   l   i   s   t   i   c       s   p   l   i   t       f   u   n   c   t   i   o   n       s   n       (   x   ;       \\xce\\x98   )       =       \\xcf\\x83   (   f   \\xcf\\x95   (   n   )       (   x   ;       \\xce\\x98   )   )   ,       w   h   e   r   e       \\xcf\\x83   (   \\xc2\\xb7   )       i   s       a       s   i   g   m   o   i   d       f   u   n   c   t   i   o   n   ,      \\xcf\\x95   (   \\xc2\\xb7   )       i   s       a   n       i   n   d   e   x       f   u   n   c   t   i   o   n       t   o       b   r   i   n   g       t   h   e       \\xcf\\x95   (   n   )   -   t   h       o   u   t   p   u   t       o   f       f   u   n   c   t   i   o   n       f       (   x   ;       \\xce\\x98   )       i   n       c   o   r   r   e   s   p   o   n   d   e   n   c   e       w   i   t   h      s   p   l   i   t       n   o   d   e       n   ,       a   n   d       f       :       x       \\xe2\\x86\\x92       r   m       i   s       a       r   e   a   l   -   v   a   l   u   e   d       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       d   e   p   e   n   d   i   n   g       o   n       t   h   e       s   a   m   p   l   e       x      a   n   d       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98   ,       a   n   d       c   a   n       t   a   k   e       a   n   y       f   o   r   m   .       f   o   r       a       s   i   m   p   l   e       f   o   r   m   ,       i   t       c   a   n       b   e       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n      o   f       x   ,       w   h   e   r   e       \\xce\\x98       i   s       t   h   e       t   r   a   n   s   f   o   r   m   a   t   i   o   n       m   a   t   r   i   x   ;       f   o   r       a       c   o   m   p   l   e   x       f   o   r   m   ,       i   t       c   a   n       b   e       a       d   e   e   p       n   e   t   w   o   r   k       t   o      p   e   r   f   o   r   m       r   e   p   r   e   s   e   n   t   a   t   i   o   n       l   e   a   r   n   i   n   g       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   ,       t   h   e   n       \\xce\\x98       i   s       t   h   e       n   e   t   w   o   r   k       p   a   r   a   m   e   t   e   r   .       t   h   e      c   o   r   r   e   s   p   o   n   d   e   n   c   e       b   e   t   w   e   e   n       t   h   e       s   p   l   i   t       n   o   d   e   s       a   n   d       t   h   e       o   u   t   p   u   t       u   n   i   t   s       o   f       f   u   n   c   t   i   o   n       f       ,       i   n   d   i   c   a   t   e   d       b   y       \\xcf\\x95   (   \\xc2\\xb7   )       t   h   a   t       i   s      r   a   n   d   o   m   l   y       g   e   n   e   r   a   t   e   d       b   e   f   o   r   e       t   r   e   e       l   e   a   r   n   i   n   g   ,       i   .   e   .   ,       w   h   i   c   h       o   u   t   p   u   t       u   n   i   t   s       f   r   o   m       \\xe2\\x80\\x9c   f       \\xe2\\x80\\x9d       a   r   e       u   s   e   d       f   o   r       c   o   n   s   t   r   u   c   t   i   n   g       a      t   r   e   e       i   s       d   e   t   e   r   m   i   n   e   d       r   a   n   d   o   m   l   y   .       a   n       e   x   a   m   p   l   e       t   o       d   e   m   o   n   s   t   r   a   t   e       \\xcf\\x95   (   \\xc2\\xb7   )       i   s       s   h   o   w   n       i   n       f   i   g   .       2   .       t   h   e   n   ,       t   h   e       p   r   o   b   a   b   i   l   i   t   y      o   f       t   h   e       s   a   m   p   l   e       x       f   a   l   l   i   n   g       i   n   t   o       l   e   a   f       n   o   d   e       `       i   s       g   i   v   e   n       b   y      y      l      r      p   (   `   |   x   ;       \\xce\\x98   )       =      s   n       (   x   ;       \\xce\\x98   )   1   (   `   \\xe2\\x88\\x88   l   n       )       (   1       \\xe2\\x88\\x92       s   n       (   x   ;       \\xce\\x98   )   )   1   (   `   \\xe2\\x88\\x88   l   n       )       ,      (   1   )      n   \\xe2\\x88\\x88   n         w   h   e   r   e       1   (   \\xc2\\xb7   )       i   s       a   n       i   n   d   i   c   a   t   o   r       f   u   n   c   t   i   o   n       a   n   d       l   l   n       a   n   d       l   r   n       d   e   n   o   t   e       t   h   e       s   e   t   s       o   f       l   e   a   f       n   o   d   e   s       h   e   l   d       b   y       t   h   e       l   e   f   t       a   n   d      r   i   g   h   t       s   u   b   t   r   e   e   s       o   f       n   o   d   e       n   ,       t   n   l       a   n   d       t   n   r       ,       r   e   s   p   e   c   t   i   v   e   l   y   .       t   h   e       o   u   t   p   u   t       o   f       t   h   e       t   r   e   e       t       w   .   r   .   t   .       x   ,       i   .   e   .   ,       t   h   e       m   a   p   p   i   n   g      f   u   n   c   t   i   o   n       g   ,       i   s       d   e   f   i   n   e   d       b   y      x      g   (   x   ;       \\xce\\x98   ,       t       )       =      p   (   `   |   x   ;       \\xce\\x98   )   q   `       .      (   2   )      `   \\xe2\\x88\\x88   l         3   .   2         t   r   e   e       o   p   t   i   m   i   z   a   t   i   o   n         g   i   v   e   n       a       t   r   a   i   n   i   n   g       s   e   t       s       =       {   (   x   i       ,       d   i       )   }   n      i   =   1       ,       o   u   r       g   o   a   l       i   s       t   o       l   e   a   r   n       a       d   e   c   i   s   i   o   n       t   r   e   e       t       d   e   s   c   r   i   b   e   d       i   n       s   e   c   .       3   .   1      w   h   i   c   h       c   a   n       o   u   t   p   u   t       a       d   i   s   t   r   i   b   u   t   i   o   n       g   (   x   i       ;       \\xce\\x98   ,       t       )       s   i   m   i   l   a   r       t   o       d   i       f   o   r       e   a   c   h       s   a   m   p   l   e       x   i       .       t   o       t   h   i   s       e   n   d   ,       a      s   t   r   a   i   g   h   t   f   o   r   w   a   r   d       w   a   y       i   s       t   o       m   i   n   i   m   i   z   e       t   h   e       k   u   l   l   b   a   c   k   -   l   e   i   b   l   e   r       (   k   -   l   )       d   i   v   e   r   g   e   n   c   e       b   e   t   w   e   e   n       e   a   c   h       g   (   x   i       ;       \\xce\\x98   ,       t       )      a   n   d       d   i       ,       o   r       e   q   u   i   v   a   l   e   n   t   l   y       t   o       m   i   n   i   m   i   z   e       t   h   e       f   o   l   l   o   w   i   n   g       c   r   o   s   s   -   e   n   t   r   o   p   y       l   o   s   s   :      r   (   q   ,       \\xce\\x98   ;       s   )       =       \\xe2\\x88\\x92         n       c      n       c      \\x10   x      \\x11      1       x       x       y   c      1       x       x       y   c      d   x   i       l   o   g   (   g   c       (   x   i       ;       \\xce\\x98   ,       t       )   )       =       \\xe2\\x88\\x92      d   x   i       l   o   g      p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       ,       (   3   )      n       i   =   1       c   =   1      n       i   =   1       c   =   1      `   \\xe2\\x88\\x88   l         4         \\x0c   w   h   e   r   e       q       d   e   n   o   t   e       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       a   l   l       t   h   e       l   e   a   f       n   o   d   e   s       l       a   n   d       g   c       (   x   i       ;       \\xce\\x98   ,       t       )       i   s       t   h   e       c   -   t   h       o   u   t   p   u   t       u   n   i   t      o   f       g   (   x   i       ;       \\xce\\x98   ,       t       )   .       l   e   a   r   n   i   n   g       t   h   e       t   r   e   e       t       r   e   q   u   i   r   e   s       t   h   e       e   s   t   i   m   a   t   i   o   n       o   f       t   w   o       p   a   r   a   m   e   t   e   r   s   :       1   )       t   h   e       s   p   l   i   t       n   o   d   e      p   a   r   a   m   e   t   e   r       \\xce\\x98       a   n   d       2   )       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       q       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s   .       t   h   e       b   e   s   t       p   a   r   a   m   e   t   e   r   s       (   \\xce\\x98   \\xe2\\x88\\x97       ,       q   \\xe2\\x88\\x97       )       a   r   e      d   e   t   e   r   m   i   n   e   d       b   y      (   \\xce\\x98   \\xe2\\x88\\x97       ,       q   \\xe2\\x88\\x97       )       =       a   r   g       m   i   n       r   (   q   ,       \\xce\\x98   ;       s   )   .      (   4   )      \\xce\\x98   ,   q         t   o       s   o   l   v   e       e   q   n   .       4   ,       w   e       c   o   n   s   i   d   e   r       a   n       a   l   t   e   r   n   a   t   i   n   g       o   p   t   i   m   i   z   a   t   i   o   n       s   t   r   a   t   e   g   y   :       f   i   r   s   t   ,       w   e       f   i   x       q       a   n   d       o   p   t   i   m   i   z   e      \\xce\\x98   ;       t   h   e   n   ,       w   e       f   i   x       \\xce\\x98       a   n   d       o   p   t   i   m   i   z   e       q   .       t   h   e   s   e       t   w   o       l   e   a   r   n   i   n   g       s   t   e   p   s       a   r   e       a   l   t   e   r   n   a   t   i   v   e   l   y       p   e   r   f   o   r   m   e   d   ,       u   n   t   i   l      c   o   n   v   e   r   g   e   n   c   e       o   r       a       m   a   x   i   m   u   m       n   u   m   b   e   r       o   f       i   t   e   r   a   t   i   o   n   s       i   s       r   e   a   c   h   e   d       (   d   e   f   i   n   e   d       i   n       t   h   e       e   x   p   e   r   i   m   e   n   t   s   )   .      3   .   2   .   1         l   e   a   r   n   i   n   g       s   p   l   i   t       n   o   d   e   s         i   n       t   h   i   s       s   e   c   t   i   o   n   ,       w   e       d   e   s   c   r   i   b   e       h   o   w       t   o       l   e   a   r   n       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98       f   o   r       s   p   l   i   t       n   o   d   e   s   ,       w   h   e   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d      b   y       t   h   e       l   e   a   f       n   o   d   e   s       q       a   r   e       f   i   x   e   d   .       w   e       c   o   m   p   u   t   e       t   h   e       g   r   a   d   i   e   n   t       o   f       t   h   e       l   o   s   s       r   (   q   ,       \\xce\\x98   ;       s   )       w   .   r   .   t   .       \\xce\\x98       b   y       t   h   e       c   h   a   i   n      r   u   l   e   :      n      \\xe2\\x88\\x82   r   (   q   ,       \\xce\\x98   ;       s   )       x       x       \\xe2\\x88\\x82   r   (   q   ,       \\xce\\x98   ;       s   )       \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      =      ,      (   5   )      \\xe2\\x88\\x82   \\xce\\x98      \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x82   \\xce\\x98      i   =   1      n   \\xe2\\x88\\x88   n         w   h   e   r   e       o   n   l   y       t   h   e       f   i   r   s   t       t   e   r   m       d   e   p   e   n   d   s       o   n       t   h   e       t   r   e   e       a   n   d       t   h   e       s   e   c   o   n   d       t   e   r   m       d   e   p   e   n   d   s       o   n       t   h   e       s   p   e   c   i   f   i   c       t   y   p   e       o   f       t   h   e      f   u   n   c   t   i   o   n       f   \\xcf\\x95   (   n   )       .       t   h   e       f   i   r   s   t       t   e   r   m       i   s       g   i   v   e   n       b   y      c      \\x01       g   c       (   x   i       ;       \\xce\\x98   ,       t   n   l       )       \\x11      g   c       (   x   i       ;       \\xce\\x98   ,       t   n   r       )      1       x       y   c       \\x10      \\xe2\\x88\\x82   r   (   q   ,       \\xce\\x98   ;       s   )      =      d   x   i       s   n       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x92       1       \\xe2\\x88\\x92       s   n       (   x   i       ;       \\xce\\x98   )      ,       (   6   )      \\xe2\\x88\\x82   f   \\xcf\\x95   (   n   )       (   x   i       ;       \\xce\\x98   )      n       c   =   1      g   c       (   x   i       ;       \\xce\\x98   ,       t       )      g   c       (   x   i       ;       \\xce\\x98   ,       t       )      p      p      w   h   e   r   e       g   c       (   x   i       ;       \\xce\\x98   ,       t   n   l       )       =       `   \\xe2\\x88\\x88   l   l   n       p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       a   n   d       g       c       (   x   i       ;       \\xce\\x98   ,       t   n   r       )       =       `   \\xe2\\x88\\x88   l   r   n       p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c       .       n   o   t   e       t   h   a   t   ,      l   e   t       t   n       b   e       t   h   e       t   r   e   e       r   o   o   t   e   d       a   t       t   h   e       n   o   d   e       n   ,       t   h   e   n       w   e       h   a   v   e       g   c       (   x   i       ;       \\xce\\x98   ,       t   n       )       =       g   c       (   x   i       ;       \\xce\\x98   ,       t   n   l       )       +       g   c       (   x   i       ;       \\xce\\x98   ,       t   n   r       )   .      t   h   i   s       m   e   a   n   s       t   h   e       g   r   a   d   i   e   n   t       c   o   m   p   u   t   a   t   i   o   n       i   n       e   q   n   .       6       c   a   n       b   e       s   t   a   r   t   e   d       a   t       t   h   e       l   e   a   f       n   o   d   e   s       a   n   d       c   a   r   r   i   e   d       o   u   t       i   n       a      b   o   t   t   o   m       u   p       m   a   n   n   e   r   .       t   h   u   s   ,       t   h   e       s   p   l   i   t       n   o   d   e       p   a   r   a   m   e   t   e   r   s       c   a   n       b   e       l   e   a   r   n   e   d       b   y       s   t   a   n   d   a   r   d       b   a   c   k   -   p   r   o   p   a   g   a   t   i   o   n   .         3   .   2   .   2         l   e   a   r   n   i   n   g       l   e   a   f       n   o   d   e   s         n   o   w   ,       f   i   x   i   n   g       t   h   e       p   a   r   a   m   e   t   e   r       \\xce\\x98   ,       w   e       s   h   o   w       h   o   w       t   o       l   e   a   r   n       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s       q   ,       w   h   i   c   h      i   s       a       c   o   n   s   t   r   a   i   n   e   d       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m   :      m   i   n       r   (   q   ,       \\xce\\x98   ;       s   )   ,       s   .   t   .   ,       \\xe2\\x88\\x80   `   ,      q         c      x         q   `   c       =       1   .         (   7   )         c   =   1         h   e   r   e   ,       w   e       p   r   o   p   o   s   e       t   o       a   d   d   r   e   s   s       t   h   i   s       c   o   n   s   t   r   a   i   n   e   d       c   o   n   v   e   x       o   p   t   i   m   i   z   a   t   i   o   n       p   r   o   b   l   e   m       b   y       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g       [   1   9   ,       2   9   ]   ,       w   h   i   c   h       l   e   a   d   s       t   o       a       s   t   e   p   -   s   i   z   e       f   r   e   e       a   n   d       f   a   s   t   -   c   o   n   v   e   r   g   e   d       u   p   d   a   t   e       r   u   l   e       f   o   r       q   .       i   n       v   a   r   i   a   t   i   o   n   a   l      b   o   u   n   d   i   n   g   ,       a   n       o   r   i   g   i   n   a   l       o   b   j   e   c   t   i   v   e       f   u   n   c   t   i   o   n       t   o       b   e       m   i   n   i   m   i   z   e   d       g   e   t   s       r   e   p   l   a   c   e   d       b   y       i   t   s       b   o   u   n   d       i   n       a   n       i   t   e   r   a   t   i   v   e      m   a   n   n   e   r   .       a       u   p   p   e   r       b   o   u   n   d       f   o   r       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       r   (   q   ,       \\xce\\x98   ;       s   )       c   a   n       b   e       o   b   t   a   i   n   e   d       b   y       j   e   n   s   e   n   \\xe2\\x80\\x99   s       i   n   e   q   u   a   l   i   t   y   :      r   (   q   ,       \\xce\\x98   ;       s   )       =       \\xe2\\x88\\x92         n       c      \\x10   x      \\x11      1       x       x       y   c      d   x   i       l   o   g      p   (   `   |   x   i       ;       \\xce\\x98   )   q   `   c      n       i   =   1       c   =   1      `   \\xe2\\x88\\x88   l         \\xe2\\x89\\xa4   \\xe2\\x88\\x92      w   h   e   r   e       \\xce\\xbe   `       (   q   `   c       ,       x   i       )       =         1      n         n       x      c      x      i   =   1       c   =   1         p   (   `   |   x   i       ;   \\xce\\x98   )   q   `   c      g   c       (   x   i       ;   \\xce\\x98   ,   t       )         \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       =       \\xe2\\x88\\x92         d   y   x   c   i         x         \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )       l   o   g         `   \\xe2\\x88\\x88   l         \\x10       p   (   `   |   x       ;       \\xce\\x98   )   q       \\x11      i      `   c      ,      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )         (   8   )         .       w   e       d   e   f   i   n   e         n       c      \\x10       p   (   `   |   x       ;       \\xce\\x98   )   q       \\x11      1       x       x       y   c       x      i      `   c      d   x   i      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )       l   o   g      .      n       i   =   1       c   =   1      \\xce\\xbe   `       (   q   \\xcc\\x84   `   c       ,       x   i       )         (   9   )         `   \\xe2\\x88\\x88   l         t   h   e   n       \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       i   s       a   n       u   p   p   e   r       b   o   u   n   d       f   o   r       r   (   q   ,       \\xce\\x98   ;       s   )   ,       w   h   i   c   h       h   a   s       t   h   e       p   r   o   p   e   r   t   y       t   h   a   t       f   o   r       a   n   y       q       a   n   d       q   \\xcc\\x84   ,      \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       \\xe2\\x89\\xa5       r   (   q   ,       \\xce\\x98   ;       s   )   ,       a   n   d       \\xcf\\x86   (   q   ,       q   )       =       r   (   q   ,       \\xce\\x98   ;       s   )   .       a   s   s   u   m   e       t   h   a   t       w   e       a   r   e       a   t       a       p   o   i   n   t       q   (   t   )       c   o   r   r   e   s   p   o   n   d   i   n   g      t   o       t   h   e       t   -   t   h       i   t   e   r   a   t   i   o   n   ,       t   h   e   n       \\xcf\\x86   (   q   ,       q   (   t   )       )       i   s       a   n       u   p   p   e   r       b   o   u   n   d       f   o   r       r   (   q   ,       \\xce\\x98   ;       s   )   .       i   n       t   h   e       n   e   x   t       i   t   e   r   a   t   i   o   n   ,       q   (   t   +   1   )      i   s       c   h   o   s   e   n       s   u   c   h       t   h   a   t       \\xcf\\x86   (   q   (   t   +   1   )       ,       q   )       \\xe2\\x89\\xa4       r   (   q   (   t   )       ,       \\xce\\x98   ;       s   )   ,       w   h   i   c   h       i   m   p   l   i   e   s       r   (   q   (   t   +   1   )       ,       \\xce\\x98   ;       s   )       \\xe2\\x89\\xa4       r   (   q   (   t   )       ,       \\xce\\x98   ;       s   )   .      5         \\x0c   c   o   n   s   e   q   u   e   n   t   l   y   ,       w   e       c   a   n       m   i   n   i   m   i   z   e       \\xcf\\x86   (   q   ,       q   \\xcc\\x84   )       i   n   s   t   e   a   d       o   f       r   (   q   ,       \\xce\\x98   ;       s   )       a   f   t   e   r       e   n   s   u   r   i   n   g       t   h   a   t       r   (   q   (   t   )       ,       \\xce\\x98   ;       s   )       =      \\xcf\\x86   (   q   (   t   )       ,       q   \\xcc\\x84   )   ,       i   .   e   .   ,       q   \\xcc\\x84       =       q   (   t   )       .       s   o       w   e       h   a   v   e      q   (   t   +   1   )       =       a   r   g       m   i   n       \\xcf\\x86   (   q   ,       q   (   t   )       )   ,       s   .   t   .   ,       \\xe2\\x88\\x80   `   ,      q         c      x         q   `   c       =       1   ,         (   1   0   )         c   =   1         w   h   i   c   h       l   e   a   d   s       t   o       m   i   n   i   m   i   z   i   n   g       t   h   e       l   a   g   r   a   n   g   i   a   n       d   e   f   i   n   e   d       b   y      \\xcf\\x95   (   q   ,       q   (   t   )       )       =       \\xcf\\x86   (   q   ,       q   (   t   )       )       +         x         \\xce\\xbb   `       (         `   \\xe2\\x88\\x88   l         w   h   e   r   e       \\xce\\xbb   `       i   s       t   h   e       l   a   g   r   a   n   g   e       m   u   l   t   i   p   l   i   e   r   .       b   y       s   e   t   t   i   n   g      \\xce\\xbb   `       =      (   t   +   1   )         n   o   t   e       t   h   a   t   ,       q   `   c         (   t   +   1   )         \\xe2\\x88\\x88       [   0   ,       1   ]       a   n   d         d   i   s   t   r   i   b   u   t   i   o   n   s       h   e   l   d       b   y       t   h   e       l   e   a   f       n   o   d   e   s   .       t   h   e       s   t   a   r   t   i   n   g      (   0   )      d   i   s   t   r   i   b   u   t   i   o   n   :       q   `   c       =       c   1       .      3   .   3         q   `   c       \\xe2\\x88\\x92       1   )   ,         (   1   1   )         c   =   1         \\xe2\\x88\\x82   \\xcf\\x95   (   q   ,   q   (   t   )       )      \\xe2\\x88\\x82   q   `   c         n       c      1       x       x       y   c      (   t   )      (   t   +   1   )      d       \\xce\\xbe   `       (   q   `   c       ,       x   i       )       a   n   d       q   `   c      n       i   =   1       c   =   1       x   i         s   a   t   i   s   f   i   e   s       t   h   a   t       q   `   c         c      x         =       0   ,       w   e       h   a   v   e      p   n       y   c      (   t   )      d   x   i       \\xce\\xbe   `       (   q   `   c       ,       x   i       )      .      =       p   c       i   =   1      p   n       y   c      (   t   )      \\xce\\xbe      (   q      ,      x      )      d      x      `      i      i      c   =   1      i   =   1      `   c         (   1   2   )         (   t   +   1   )      =       1   .       e   q   n   .       1   2       i   s       t   h   e       u   p   d   a   t   e       s   c   h   e   m   e       f   o   r      c   =   1       q   `   c      (   0   )      p   o   i   n   t       q   `       c   a   n       b   e       s   i   m   p   l   y       i   n   i   t   i   a   l   i   z   e   d       b   y       t   h   e       u   n   i   f   o   r   m         p   c         l   e   a   r   n   i   n   g       a       f   o   r   e   s   t         a       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       o   f       d   e   c   i   s   i   o   n       t   r   e   e   s       f       =       {   t   1       ,       .       .       .       ,       t   k       }   .       i   n       t   h   e       t   r   a   i   n   i   n   g       s   t   a   g   e   ,       a   l   l       t   r   e   e   s       i   n       t   h   e      f   o   r   e   s   t       f       u   s   e       t   h   e       s   a   m   e       p   a   r   a   m   e   t   e   r   s       \\xce\\x98       f   o   r       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       f       (   \\xc2\\xb7   ;       \\xce\\x98   )       (   b   u   t       c   o   r   r   e   s   p   o   n   d       t   o       d   i   f   f   e   r   e   n   t      o   u   t   p   u   t       u   n   i   t   s       o   f       f       a   s   s   i   g   n   e   d       b   y       \\xcf\\x95   ,       s   e   e       f   i   g   .       2   )   ,       b   u   t       e   a   c   h       t   r   e   e       h   a   s       i   n   d   e   p   e   n   d   e   n   t       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s      q   .       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       a       f   o   r   e   s   t       i   s       g   i   v   e   n       b   y       a   v   e   r   a   g   i   n   g       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n   s       f   o   r       a   l   l       i   n   d   i   v   i   d   u   a   l       t   r   e   e   s   :      p   k      1      r   f       =       k      k   =   1       r   t   k       ,       w   h   e   r   e       r   t   k       i   s       t   h   e       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       t   r   e   e       t   k       d   e   f   i   n   e   d       b   y       e   q   n   .       3   .       t   o       l   e   a   r   n       \\xce\\x98       b   y      f   i   x   i   n   g       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q       o   f       a   l   l       t   h   e       t   r   e   e   s       i   n       t   h   e       f   o   r   e   s   t       f   ,       b   a   s   e   d       o   n       t   h   e       d   e   r   i   v   a   t   i   o   n       i   n       s   e   c   .       3   .   2      a   n   d       r   e   f   e   r   r   i   n   g       t   o       f   i   g   .       2   ,       w   e       h   a   v   e      n       k      \\xe2\\x88\\x82   f   \\xcf\\x95   k       (   n   )       (   x   i       ;       \\xce\\x98   )      1       x   x       x      \\xe2\\x88\\x82   r   f      \\xe2\\x88\\x82   r   t   k      =      ,      \\xe2\\x88\\x82   \\xce\\x98      k       i   =   1      \\xe2\\x88\\x82   f   \\xcf\\x95   k       (   n   )       (   x   i       ;       \\xce\\x98   )      \\xe2\\x88\\x82   \\xce\\x98         (   1   3   )         k   =   1       n   \\xe2\\x88\\x88   n   k         w   h   e   r   e       n   k       a   n   d       \\xcf\\x95   k       (   \\xc2\\xb7   )       a   r   e       t   h   e       s   p   l   i   t       n   o   d   e       s   e   t       a   n   d       t   h   e       i   n   d   e   x       f   u   n   c   t   i   o   n       o   f       t   k       ,       r   e   s   p   e   c   t   i   v   e   l   y   .       n   o   t   e       t   h   a   t   ,      t   h   e       i   n   d   e   x       f   u   n   c   t   i   o   n       \\xcf\\x95   k       (   \\xc2\\xb7   )       f   o   r       e   a   c   h       t   r   e   e       i   s       r   a   n   d   o   m   l   y       a   s   s   i   g   n   e   d       b   e   f   o   r   e       t   r   e   e       l   e   a   r   n   i   n   g   ,       a   n   d       t   h   u   s       s   p   l   i   t      n   o   d   e   s       c   o   r   r   e   s   p   o   n   d       t   o       a       s   u   b   s   e   t       o   f       o   u   t   p   u   t       u   n   i   t   s       o   f       f       .       t   h   i   s       s   t   r   a   t   e   g   y       i   s       s   i   m   i   l   a   r       t   o       t   h   e       r   a   n   d   o   m       s   u   b   s   p   a   c   e      m   e   t   h   o   d       [   1   7   ]   ,       w   h   i   c   h       i   n   c   r   e   a   s   e   s       t   h   e       r   a   n   d   o   m   n   e   s   s       i   n       t   r   a   i   n   i   n   g       t   o       r   e   d   u   c   e       t   h   e       r   i   s   k       o   f       o   v   e   r   f   i   t   t   i   n   g   .      a   s       f   o   r       q   ,       s   i   n   c   e       e   a   c   h       t   r   e   e       i   n       t   h   e       f   o   r   e   s   t       f       h   a   s       i   t   s       o   w   n       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       q   ,       w   e       c   a   n       u   p   d   a   t   e       t   h   e   m      i   n   d   e   p   e   n   d   e   n   t   l   y       b   y       e   q   n   .       1   2   ,       g   i   v   e   n       b   y       \\xce\\x98   .       f   o   r       i   m   p   l   e   m   e   n   t   a   t   i   o   n   a   l       c   o   n   v   e   n   i   e   n   c   e   ,       w   e       d   o       n   o   t       c   o   n   d   u   c   t       t   h   i   s      u   p   d   a   t   e       s   c   h   e   m   e       o   n       t   h   e       w   h   o   l   e       d   a   t   a   s   e   t       s       b   u   t       o   n       a       s   e   t       o   f       m   i   n   i   -   b   a   t   c   h   e   s       b   .       t   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       o   f       a      l   d   l   f       i   s       s   h   o   w   n       i   n       a   l   g   o   r   i   t   h   m   .       1   .      a   l   g   o   r   i   t   h   m       1       t   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       o   f       a       l   d   l   f   .      r   e   q   u   i   r   e   :       s   :       t   r   a   i   n   i   n   g       s   e   t   ,       n   b       :       t   h   e       n   u   m   b   e   r       o   f       m   i   n   i   -   b   a   t   c   h   e   s       t   o       u   p   d   a   t   e       q      i   n   i   t   i   a   l   i   z   e       \\xce\\x98       r   a   n   d   o   m   l   y       a   n   d       q       u   n   i   f   o   r   m   l   y   ,       s   e   t       b       =       {   \\xe2\\x88\\x85   }      w   h   i   l   e       n   o   t       c   o   n   v   e   r   g   e       d   o      w   h   i   l   e       |   b   |       <       n   b       d   o      r   a   n   d   o   m   l   y       s   e   l   e   c   t       a       m   i   n   i   -   b   a   t   c   h       b       f   r   o   m       s      u   p   d   a   t   e   s      \\xce\\x98       b   y       c   o   m   p   u   t   i   n   g       g   r   a   d   i   e   n   t       (   e   q   n   .       1   3   )       o   n       b      b   =   b       b      e   n   d       w   h   i   l   e      u   p   d   a   t   e       q       b   y       i   t   e   r   a   t   i   n   g       e   q   n   .       1   2       o   n       b      b       =       {   \\xe2\\x88\\x85   }      e   n   d       w   h   i   l   e      i   n       t   h   e       t   e   s   t   i   n   g       s   t   a   g   e   ,       t   h   e       o   u   t   p   u   t       o   f       t   h   e       f   o   r   e   s   t       f       i   s       g   i   v   e   n       b   y       a   v   e   r   a   g   i   n   g       t   h   e       p   r   e   d   i   c   t   i   o   n   s       f   r   o   m       a   l   l       t   h   e      p   k      1      i   n   d   i   v   i   d   u   a   l       t   r   e   e   s   :       g   (   x   ;       \\xce\\x98   ,       f   )       =       k      k   =   1       g   (   x   ;       \\xce\\x98   ,       t   k       )   .      6         \\x0c   4         e   x   p   e   r   i   m   e   n   t   a   l       r   e   s   u   l   t   s         o   u   r       r   e   a   l   i   z   a   t   i   o   n       o   f       l   d   l   f   s       i   s       b   a   s   e   d       o   n       \\xe2\\x80\\x9c   c   a   f   f   e   \\xe2\\x80\\x9d       [   1   8   ]   .       i   t       i   s       m   o   d   u   l   a   r       a   n   d       i   m   p   l   e   m   e   n   t   e   d       a   s       a       s   t   a   n   d   a   r   d      n   e   u   r   a   l       n   e   t   w   o   r   k       l   a   y   e   r   .       w   e       c   a   n       e   i   t   h   e   r       u   s   e       i   t       a   s       a       s   h   a   l   l   o   w       s   t   a   n   d   -   a   l   o   n   e       m   o   d   e   l       (   s   l   d   l   f   s   )       o   r       i   n   t   e   g   r   a   t   e       i   t      w   i   t   h       a   n   y       d   e   e   p       n   e   t   w   o   r   k   s       (   d   l   d   l   f   s   )   .       w   e       e   v   a   l   u   a   t   e       s   l   d   l   f   s       o   n       d   i   f   f   e   r   e   n   t       l   d   l       t   a   s   k   s       a   n   d       c   o   m   p   a   r   e       i   t       w   i   t   h      o   t   h   e   r       s   t   a   n   d   -   a   l   o   n   e       l   d   l       m   e   t   h   o   d   s   .       a   s       d   l   d   l   f   s       c   a   n       b   e       l   e   a   r   n   e   d       f   r   o   m       r   a   w       i   m   a   g   e       d   a   t   a       i   n       a   n       e   n   d   -   t   o   -   e   n   d      m   a   n   n   e   r   ,       w   e       v   e   r   i   f   y       d   l   d   l   f   s       o   n       a       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       i   .   e   .   ,       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n   .       t   h   e       d   e   f   a   u   l   t      s   e   t   t   i   n   g   s       f   o   r       t   h   e       p   a   r   a   m   e   t   e   r   s       o   f       o   u   r       f   o   r   e   s   t   s       a   r   e   :       t   r   e   e       n   u   m   b   e   r       (   5   )   ,       t   r   e   e       d   e   p   t   h       (   7   )   ,       o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f      t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       (   6   4   )   ,       i   t   e   r   a   t   i   o   n       t   i   m   e   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       (   2   0   )   ,       t   h   e       n   u   m   b   e   r       o   f      m   i   n   i   -   b   a   t   c   h   e   s       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       (   1   0   0   )   ,       m   a   x   i   m   u   m       i   t   e   r   a   t   i   o   n       (   2   5   0   0   0   )   .      4   .   1         c   o   m   p   a   r   i   s   o   n       o   f       s   l   d   l   f   s       t   o       s   t   a   n   d   -   a   l   o   n   e       l   d   l       m   e   t   h   o   d   s         w   e       c   o   m   p   a   r   e       o   u   r       s   h   a   l   l   o   w       m   o   d   e   l       s   l   d   l   f   s       w   i   t   h       o   t   h   e   r       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       s   t   a   n   d   -   a   l   o   n   e       l   d   l       m   e   t   h   o   d   s   .      f   o   r       s   l   d   l   f   s   ,       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       f       (   x   ,       \\xce\\x98   )       i   s       a       l   i   n   e   a   r       t   r   a   n   s   f   o   r   m   a   t   i   o   n       o   f       x   ,       i   .   e   .   ,       t   h   e       i   -   t   h      o   u   t   p   u   t       u   n   i   t       f   i       (   x   ,       \\xce\\xb8   i       )       =       \\xce\\xb8   i   >       x   ,       w   h   e   r   e       \\xce\\xb8   i       i   s       t   h   e       i   -   t   h       c   o   l   u   m   n       o   f       t   h   e       t   r   a   n   s   f   o   r   m   a   t   i   o   n       m   a   t   r   i   x       \\xce\\x98   .       w   e      u   s   e   d       3       p   o   p   u   l   a   r       l   d   l       d   a   t   a   s   e   t   s       i   n       [   6   ]   ,       m   o   v   i   e   ,       h   u   m   a   n       g   e   n   e       a   n   d       n   a   t   u   r   a   l       s   c   e   n   e   1       .       t   h   e       s   a   m   p   l   e   s      i   n       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   r   e       r   e   p   r   e   s   e   n   t   e   d       b   y       n   u   m   e   r   i   c   a   l       d   e   s   c   r   i   p   t   o   r   s   ,       a   n   d       t   h   e       g   r   o   u   n   d       t   r   u   t   h   s       f   o   r       t   h   e   m       a   r   e      t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       m   o   v   i   e   s   ,       t   h   e       d   i   s   e   a   s   e   s       d   i   s   t   r   i   b   u   t   i   o   n   s       r   e   l   a   t   e   d       t   o       h   u   m   a   n      g   e   n   e   s       a   n   d       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       o   n       s   c   e   n   e   s   ,       s   u   c   h       a   s       p   l   a   n   t   ,       s   k   y       a   n   d       c   l   o   u   d   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       t   h   e       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n   s       o   f       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   r   e       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   ,       s   u   c   h       a   s       t   h   e       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n       s   h   o   w   n       i   n      f   i   g   .       1   (   b   )   .       f   o   l   l   o   w   i   n   g       [   7   ,       2   7   ]   ,       w   e       u   s   e       6       m   e   a   s   u   r   e   s       t   o       e   v   a   l   u   a   t   e       t   h   e       p   e   r   f   o   r   m   a   n   c   e   s       o   f       l   d   l       m   e   t   h   o   d   s   ,      w   h   i   c   h       c   o   m   p   u   t   e       t   h   e       a   v   e   r   a   g   e       s   i   m   i   l   a   r   i   t   y   /   d   i   s   t   a   n   c   e       b   e   t   w   e   e   n       t   h   e       p   r   e   d   i   c   t   e   d       r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s       a   n   d       t   h   e       r   e   a   l      r   a   t   i   n   g       d   i   s   t   r   i   b   u   t   i   o   n   s   ,       i   n   c   l   u   d   i   n   g       4       d   i   s   t   a   n   c   e       m   e   a   s   u   r   e   s       (   k   -   l   ,       e   u   c   l   i   d   e   a   n   ,       s   \\xcf\\x86   r   e   n   s   e   n   ,       s   q   u   a   r   e   d       \\xcf\\x87   2       )       a   n   d       t   w   o      s   i   m   i   l   a   r   i   t   y       m   e   a   s   u   r   e   s       (   f   i   d   e   l   i   t   y   ,       i   n   t   e   r   s   e   c   t   i   o   n   )   .      w   e       e   v   a   l   u   a   t   e       o   u   r       s   h   a   l   l   o   w       m   o   d   e   l       s   l   d   l   f   s       o   n       t   h   e   s   e       3       d   a   t   a   s   e   t   s       a   n   d       c   o   m   p   a   r   e       i   t       w   i   t   h       o   t   h   e   r       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t      s   t   a   n   d   -   a   l   o   n   e       l   d   l       m   e   t   h   o   d   s   .       t   h   e       r   e   s   u   l   t   s       o   f       s   l   d   l   f   s       a   n   d       t   h   e       c   o   m   p   e   t   i   t   o   r   s       a   r   e       s   u   m   m   a   r   i   z   e   d       i   n       t   a   b   l   e       1   .      f   o   r       m   o   v   i   e       w   e       q   u   o   t   e       t   h   e       r   e   s   u   l   t   s       r   e   p   o   r   t   e   d       i   n       [   2   7   ]   ,       a   s       t   h   e       c   o   d   e       o   f       [   2   7   ]       i   s       n   o   t       p   u   b   l   i   c   l   y       a   v   a   i   l   a   b   l   e   .       f   o   r       t   h   e      r   e   s   u   l   t   s       o   f       t   h   e       o   t   h   e   r   s       t   w   o   ,       w   e       r   u   n       c   o   d   e       t   h   a   t       t   h   e       a   u   t   h   o   r   s       h   a   d       m   a   d   e       a   v   a   i   l   a   b   l   e   .       i   n       a   l   l       c   a   s   e   ,       f   o   l   l   o   w   i   n   g       [   2   7   ,       6   ]   ,      w   e       s   p   l   i   t       e   a   c   h       d   a   t   a   s   e   t       i   n   t   o       1   0       f   i   x   e   d       f   o   l   d   s       a   n   d       d   o       s   t   a   n   d   a   r   d       t   e   n   -   f   o   l   d       c   r   o   s   s       v   a   l   i   d   a   t   i   o   n   ,       w   h   i   c   h       r   e   p   r   e   s   e   n   t   s      t   h   e       r   e   s   u   l   t       b   y       \\xe2\\x80\\x9c   m   e   a   n   \\xc2\\xb1   s   t   a   n   d   a   r   d       d   e   v   i   a   t   i   o   n   \\xe2\\x80\\x9d       a   n   d       m   a   t   t   e   r   s       l   e   s   s       h   o   w       t   r   a   i   n   i   n   g       a   n   d       t   e   s   t   i   n   g       d   a   t   a       g   e   t       d   i   v   i   d   e   d   .      a   s       c   a   n       b   e       s   e   e   n       f   r   o   m       t   a   b   l   e       1   ,       s   l   d   l   f   s       p   e   r   f   o   r   m       b   e   s   t       o   n       a   l   l       o   f       t   h   e       s   i   x       m   e   a   s   u   r   e   s   .      t   a   b   l   e       1   :       c   o   m   p   a   r   i   s   o   n       r   e   s   u   l   t   s       o   n       t   h   r   e   e       l   d   l       d   a   t   a   s   e   t   s       [   6   ]   .       \\xe2\\x80\\x9c   \\xe2\\x86\\x91   \\xe2\\x80\\x9d       a   n   d       \\xe2\\x80\\x9c   \\xe2\\x86\\x93   \\xe2\\x80\\x9d       i   n   d   i   c   a   t   e       t   h   e       l   a   r   g   e   r       a   n   d       t   h   e       s   m   a   l   l   e   r      t   h   e       b   e   t   t   e   r   ,       r   e   s   p   e   c   t   i   v   e   l   y   .      d   a   t   a   s   e   t         m   e   t   h   o   d         k   -   l       \\xe2\\x86\\x93         e   u   c   l   i   d   e   a   n       \\xe2\\x86\\x93         s   \\xcf\\x86   r   e   n   s   e   n       \\xe2\\x86\\x93         s   q   u   a   r   e   d       \\xcf\\x87   2       \\xe2\\x86\\x93         f   i   d   e   l   i   t   y       \\xe2\\x86\\x91         i   n   t   e   r   s   e   c   t   i   o   n       \\xe2\\x86\\x91         m   o   v   i   e         s   l   d   l   f       (   o   u   r   s   )      a   o   s   o   -   l   d   l   o   g   i   t   b   o   o   s   t       [   2   7   ]      l   d   l   o   g   i   t   b   o   o   s   t       [   2   7   ]      l   d   s   v   r       [   7   ]      b   f   g   s   -   l   d   l       [   6   ]      i   i   s   -   l   d   l       [   1   1   ]         0   .   0   7   3   \\xc2\\xb1   0   .   0   0   5      0   .   0   8   6   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   0   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   2   \\xc2\\xb1   0   .   0   0   5      0   .   0   9   9   \\xc2\\xb1   0   .   0   0   4      0   .   1   2   9   \\xc2\\xb1   0   .   0   0   7         0   .   1   3   3   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   5   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   9   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   8   \\xc2\\xb1   0   .   0   0   4      0   .   1   6   7   \\xc2\\xb1   0   .   0   0   4      0   .   1   8   7   \\xc2\\xb1   0   .   0   0   4         0   .   1   3   0   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   2   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   5   \\xc2\\xb1   0   .   0   0   3      0   .   1   5   6   \\xc2\\xb1   0   .   0   0   4      0   .   1   6   4   \\xc2\\xb1   0   .   0   0   3      0   .   1   8   3   \\xc2\\xb1   0   .   0   0   4         0   .   0   7   0   \\xc2\\xb1   0   .   0   0   4      0   .   0   8   4   \\xc2\\xb1   0   .   0   0   3      0   .   0   8   8   \\xc2\\xb1   0   .   0   0   3      0   .   0   8   8   \\xc2\\xb1   0   .   0   0   4      0   .   0   9   6   \\xc2\\xb1   0   .   0   0   4      0   .   1   2   0   \\xc2\\xb1   0   .   0   0   5         0   .   9   8   1   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   8   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   7   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   7   \\xc2\\xb1   0   .   0   0   1      0   .   9   7   4   \\xc2\\xb1   0   .   0   0   1      0   .   9   6   7   \\xc2\\xb1   0   .   0   0   1         0   .   8   7   0   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   8   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   5   \\xc2\\xb1   0   .   0   0   3      0   .   8   4   4   \\xc2\\xb1   0   .   0   0   4      0   .   8   3   6   \\xc2\\xb1   0   .   0   0   3      0   .   8   1   7   \\xc2\\xb1   0   .   0   0   4         s   l   d   l   f       (   o   u   r   s   )      l   d   s   v   r       [   7   ]      b   f   g   s   -   l   d   l       [   6   ]      i   i   s   -   l   d   l       [   1   1   ]         0   .   2   2   8   \\xc2\\xb1   0   .   0   0   6      0   .   2   4   5   \\xc2\\xb1   0   .   0   1   9      0   .   2   3   1   \\xc2\\xb1   0   .   0   2   1      0   .   2   3   9   \\xc2\\xb1   0   .   0   1   8         0   .   0   8   5   \\xc2\\xb1   0   .   0   0   2      0   .   0   9   9   \\xc2\\xb1   0   .   0   0   5      0   .   0   7   6   \\xc2\\xb1   0   .   0   0   6      0   .   0   8   9   \\xc2\\xb1   0   .   0   0   6         0   .   2   1   2   \\xc2\\xb1   0   .   0   0   2      0   .   2   2   9   \\xc2\\xb1   0   .   0   1   5      0   .   2   3   1   \\xc2\\xb1   0   .   0   1   2      0   .   2   5   3   \\xc2\\xb1   0   .   0   0   9         0   .   1   7   9   \\xc2\\xb1   0   .   0   0   4      0   .   1   8   9   \\xc2\\xb1   0   .   0   2   1      0   .   2   1   1   \\xc2\\xb1   0   .   0   1   8      0   .   2   0   5   \\xc2\\xb1   0   .   0   1   2         0   .   9   4   8   \\xc2\\xb1   0   .   0   0   1      0   .   9   4   0   \\xc2\\xb1   0   .   0   0   6      0   .   9   3   8   \\xc2\\xb1   0   .   0   0   8      0   .   9   4   4   \\xc2\\xb1   0   .   0   0   3         0   .   7   8   8   \\xc2\\xb1   0   .   0   0   2      0   .   7   7   1   \\xc2\\xb1   0   .   0   1   5      0   .   7   6   9   \\xc2\\xb1   0   .   0   1   2      0   .   7   4   7   \\xc2\\xb1   0   .   0   0   9         s   l   d   l   f       (   o   u   r   s   )      l   d   s   v   r       [   7   ]      b   f   g   s   -   l   d   l       [   6   ]      i   i   s   -   l   d   l       [   1   1   ]         0   .   5   3   4   \\xc2\\xb1   0   .   0   1   3      0   .   8   5   2   \\xc2\\xb1   0   .   0   2   3      0   .   8   5   6   \\xc2\\xb1   0   .   0   6   1      0   .   8   7   9   \\xc2\\xb1   0   .   0   2   3         0   .   3   1   7   \\xc2\\xb1   0   .   0   1   4      0   .   5   1   1   \\xc2\\xb1   0   .   0   2   1      0   .   4   7   5   \\xc2\\xb1   0   .   0   2   9      0   .   4   5   8   \\xc2\\xb1   0   .   0   1   4         0   .   3   3   6   \\xc2\\xb1   0   .   0   1   0      0   .   4   9   2   \\xc2\\xb1   0   .   0   1   6      0   .   5   0   8   \\xc2\\xb1   0   .   0   2   6      0   .   5   3   9   \\xc2\\xb1   0   .   0   1   1         0   .   4   4   8   \\xc2\\xb1   0   .   0   1   7      0   .   5   9   5   \\xc2\\xb1   0   .   0   2   6      0   .   7   1   6   \\xc2\\xb1   0   .   0   4   1      0   .   7   9   2   \\xc2\\xb1   0   .   0   1   9         0   .   8   2   4   \\xc2\\xb1   0   .   0   0   8      0   .   8   1   3   \\xc2\\xb1   0   .   0   0   8      0   .   7   2   2   \\xc2\\xb1   0   .   0   2   1      0   .   6   8   6   \\xc2\\xb1   0   .   0   0   9         0   .   6   6   4   \\xc2\\xb1   0   .   0   1   0      0   .   5   0   9   \\xc2\\xb1   0   .   0   1   6      0   .   4   9   2   \\xc2\\xb1   0   .   0   2   6      0   .   4   6   1   \\xc2\\xb1   0   .   0   1   1         h   u   m   a   n       g   e   n   e         n   a   t   u   r   a   l       s   c   e   n   e         4   .   2         e   v   a   l   u   a   t   i   o   n       o   f       d   l   d   l   f   s       o   n       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n         i   n       s   o   m   e       l   i   t   e   r   a   t   u   r   e       [   8   ,       1   1   ,       2   8   ,       1   5   ,       5   ]   ,       a   g   e       e   s   t   i   m   a   t   i   o   n       i   s       f   o   r   m   u   l   a   t   e   d       a   s       a       l   d   l       p   r   o   b   l   e   m   .       w   e       c   o   n   d   u   c   t      f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       e   x   p   e   r   i   m   e   n   t   s       o   n       m   o   r   p   h       [   2   4   ]   ,       w   h   i   c   h       c   o   n   t   a   i   n   s       m   o   r   e       t   h   a   n       5   0   ,   0   0   0       f   a   c   i   a   l       i   m   a   g   e   s      f   r   o   m       a   b   o   u   t       1   3   ,   0   0   0       p   e   o   p   l   e       o   f       d   i   f   f   e   r   e   n   t       r   a   c   e   s   .       e   a   c   h       f   a   c   i   a   l       i   m   a   g   e       i   s       a   n   n   o   t   a   t   e   d       w   i   t   h       a       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e   .      t   o       g   e   n   e   r   a   t   e       a   n       a   g   e       d   i   s   t   r   i   b   u   t   i   o   n       f   o   r       e   a   c   h       f   a   c   e       i   m   a   g   e   ,       w   e       f   o   l   l   o   w       t   h   e       s   a   m   e       s   t   r   a   t   e   g   y       u   s   e   d       i   n       [   8   ,       2   8   ,       5   ]   ,      w   h   i   c   h       u   s   e   s       a       g   a   u   s   s   i   a   n       d   i   s   t   r   i   b   u   t   i   o   n       w   h   o   s   e       m   e   a   n       i   s       t   h   e       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e       o   f       t   h   e       f   a   c   e       i   m   a   g   e       (   f   i   g   .       1   (   a   )   )   .      t   h   e       p   r   e   d   i   c   t   e   d       a   g   e       f   o   r       a       f   a   c   e       i   m   a   g   e       i   s       s   i   m   p   l   y       t   h   e       a   g   e       h   a   v   i   n   g       t   h   e       h   i   g   h   e   s   t       p   r   o   b   a   b   i   l   i   t   y       i   n       t   h   e       p   r   e   d   i   c   t   e   d      1         w   e       d   o   w   n   l   o   a   d       t   h   e   s   e       d   a   t   a   s   e   t   s       f   r   o   m       h   t   t   p   :   /   /   c   s   e   .   s   e   u   .   e   d   u   .   c   n   /   p   e   o   p   l   e   /   x   g   e   n   g   /   l   d   l   /   i   n   d   e   x   .   h   t   m   .         7         \\x0c   l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   .       t   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       i   s       e   v   a   l   u   a   t   e   d       b   y       t   h   e       m   e   a   n       a   b   s   o   l   u   t   e       e   r   r   o   r       (   m   a   e   )      b   e   t   w   e   e   n       p   r   e   d   i   c   t   e   d       a   g   e   s       a   n   d       c   h   r   o   n   o   l   o   g   i   c   a   l       a   g   e   s   .       a   s       t   h   e       c   u   r   r   e   n   t       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       r   e   s   u   l   t       o   n       m   o   r   p   h      i   s       o   b   t   a   i   n       b   y       f   i   n   e   -   t   u   n   i   n   g       d   l   d   l       [   5   ]       o   n       v   g   g   -   f   a   c   e       [   2   3   ]   ,       w   e       a   l   s   o       b   u   i   l   d       a       d   l   d   l   f       o   n       v   g   g   -   f   a   c   e   ,       b   y      r   e   p   l   a   c   i   n   g       t   h   e       s   o   f   t   m   a   x       l   a   y   e   r       i   n       v   g   g   n   e   t       b   y       a       l   d   l   f   .       f   o   l   l   o   w   i   n   g       [   5   ]   ,       w   e       d   o       s   t   a   n   d   a   r   d       1   0       t   e   n   -   f   o   l   d       c   r   o   s   s      v   a   l   i   d   a   t   i   o   n       a   n   d       t   h   e       r   e   s   u   l   t   s       a   r   e       s   u   m   m   a   r   i   z   e   d       i   n       t   a   b   l   e   .       2   ,       w   h   i   c   h       s   h   o   w   s       d   l   d   l   f       a   c   h   i   e   v   e       t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t      p   e   r   f   o   r   m   a   n   c   e       o   n       m   o   r   p   h   .       n   o   t   e       t   h   a   t   ,       t   h   e       s   i   g   n   i   f   i   c   a   n   t       p   e   r   f   o   r   m   a   n   c   e       g   a   i   n       b   e   t   w   e   e   n       d   e   e   p       l   d   l       m   o   d   e   l   s      (   d   l   d   l       a   n   d       d   l   d   l   f   )       a   n   d       n   o   n   -   d   e   e   p       l   d   l       m   o   d   e   l   s       (   i   i   s   -   l   d   l   ,       c   p   n   n   ,       b   f   g   s   -   l   d   l   )       a   n   d       t   h   e       s   u   p   e   r   i   o   r   i   t   y      o   f       d   l   d   l   f       c   o   m   p   a   r   e   d       w   i   t   h       d   l   d   l       v   e   r   i   f   i   e   s       t   h   e       e   f   f   e   c   t   i   v   e   n   e   s   s       o   f       e   n   d   -   t   o   -   e   n   d       l   e   a   r   n   i   n   g       a   n   d       o   u   r       t   r   e   e   -   b   a   s   e   d      m   o   d   e   l       f   o   r       l   d   l   ,       r   e   s   p   e   c   t   i   v   e   l   y   .      t   a   b   l   e       2   :       m   a   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       c   o   m   p   a   r   i   s   o   n       o   n       m   o   r   p   h       [   2   4   ]   .      m   e   t   h   o   d         i   i   s   -   l   d   l       [   1   1   ]         c   p   n   n       [   1   1   ]         b   f   g   s   -   l   d   l       [   6   ]         d   l   d   l   +   v   g   g   -   f   a   c   e       [   5   ]         d   l   d   l   f   +   v   g   g   -   f   a   c   e       (   o   u   r   s   )         m   a   e         5   .   6   7   \\xc2\\xb1   0   .   1   5         4   .   8   7   \\xc2\\xb1   0   .   3   1         3   .   9   4   \\xc2\\xb1   0   .   0   5         2   .   4   2   \\xc2\\xb1   0   .   0   1         2   .   2   4   \\xc2\\xb1   0   .   0   2         a   s       t   h   e       d   i   s   t   r   i   b   u   t   i   o   n       o   f       g   e   n   d   e   r       a   n   d       e   t   h   n   i   c   i   t   y       i   s       v   e   r   y       u   n   b   a   l   a   n   c   e   d       i   n       m   o   r   p   h   ,       m   a   n   y       a   g   e       e   s   t   i   m   a   t   i   o   n       m   e   t   h   o   d   s       [   1   3   ,       1   4   ,       1   5   ]       a   r   e       e   v   a   l   u   a   t   e   d       o   n       a       s   u   b   s   e   t       o   f       m   o   r   p   h   ,       c   a   l   l   e   d       m   o   r   p   h   _   s   u   b       f   o   r       s   h   o   r   t   ,       w   h   i   c   h       c   o   n   s   i   s   t   s       o   f      2   0   ,   1   6   0       s   e   l   e   c   t   e   d       f   a   c   i   a   l       i   m   a   g   e   s       t   o       a   v   o   i   d       t   h   e       i   n   f   l   u   e   n   c   e       o   f       u   n   b   a   l   a   n   c   e   d       d   i   s   t   r   i   b   u   t   i   o   n   .       t   h   e       b   e   s   t       p   e   r   f   o   r   m   a   n   c   e      r   e   p   o   r   t   e   d       o   n       m   o   r   p   h   _   s   u   b       i   s       g   i   v   e   n       b   y       d   2   l   d   l       [   1   5   ]   ,       a       d   a   t   a   -   d   e   p   e   n   d   e   n   t       l   d   l       m   e   t   h   o   d   .       a   s       d   2   l   d   l       u   s   e   d      t   h   e       o   u   t   p   u   t       o   f       t   h   e       \\xe2\\x80\\x9c   f   c   7   \\xe2\\x80\\x9d       l   a   y   e   r       i   n       a   l   e   x   n   e   t       [   2   1   ]       a   s       t   h   e       f   a   c   e       i   m   a   g   e       f   e   a   t   u   r   e   s   ,       h   e   r   e       w   e       i   n   t   e   g   r   a   t   e       a       l   d   l   f      w   i   t   h       a   l   e   x   n   e   t   .       f   o   l   l   o   w   i   n   g       t   h   e       e   x   p   e   r   i   m   e   n   t       s   e   t   t   i   n   g       u   s   e   d       i   n       d   2   l   d   l   ,       w   e       e   v   a   l   u   a   t   e       o   u   r       d   l   d   l   f       a   n   d       t   h   e      c   o   m   p   e   t   i   t   o   r   s   ,       i   n   c   l   u   d   i   n   g       b   o   t   h       s   l   l       a   n   d       l   d   l       b   a   s   e   d       m   e   t   h   o   d   s   ,       u   n   d   e   r       s   i   x       d   i   f   f   e   r   e   n   t       t   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o   s       (   1   0   %      t   o       6   0   %   )   .       a   l   l       o   f       t   h   e       c   o   m   p   e   t   i   t   o   r   s       a   r   e       t   r   a   i   n   e   d       o   n       t   h   e       s   a   m   e       d   e   e   p       f   e   a   t   u   r   e   s       u   s   e   d       b   y       d   2   l   d   l   .       a   s       c   a   n       b   e      s   e   e   n       f   r   o   m       t   a   b   l   e       3   ,       o   u   r       d   l   d   l   f   s       s   i   g   n   i   f   i   c   a   n   t   l   y       o   u   t   p   e   r   f   o   r   m       o   t   h   e   r   s       f   o   r       a   l   l       t   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o   s   .      n   o   t   e       t   h   a   t   ,       t   h   e       g   e   n   e   r   a   t   e   d       a   g   e       d   i   s   t   r   i   -       f   i   g   u   r   e       3   :       m   a   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       c   o   m   p   a   r   i   s   o   n       o   n      b   u   t   i   o   n   s       a   r   e       u   n   i   m   o   d   a   l       d   i   s   t   r   i   b   u   t   i   o   n   s       m   o   r   p   h   _   s   u   b   .      a   n   d       t   h   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s       u   s   e   d       i   n      t   r   a   i   n   i   n   g       s   e   t       r   a   t   i   o      m   e   t   h   o   d      s   e   c   .       4   .   1       a   r   e       m   i   x   t   u   r   e       d   i   s   t   r   i   b   u   t   i   o   n   s   .      1   0   %      2   0   %      3   0   %      4   0   %      5   0   %      6   0   %      t   h   e       p   r   o   p   o   s   e   d       m   e   t   h   o   d       l   d   l   f   s       a   c   h   i   e   v   e      a   a   s       [   2   2   ]      4   .   9   0   8   1      4   .   7   6   1   6      4   .   6   5   0   7      4   .   5   5   5   3      4   .   4   6   9   0      4   .   4   0   6   1      t   h   e       s   t   a   t   e   -   o   f   -   t   h   e   -   a   r   t       r   e   s   u   l   t   s       o   n       b   o   t   h       o   f      l   a   r   r       [   1   2   ]      4   .   7   5   0   1      4   .   6   1   1   2      4   .   5   1   3   1      4   .   4   2   7   3      4   .   3   5   0   0      4   .   2   9   4   9      i   i   s   -   a   l   d   l       [   9   ]      4   .   1   7   9   1      4   .   1   6   8   3      4   .   1   2   2   8      4   .   1   1   0   7      4   .   1   0   2   4      4   .   0   9   0   2      t   h   e   m   ,       w   h   i   c   h       v   e   r   i   f   i   e   s       t   h   a   t       o   u   r       m   o   d   e   l      d   2   l   d   l       [   1   5   ]      4   .   1   0   8   0      3   .   9   8   5   7      3   .   9   2   0   4      3   .   8   7   1   2      3   .   8   5   6   0      3   .   8   3   8   5      h   a   s       t   h   e       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l      d   l   d   l   f       (   o   u   r   s   )      3   .   8   4   9   5      3   .   6   2   2   0      3   .   3   9   9   1      3   .   2   4   0   1      3   .   1   9   1   7      3   .   1   2   2   4      f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .      4   .   3         t   i   m   e       c   o   m   p   l   e   x   i   t   y         l   e   t       h       a   n   d       s   b       b   e       t   h   e       t   r   e   e       d   e   p   t   h       a   n   d       t   h   e      b   a   t   c   h       s   i   z   e   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       e   a   c   h       t   r   e   e       h   a   s       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1       s   p   l   i   t       n   o   d   e   s       a   n   d       2   h   \\xe2\\x88\\x92   1       l   e   a   f       n   o   d   e   s   .       l   e   t       d       =       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1   .      f   o   r       o   n   e       t   r   e   e       a   n   d       o   n   e       s   a   m   p   l   e   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a       f   o   r   w   a   r   d       p   a   s   s       a   n   d       a       b   a   c   k   w   a   r   d       p   a   s   s       a   r   e       o   (   d       +      d       +       1   \\xc3\\x97   c   )       =       o   (   d   \\xc3\\x97   c   )       a   n   d       o   (   d       +       1   \\xc3\\x97   c       +       d   \\xc3\\x97   c   )       =       o   (   d   \\xc3\\x97   c   )   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       s   o       f   o   r       k       t   r   e   e   s       a   n   d      n   b       b   a   t   c   h   e   s   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a       f   o   r   w   a   r   d       a   n   d       b   a   c   k   w   a   r   d       p   a   s   s       i   s       o   (   d   \\xc3\\x97   c   \\xc3\\x97   k   \\xc3\\x97   n   b       \\xc3\\x97   s   b       )   .       t   h   e       c   o   m   p   l   e   x   i   t   y       o   f       a   n       i   t   e   r   a   t   i   o   n       t   o       u   p   d   a   t   e       l   e   a   f       n   o   d   e   s       a   r   e       o   (   n   b       \\xc3\\x97   s   b       \\xc3\\x97   k   \\xc3\\x97   c   \\xc3\\x97   d       +       1   )       =       o   (   d   \\xc3\\x97   c   \\xc3\\x97   k   \\xc3\\x97   n   b       \\xc3\\x97   s   b       )   .      t   h   u   s   ,       t   h   e       c   o   m   p   l   e   x   i   t   y       f   o   r       t   h   e       t   r   a   i   n   i   n   g       p   r   o   c   e   d   u   r   e       (   o   n   e       e   p   o   c   h   ,       n   b       b   a   t   c   h   e   s   )       a   n   d       t   h   e       t   e   s   t   i   n   g       p   r   o   c   e   d   u   r   e      (   o   n   e       s   a   m   p   l   e   )       a   r   e       o   (   d   \\xc3\\x97   c   \\xc3\\x97   k   \\xc3\\x97   n   b       \\xc3\\x97   s   b       )       a   n   d       o   (   d   \\xc3\\x97   c   \\xc3\\x97   k   )   ,       r   e   s   p   e   c   t   i   v   e   l   y   .       l   d   l   f   s       a   r   e       e   f   f   i   c   i   e   n   t   :       o   n      m   o   r   p   h   _   s   u   b       (   1   2   6   3   6       t   r   a   i   n   i   n   g       i   m   a   g   e   s   ,       8   4   2   4       t   e   s   t   i   n   g       i   m   a   g   e   s   )   ,       o   u   r       m   o   d   e   l       o   n   l   y       t   a   k   e   s       5   2   5   0   s       f   o   r       t   r   a   i   n   i   n   g      (   2   5   0   0   0       i   t   e   r   a   t   i   o   n   s   )       a   n   d       8   s       f   o   r       t   e   s   t   i   n   g       a   l   l       8   4   2   4       i   m   a   g   e   s   .      4   .   4         p   a   r   a   m   e   t   e   r       d   i   s   c   u   s   s   i   o   n         n   o   w       w   e       d   i   s   c   u   s   s       t   h   e       i   n   f   l   u   e   n   c   e       o   f       p   a   r   a   m   e   t   e   r       s   e   t   t   i   n   g   s       o   n       p   e   r   f   o   r   m   a   n   c   e   .       w   e       r   e   p   o   r   t       t   h   e       r   e   s   u   l   t   s       o   f       r   a   t   i   n   g      p   r   e   d   i   c   t   i   o   n       o   n       m   o   v   i   e       (   m   e   a   s   u   r   e   d       b   y       k   -   l   )       a   n   d       a   g   e       e   s   t   i   m   a   t   i   o   n       o   n       m   o   r   p   h   _   s   u   b       w   i   t   h       6   0   %       t   r   a   i   n   i   n   g       s   e   t      r   a   t   i   o       (   m   e   a   s   u   r   e   d       b   y       m   a   e   )       f   o   r       d   i   f   f   e   r   e   n   t       p   a   r   a   m   e   t   e   r       s   e   t   t   i   n   g   s       i   n       t   h   i   s       s   e   c   t   i   o   n   .      t   r   e   e       n   u   m   b   e   r   .       a   s       a       f   o   r   e   s   t       i   s       a   n       e   n   s   e   m   b   l   e       m   o   d   e   l   ,       i   t       i   s       n   e   c   e   s   s   a   r   y       t   o       i   n   v   e   s   t   i   g   a   t   e       h   o   w       p   e   r   f   o   r   m   a   n   c   e   s      c   h   a   n   g   e       b   y       v   a   r   y   i   n   g       t   h   e       t   r   e   e       n   u   m   b   e   r       u   s   e   d       i   n       a       f   o   r   e   s   t   .       n   o   t   e       t   h   a   t   ,       a   s       w   e       d   i   s   c   u   s   s   e   d       i   n       s   e   c   .       2   ,       t   h   e      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       t   o       l   e   a   r   n       a       f   o   r   e   s   t       p   r   o   p   o   s   e   d       i   n       d   n   d   f   s       [   2   0   ]       i   s       d   i   f   f   e   r   e   n   t       f   r   o   m       o   u   r   s   .       t   h   e   r   e   f   o   r   e   ,       i   t       i   s      n   e   c   e   s   s   a   r   y       t   o       s   e   e       w   h   i   c   h       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       i   s       b   e   t   t   e   r       t   o       l   e   a   r   n       a       f   o   r   e   s   t   .       t   o   w   a   r   d   s       t   h   i   s       e   n   d   ,       w   e       r   e   p   l   a   c   e       o   u   r      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       i   n       d   l   d   l   f   s       b   y       t   h   e       o   n   e       u   s   e   d       i   n       d   n   d   f   s   ,       a   n   d       n   a   m   e       t   h   i   s       m   e   t   h   o   d       d   n   d   f   s   -   l   d   l   .       t   h   e      c   o   r   r   e   s   p   o   n   d   i   n   g       s   h   a   l   l   o   w       m   o   d   e   l       i   s       n   a   m   e   d       b   y       s   n   d   f   s   -   l   d   l   .       w   e       f   i   x       o   t   h   e   r       p   a   r   a   m   e   t   e   r   s   ,       i   .   e   .   ,       t   r   e   e       d   e   p   t   h       a   n   d      8         \\x0c   o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n   ,       a   s       t   h   e       d   e   f   a   u   l   t       s   e   t   t   i   n   g   .       a   s       s   h   o   w   n       i   n       f   i   g   .       4       (   a   )   ,       o   u   r      e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       c   a   n       i   m   p   r   o   v   e       t   h   e       p   e   r   f   o   r   m   a   n   c   e       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       w   h   i   l   e       t   h   e       o   n   e       u   s   e   d       i   n       d   n   d   f   s      e   v   e   n       l   e   a   d   s       t   o       a       w   o   r   s   e       p   e   r   f   o   r   m   a   n   c   e       t   h   a   n       o   n   e       f   o   r       a       s   i   n   g   l   e       t   r   e   e   .      o   b   s   e   r   v   e   d       f   r   o   m       f   i   g   .       4   ,       t   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       l   d   l   f   s       c   a   n       b   e       i   m   p   r   o   v   e   d       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       b   u   t       t   h   e      i   m   p   r   o   v   e   m   e   n   t       b   e   c   o   m   e   s       i   n   c   r   e   a   s   i   n   g   l   y       s   m   a   l   l   e   r       a   n   d       s   m   a   l   l   e   r   .       t   h   e   r   e   f   o   r   e   ,       u   s   i   n   g       m   u   c   h       l   a   r   g   e   r       e   n   s   e   m   b   l   e   s      d   o   e   s       n   o   t       y   i   e   l   d       a       b   i   g       i   m   p   r   o   v   e   m   e   n   t       (   o   n       m   o   v   i   e   ,       t   h   e       n   u   m   b   e   r       o   f       t   r   e   e   s       k       =       1   0   0   :       k   -   l       =       0   .   0   7   0       v   s       k       =       2   0   :      k   -   l       =       0   .   0   7   1   )   .       n   o   t   e       t   h   a   t   ,       n   o   t       a   l   l       r   a   n   d   o   m       f   o   r   e   s   t   s       b   a   s   e   d       m   e   t   h   o   d   s       u   s   e       a       l   a   r   g   e       n   u   m   b   e   r       o   f       t   r   e   e   s   ,       e   .   g   .   ,      s   h   o   t   t   o   n       e   t       a   l   .       [   2   5   ]       o   b   t   a   i   n   e   d       v   e   r   y       g   o   o   d       p   o   s   e       e   s   t   i   m   a   t   i   o   n       r   e   s   u   l   t   s       f   r   o   m       d   e   p   t   h       i   m   a   g   e   s       b   y       o   n   l   y       3       d   e   c   i   s   i   o   n      t   r   e   e   s   .      t   r   e   e       d   e   p   t   h   .       t   r   e   e       d   e   p   t   h       i   s       a   n   o   t   h   e   r       i   m   p   o   r   t   a   n   t       p   a   r   a   m   e   t   e   r       f   o   r       d   e   c   i   s   i   o   n       t   r   e   e   s   .       i   n       l   d   l   f   s   ,       t   h   e   r   e       i   s       a   n      i   m   p   l   i   c   i   t       c   o   n   s   t   r   a   i   n   t       b   e   t   w   e   e   n       t   r   e   e       d   e   p   t   h       h       a   n   d       o   u   t   p   u   t       u   n   i   t       n   u   m   b   e   r       o   f       t   h   e       f   e   a   t   u   r   e       l   e   a   r   n   i   n   g       f   u   n   c   t   i   o   n       \\xcf\\x84       :      \\xcf\\x84       \\xe2\\x89\\xa5       2   h   \\xe2\\x88\\x92   1       \\xe2\\x88\\x92       1   .       t   o       d   i   s   c   u   s   s       t   h   e       i   n   f   l   u   e   n   c   e       o   f       t   r   e   e       d   e   p   t   h       t   o       t   h   e       p   e   r   f   o   r   m   a   n   c   e       o   f       d   l   d   l   f   s   ,       w   e       s   e   t       \\xcf\\x84       =       2   h   \\xe2\\x88\\x92   1      a   n   d       f   i   x       t   r   e   e       n   u   m   b   e   r       k       =       1   ,       a   n   d       t   h   e       p   e   r   f   o   r   m   a   n   c   e       c   h   a   n   g   e       b   y       v   a   r   y   i   n   g       t   r   e   e       d   e   p   t   h       i   s       s   h   o   w   n       i   n       f   i   g   .       4       (   b   )   .      w   e       s   e   e       t   h   a   t       t   h   e       p   e   r   f   o   r   m   a   n   c   e       f   i   r   s   t       i   m   p   r   o   v   e   s       t   h   e   n       d   e   c   r   e   a   s   e   s       w   i   t   h       t   h   e       i   n   c   r   e   a   s   e       o   f       t   h   e       t   r   e   e       d   e   p   t   h   .       t   h   e      r   e   a   s   o   n       i   s       a   s       t   h   e       t   r   e   e       d   e   p   t   h       i   n   c   r   e   a   s   e   s   ,       t   h   e       d   i   m   e   n   s   i   o   n       o   f       l   e   a   r   n   e   d       f   e   a   t   u   r   e   s       i   n   c   r   e   a   s   e   s       e   x   p   o   n   e   n   t   i   a   l   l   y   ,       w   h   i   c   h      g   r   e   a   t   l   y       i   n   c   r   e   a   s   e   s       t   h   e       t   r   a   i   n   i   n   g       d   i   f   f   i   c   u   l   t   y   .       s   o       u   s   i   n   g       m   u   c   h       l   a   r   g   e   r       d   e   p   t   h   s       m   a   y       l   e   a   d       t   o       b   a   d       p   e   r   f   o   r   m   a   n   c   e      (   o   n       m   o   v   i   e   ,       t   r   e   e       d   e   p   t   h       h       =       1   8   :       k   -   l       =       0   .   1   1   6   2       v   s       h       =       9   :       k   -   l       =       0   .   0   8   3   1   )   .         f   i   g   u   r   e       4   :       t   h   e       p   e   r   f   o   r   m   a   n   c   e       c   h   a   n   g   e       o   f       a   g   e       e   s   t   i   m   a   t   i   o   n       o   n       m   o   r   p   h   _   s   u   b       a   n   d       r   a   t   i   n   g       p   r   e   d   i   c   t   i   o   n       o   n       m   o   v   i   e      b   y       v   a   r   y   i   n   g       (   a   )       t   r   e   e       n   u   m   b   e   r       a   n   d       (   b   )       t   r   e   e       d   e   p   t   h   .       o   u   r       a   p   p   r   o   a   c   h       (   d   l   d   l   f   s   /   s   l   d   l   f   s   )       c   a   n       i   m   p   r   o   v   e       t   h   e      p   e   r   f   o   r   m   a   n   c   e       b   y       u   s   i   n   g       m   o   r   e       t   r   e   e   s   ,       w   h   i   l   e       u   s   i   n   g       t   h   e       e   n   s   e   m   b   l   e       s   t   r   a   t   e   g   y       p   r   o   p   o   s   e   d       i   n       d   n   d   f   s       (   d   n   d   f   s   l   d   l   /   s   n   d   f   s   -   l   d   l   )       e   v   e   n       l   e   a   d   s       t   o       a       w   o   r   s   e       p   e   r   f   o   r   m   a   n   c   e       t   h   a   n       o   n   e       f   o   r       a       s   i   n   g   l   e       t   r   e   e   .         5         c   o   n   c   l   u   s   i   o   n         w   e       p   r   e   s   e   n   t       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r   e   s   t   s   ,       a       n   o   v   e   l       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       a   l   g   o   r   i   t   h   m       i   n   s   p   i   r   e   d       b   y      d   i   f   f   e   r   e   n   t   i   a   b   l   e       d   e   c   i   s   i   o   n       t   r   e   e   s   .       w   e       d   e   f   i   n   e   d       a       d   i   s   t   r   i   b   u   t   i   o   n   -   b   a   s   e   d       l   o   s   s       f   u   n   c   t   i   o   n       f   o   r       t   h   e       f   o   r   e   s   t   s       a   n   d       f   o   u   n   d      t   h   a   t       t   h   e       l   e   a   f       n   o   d   e       p   r   e   d   i   c   t   i   o   n   s       c   a   n       b   e       o   p   t   i   m   i   z   e   d       v   i   a       v   a   r   i   a   t   i   o   n   a   l       b   o   u   n   d   i   n   g   ,       w   h   i   c   h       e   n   a   b   l   e   s       a   l   l       t   h   e       t   r   e   e   s      a   n   d       t   h   e       f   e   a   t   u   r   e       t   h   e   y       u   s   e       t   o       b   e       l   e   a   r   n   e   d       j   o   i   n   t   l   y       i   n       a   n       e   n   d   -   t   o   -   e   n   d       m   a   n   n   e   r   .       e   x   p   e   r   i   m   e   n   t   a   l       r   e   s   u   l   t   s       s   h   o   w   e   d      t   h   e       s   u   p   e   r   i   o   r   i   t   y       o   f       o   u   r       a   l   g   o   r   i   t   h   m       f   o   r       s   e   v   e   r   a   l       l   d   l       t   a   s   k   s       a   n   d       a       r   e   l   a   t   e   d       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   p   p   l   i   c   a   t   i   o   n   ,       a   n   d      v   e   r   i   f   i   e   d       o   u   r       m   o   d   e   l       h   a   s       t   h   e       a   b   i   l   i   t   y       t   o       m   o   d   e   l       a   n   y       g   e   n   e   r   a   l       f   o   r   m       o   f       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .      a   c   k   n   o   w   l   e   d   g   e   m   e   n   t   .       t   h   i   s       w   o   r   k       w   a   s       s   u   p   p   o   r   t   e   d       i   n       p   a   r   t       b   y       t   h   e       n   a   t   i   o   n   a   l       n   a   t   u   r   a   l       s   c   i   e   n   c   e       f   o   u   n   d   a   t   i   o   n       o   f      c   h   i   n   a       n   o   .       6   1   6   7   2   3   3   6   ,       i   n       p   a   r   t       b   y       \\xe2\\x80\\x9c   c   h   e   n       g   u   a   n   g   \\xe2\\x80\\x9d       p   r   o   j   e   c   t       s   u   p   p   o   r   t   e   d       b   y       s   h   a   n   g   h   a   i       m   u   n   i   c   i   p   a   l       e   d   u   c   a   t   i   o   n      c   o   m   m   i   s   s   i   o   n       a   n   d       s   h   a   n   g   h   a   i       e   d   u   c   a   t   i   o   n       d   e   v   e   l   o   p   m   e   n   t       f   o   u   n   d   a   t   i   o   n       n   o   .       1   5   c   g   4   3       a   n   d       i   n       p   a   r   t       b   y       o   n   r      n   0   0   0   1   4   -   1   5   -   1   -   2   3   5   6   .         r   e   f   e   r   e   n   c   e   s      [   1   ]       y   .       a   m   i   t       a   n   d       d   .       g   e   m   a   n   .       s   h   a   p   e       q   u   a   n   t   i   z   a   t   i   o   n       a   n   d       r   e   c   o   g   n   i   t   i   o   n       w   i   t   h       r   a   n   d   o   m   i   z   e   d       t   r   e   e   s   .       n   e   u   r   a   l       c   o   m   p   u   t   a   t   i   o   n   ,      9   (   7   )   :   1   5   4   5   \\xe2\\x80\\x93   1   5   8   8   ,       1   9   9   7   .      [   2   ]       a   .       l   .       b   e   r   g   e   r   ,       s   .       d   .       p   i   e   t   r   a   ,       a   n   d       v   .       j   .       d   .       p   i   e   t   r   a   .       a       m   a   x   i   m   u   m       e   n   t   r   o   p   y       a   p   p   r   o   a   c   h       t   o       n   a   t   u   r   a   l       l   a   n   g   u   a   g   e       p   r   o   c   e   s   s   i   n   g   .      c   o   m   p   u   t   a   t   i   o   n   a   l       l   i   n   g   u   i   s   t   i   c   s   ,       2   2   (   1   )   :   3   9   \\xe2\\x80\\x93   7   1   ,       1   9   9   6   .      [   3   ]       l   .       b   r   e   i   m   a   n   .       r   a   n   d   o   m       f   o   r   e   s   t   s   .       m   a   c   h   i   n   e       l   e   a   r   n   i   n   g   ,       4   5   (   1   )   :   5   \\xe2\\x80\\x93   3   2   ,       2   0   0   1   .      [   4   ]       a   .       c   r   i   m   i   n   i   s   i       a   n   d       j   .       s   h   o   t   t   o   n   .       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s       f   o   r       c   o   m   p   u   t   e   r       v   i   s   i   o   n       a   n   d       m   e   d   i   c   a   l       i   m   a   g   e       a   n   a   l   y   s   i   s   .       s   p   r   i   n   g   e   r   ,      2   0   1   3   .      [   5   ]       b   .   -   b   .       g   a   o   ,       c   .       x   i   n   g   ,       c   .   -   w   .       x   i   e   ,       j   .       w   u   ,       a   n   d       x   .       g   e   n   g   .       d   e   e   p       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       w   i   t   h       l   a   b   e   l       a   m   b   i   g   u   i   t   y   .      a   r   x   i   v   :   1   6   1   1   .   0   1   7   3   1   ,       2   0   1   7   .      [   6   ]       x   .       g   e   n   g   .       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       i   e   e   e       t   r   a   n   s   .       k   n   o   w   l   .       d   a   t   a       e   n   g   .   ,       2   8   (   7   )   :   1   7   3   4   \\xe2\\x80\\x93   1   7   4   8   ,       2   0   1   6   .         9         \\x0c   [   7   ]       x   .       g   e   n   g       a   n   d       p   .       h   o   u   .       p   r   e   -   r   e   l   e   a   s   e       p   r   e   d   i   c   t   i   o   n       o   f       c   r   o   w   d       o   p   i   n   i   o   n       o   n       m   o   v   i   e   s       b   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       i   n      p   r   o   .       i   j   c   a   i   ,       p   a   g   e   s       3   5   1   1   \\xe2\\x80\\x93   3   5   1   7   ,       2   0   1   5   .      [   8   ]       x   .       g   e   n   g   ,       k   .       s   m   i   t   h   -   m   i   l   e   s   ,       a   n   d       z   .       z   h   o   u   .       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       l   e   a   r   n   i   n   g       f   r   o   m       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       i   n       p   r   o   c   .      a   a   a   i   ,       2   0   1   0   .      [   9   ]       x   .       g   e   n   g   ,       q   .       w   a   n   g   ,       a   n   d       y   .       x   i   a   .       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       a   d   a   p   t   i   v   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       i   n       p   r   o   c   .      i   c   p   r   ,       p   a   g   e   s       4   4   6   5   \\xe2\\x80\\x93   4   4   7   0   ,       2   0   1   4   .      [   1   0   ]       x   .       g   e   n   g       a   n   d       y   .       x   i   a   .       h   e   a   d       p   o   s   e       e   s   t   i   m   a   t   i   o   n       b   a   s   e   d       o   n       m   u   l   t   i   v   a   r   i   a   t   e       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   .       i   n       p   r   o   c   .       c   v   p   r   ,       p   a   g   e   s      1   8   3   7   \\xe2\\x80\\x93   1   8   4   2   ,       2   0   1   4   .      [   1   1   ]       x   .       g   e   n   g   ,       c   .       y   i   n   ,       a   n   d       z   .       z   h   o   u   .       f   a   c   i   a   l       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       l   e   a   r   n   i   n   g       f   r   o   m       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n   s   .       i   e   e   e       t   r   a   n   s   .      p   a   t   t   e   r   n       a   n   a   l   .       m   a   c   h   .       i   n   t   e   l   l   .   ,       3   5   (   1   0   )   :   2   4   0   1   \\xe2\\x80\\x93   2   4   1   2   ,       2   0   1   3   .      [   1   2   ]       g   .       g   u   o   ,       y   .       f   u   ,       c   .       r   .       d   y   e   r   ,       a   n   d       t   .       s   .       h   u   a   n   g   .       i   m   a   g   e   -   b   a   s   e   d       h   u   m   a   n       a   g   e       e   s   t   i   m   a   t   i   o   n       b   y       m   a   n   i   f   o   l   d       l   e   a   r   n   i   n   g       a   n   d      l   o   c   a   l   l   y       a   d   j   u   s   t   e   d       r   o   b   u   s   t       r   e   g   r   e   s   s   i   o   n   .       i   e   e   e       t   r   a   n   s   .       i   m   a   g   e       p   r   o   c   e   s   s   i   n   g   ,       1   7   (   7   )   :   1   1   7   8   \\xe2\\x80\\x93   1   1   8   8   ,       2   0   0   8   .      [   1   3   ]       g   .       g   u   o       a   n   d       g   .       m   u   .       h   u   m   a   n       a   g   e       e   s   t   i   m   a   t   i   o   n   :       w   h   a   t       i   s       t   h   e       i   n   f   l   u   e   n   c   e       a   c   r   o   s   s       r   a   c   e       a   n   d       g   e   n   d   e   r   ?       i   n       c   v   p   r      w   o   r   k   s   h   o   p   s   ,       p   a   g   e   s       7   1   \\xe2\\x80\\x93   7   8   ,       2   0   1   0   .      [   1   4   ]       g   .       g   u   o       a   n   d       c   .       z   h   a   n   g   .       a       s   t   u   d   y       o   n       c   r   o   s   s   -   p   o   p   u   l   a   t   i   o   n       a   g   e       e   s   t   i   m   a   t   i   o   n   .       i   n       p   r   o   c   .       c   v   p   r   ,       p   a   g   e   s       4   2   5   7   \\xe2\\x80\\x93   4   2   6   3   ,      2   0   1   4   .      [   1   5   ]       z   .       h   e   ,       x   .       l   i   ,       z   .       z   h   a   n   g   ,       f   .       w   u   ,       x   .       g   e   n   g   ,       y   .       z   h   a   n   g   ,       m   .   -   h   .       y   a   n   g   ,       a   n   d       y   .       z   h   u   a   n   g   .       d   a   t   a   -   d   e   p   e   n   d   e   n   t       l   a   b   e   l      d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r       a   g   e       e   s   t   i   m   a   t   i   o   n   .       i   e   e   e       t   r   a   n   s   .       o   n       i   m   a   g   e       p   r   o   c   e   s   s   i   n   g   ,       2   0   1   7   .      [   1   6   ]       t   .       k   .       h   o   .       r   a   n   d   o   m       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       i   n       p   r   o   c   .       i   c   d   a   r   ,       p   a   g   e   s       2   7   8   \\xe2\\x80\\x93   2   8   2   ,       1   9   9   5   .      [   1   7   ]       t   .       k   .       h   o   .       t   h   e       r   a   n   d   o   m       s   u   b   s   p   a   c   e       m   e   t   h   o   d       f   o   r       c   o   n   s   t   r   u   c   t   i   n   g       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       i   e   e   e       t   r   a   n   s   .       p   a   t   t   e   r   n       a   n   a   l   .       m   a   c   h   .      i   n   t   e   l   l   .   ,       2   0   (   8   )   :   8   3   2   \\xe2\\x80\\x93   8   4   4   ,       1   9   9   8   .      [   1   8   ]       y   .       j   i   a   ,       e   .       s   h   e   l   h   a   m   e   r   ,       j   .       d   o   n   a   h   u   e   ,       s   .       k   a   r   a   y   e   v   ,       j   .       l   o   n   g   ,       r   .       g   i   r   s   h   i   c   k   ,       s   .       g   u   a   d   a   r   r   a   m   a   ,       a   n   d       t   .       d   a   r   r   e   l   l   .       c   a   f   f   e   :      c   o   n   v   o   l   u   t   i   o   n   a   l       a   r   c   h   i   t   e   c   t   u   r   e       f   o   r       f   a   s   t       f   e   a   t   u   r   e       e   m   b   e   d   d   i   n   g   .       a   r   x   i   v       p   r   e   p   r   i   n   t       a   r   x   i   v   :   1   4   0   8   .   5   0   9   3   ,       2   0   1   4   .      [   1   9   ]       m   .       i   .       j   o   r   d   a   n   ,       z   .       g   h   a   h   r   a   m   a   n   i   ,       t   .       s   .       j   a   a   k   k   o   l   a   ,       a   n   d       l   .       k   .       s   a   u   l   .       a   n       i   n   t   r   o   d   u   c   t   i   o   n       t   o       v   a   r   i   a   t   i   o   n   a   l       m   e   t   h   o   d   s       f   o   r      g   r   a   p   h   i   c   a   l       m   o   d   e   l   s   .       m   a   c   h   i   n   e       l   e   a   r   n   i   n   g   ,       3   7   (   2   )   :   1   8   3   \\xe2\\x80\\x93   2   3   3   ,       1   9   9   9   .      [   2   0   ]       p   .       k   o   n   t   s   c   h   i   e   d   e   r   ,       m   .       f   i   t   e   r   a   u   ,       a   .       c   r   i   m   i   n   i   s   i   ,       a   n   d       s   .       r   .       b   u   l   \\xc3\\xb2   .       d   e   e   p       n   e   u   r   a   l       d   e   c   i   s   i   o   n       f   o   r   e   s   t   s   .       i   n       p   r   o   c   .       i   c   c   v   ,      p   a   g   e   s       1   4   6   7   \\xe2\\x80\\x93   1   4   7   5   ,       2   0   1   5   .      [   2   1   ]       a   .       k   r   i   z   h   e   v   s   k   y   ,       i   .       s   u   t   s   k   e   v   e   r   ,       a   n   d       g   .       e   .       h   i   n   t   o   n   .       i   m   a   g   e   n   e   t       c   l   a   s   s   i   f   i   c   a   t   i   o   n       w   i   t   h       d   e   e   p       c   o   n   v   o   l   u   t   i   o   n   a   l       n   e   u   r   a   l      n   e   t   w   o   r   k   s   .       i   n       p   r   o   c   .       n   i   p   s   ,       p   a   g   e   s       1   1   0   6   \\xe2\\x80\\x93   1   1   1   4   ,       2   0   1   2   .      [   2   2   ]       a   .       l   a   n   i   t   i   s   ,       c   .       d   r   a   g   a   n   o   v   a   ,       a   n   d       c   .       c   h   r   i   s   t   o   d   o   u   l   o   u   .       c   o   m   p   a   r   i   n   g       d   i   f   f   e   r   e   n   t       c   l   a   s   s   i   f   i   e   r   s       f   o   r       a   u   t   o   m   a   t   i   c       a   g   e      e   s   t   i   m   a   t   i   o   n   .       i   e   e   e       t   r   a   n   s   .       o   n       c   y   b   e   r   n   e   t   i   c   s   ,   ,       3   4   (   1   )   :   6   2   1   \\xe2\\x80\\x93   6   2   8   ,       2   0   0   4   .      [   2   3   ]       o   .       m   .       p   a   r   k   h   i   ,       a   .       v   e   d   a   l   d   i   ,       a   n   d       a   .       z   i   s   s   e   r   m   a   n   .       d   e   e   p       f   a   c   e       r   e   c   o   g   n   i   t   i   o   n   .       i   n       p   r   o   c   .       b   m   v   c   ,       p   a   g   e   s       4   1   .   1   \\xe2\\x80\\x93   4   1   .   1   2   ,      2   0   1   5   .      [   2   4   ]       k   .       r   i   c   a   n   e   k       a   n   d       t   .       t   e   s   a   f   a   y   e   .       m   o   r   p   h   :       a       l   o   n   g   i   t   u   d   i   n   a   l       i   m   a   g   e       d   a   t   a   b   a   s   e       o   f       n   o   r   m   a   l       a   d   u   l   t       a   g   e   -   p   r   o   g   r   e   s   s   i   o   n   .       i   n      p   r   o   c   .       f   g   ,       p   a   g   e   s       3   4   1   \\xe2\\x80\\x93   3   4   5   ,       2   0   0   6   .      [   2   5   ]       j   .       s   h   o   t   t   o   n   ,       a   .       w   .       f   i   t   z   g   i   b   b   o   n   ,       m   .       c   o   o   k   ,       t   .       s   h   a   r   p   ,       m   .       f   i   n   o   c   c   h   i   o   ,       r   .       m   o   o   r   e   ,       a   .       k   i   p   m   a   n   ,       a   n   d       a   .       b   l   a   k   e   .      r   e   a   l   -   t   i   m   e       h   u   m   a   n       p   o   s   e       r   e   c   o   g   n   i   t   i   o   n       i   n       p   a   r   t   s       f   r   o   m       s   i   n   g   l   e       d   e   p   t   h       i   m   a   g   e   s   .       i   n       p   r   o   c   .       c   v   p   r   ,       p   a   g   e   s       1   2   9   7   \\xe2\\x80\\x93   1   3   0   4   ,      2   0   1   1   .      [   2   6   ]       g   .       t   s   o   u   m   a   k   a   s       a   n   d       i   .       k   a   t   a   k   i   s   .       m   u   l   t   i   -   l   a   b   e   l       c   l   a   s   s   i   f   i   c   a   t   i   o   n   :       a   n       o   v   e   r   v   i   e   w   .       i   n   t   e   r   n   a   t   i   o   n   a   l       j   o   u   r   n   a   l       o   f       d   a   t   a      w   a   r   e   h   o   u   s   i   n   g       a   n   d       m   i   n   i   n   g   ,       3   (   3   )   :   1   \\xe2\\x80\\x93   1   3   ,       2   0   0   7   .      [   2   7   ]       c   .       x   i   n   g   ,       x   .       g   e   n   g   ,       a   n   d       h   .       x   u   e   .       l   o   g   i   s   t   i   c       b   o   o   s   t   i   n   g       r   e   g   r   e   s   s   i   o   n       f   o   r       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g   .       i   n       p   r   o   c   .       c   v   p   r   ,      p   a   g   e   s       4   4   8   9   \\xe2\\x80\\x93   4   4   9   7   ,       2   0   1   6   .      [   2   8   ]       x   .       y   a   n   g   ,       x   .       g   e   n   g   ,       a   n   d       d   .       z   h   o   u   .       s   p   a   r   s   i   t   y       c   o   n   d   i   t   i   o   n   a   l       e   n   e   r   g   y       l   a   b   e   l       d   i   s   t   r   i   b   u   t   i   o   n       l   e   a   r   n   i   n   g       f   o   r       a   g   e       e   s   t   i   m   a   t   i   o   n   .      i   n       p   r   o   c   .       i   j   c   a   i   ,       p   a   g   e   s       2   2   5   9   \\xe2\\x80\\x93   2   2   6   5   ,       2   0   1   6   .      [   2   9   ]       a   .       l   .       y   u   i   l   l   e       a   n   d       a   .       r   a   n   g   a   r   a   j   a   n   .       t   h   e       c   o   n   c   a   v   e   -   c   o   n   v   e   x       p   r   o   c   e   d   u   r   e   .       n   e   u   r   a   l       c   o   m   p   u   t   a   t   i   o   n   ,       1   5   (   4   )   :   9   1   5   \\xe2\\x80\\x93   9   3   6   ,      2   0   0   3   .      [   3   0   ]       y   .       z   h   o   u   ,       h   .       x   u   e   ,       a   n   d       x   .       g   e   n   g   .       e   m   o   t   i   o   n       d   i   s   t   r   i   b   u   t   i   o   n       r   e   c   o   g   n   i   t   i   o   n       f   r   o   m       f   a   c   i   a   l       e   x   p   r   e   s   s   i   o   n   s   .       i   n       p   r   o   c   .       m   m   ,      p   a   g   e   s       1   2   4   7   \\xe2\\x80\\x93   1   2   5   0   ,       2   0   1   5   .         1   0         \\x0c   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 04:17:37.104521: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.14\n",
      "XGBoost Accuracy on Test set -> 0.32\n",
      "RandomForest Accuracy on Test set -> 0.42\n",
      "DecisionTree Accuracy on Test set -> 0.28\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'L b e l D r b u n L e r n n g F r e W e S h e n 1 2 K Z h 1 Y l u G u 1 A l n Y u l l e 2 K e L b r r f S p e c l F b e r O p c n O p c l A c c e N e w r k S h n g h I n u e f r A v n c e C u n c n n D S c e n c e S c h l f C u n c n n I n f r n E n g n e e r n g S h n g h U n v e r 2 D e p r e n f C p u e r S c e n c e J h n H p k n U n v e r r X v 1 7 0 2 0 6 0 8 6 v 4 c L G 1 6 O c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c A b r c L b e l r b u n l e r n n g L D L g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l C u r r e n L D L e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r T h p p e r p r e e n l b e l r b u n l e r n n g f r e L D L F n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 D e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 T h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g W e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g T h e e f f e c v e n e f h e p r p e L D L F v e r f e n e v e r l L D L k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r L D L e h 1 I n r u c n L b e l r b u n l e r n n g L D L 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u U n l k e n g l e l b e l l e r n n g S L L n u l l b e l l e r n n g M L L 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l L D L l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l S u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u A n e x p l e f c l g e e n 8 E v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e T h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r H e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e F g 1 n e f u n g n g l e g e l b e l A n h e r e x p l e v e r n g p r e c n 7 M n f u v e r e v e w w e b e u c h N e f l x I M D b n D u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r F g 1 b I f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h M n L D L e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 B u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n S e h e r L D L e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 C n f e r e n c e n N e u r l I n f r n P r c e n g S e N I P S 2 0 1 7 L n g B e c h C A U S A x0c F g u r e 1 T h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g E e f c l g e u n l r b u n b R n g r b u n f c r w p n n n v e u l l r b u n I n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e L D L F n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 E x e n n g f f e r e n b l e e c n r e e e l w h h e L D L k h w v n g e O n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n T h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r W e e f n e r b u n b e l f u n c n f r r e e b h e K u l l b c k L e b l e r v e r g e n c e K L b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e B f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n F l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n T l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n I n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l O u r L D L F c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l F g 2 l l u r e k e c h c h r f u r L D L F w h e r e f r e c n f w r e e h w n W e v e r f h e e f f e c v e n e f u r e l n e v e r l L D L k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r L D L e h T h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n F g 1 n x u r e r b u n h e r n g r b u n n v e n F g 1 b T h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n F g u r e 2 I l l u r n f l b e l r b u n l e r n n g f r e T h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b xce\\\\x98 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k T h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l T w n e x f u n c n xcf\\\\x95 1 n xcf\\\\x95 2 r e g n e h e e w r e e r e p e c v e l T h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f N e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e E c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e T h e u p u f h e f r e x u r e f h e r e e p r e c n f xc2\\\\xb7 xce\\\\x98 n q r e l e r n e j n l n n e n e n n n e r 2 x0c 2 R e l e W r k S n c e u r L D L l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e T h e n w e c u c u r r e n L D L e h D e c n r e e R n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k I n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r T h e n e w l p r p e e e p n e u r l e c n f r e N D F 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e T h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n O u r e h e x e n N D F r e L D L p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e A l h u g h e p z e f r e e u p e f u n c n w g v e n n N D F u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l C n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e W e b e r v e h w e v e r h h e u p e f u n c n n N D F c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r L D L l I n n h e r e g e u e n L D L F n N D F l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n N D F 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e N D F n 3 l l r e e n L D L F c n b e l e r n e j n l w h l e r e e n N D F w e r e l e r n e l e r n v e l T h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n S e c 4 4 L D L F c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n N D F h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e T u u p w r N D F 2 0 h e c n r b u n f L D L F r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e L b e l r b u n l e r n n g A n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e L D L k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 G e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l T h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g G u n r T r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e I I S L L D w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l Y n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e S C E L D L n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l G e n g 6 e v e l p e n c c e l e r e v e r n f I I S L L D c l l e B F G S L D L b u n g q u N e w n p z n A l l h e b v e L D L e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r A n h e r w r e h e L D L k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n G e n g n H u 7 p r p e L D S V R L D L e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e X n g e l 2 7 h e n e x e n e b n g r e h e L D L k b v e w e g h e r e g r e r T h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h A O S O L D L L g B A h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e A O S O L D L L g B u n b l e b e c b n e w h r e p r e e n n l e r n n g E x e n n g c u r r e n e e p l e r n n g l g r h 3 x0c r e h e L D L k n n e r e n g p c B u h e e x n g u c h e h c l l e D L D L 5 l l f c u e n x u e n r p e l b e L D L O u r e h L D L F e x e n f f e r e n b l e e c n r e e r e L D L k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l I n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n L D L F c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 L b e l D r b u n L e r n n g F r e A f r e n e n e b l e f e c n r e e W e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 P r b l e F r u l n L e X R e n e h e n p u p c e n Y 1 2 C e n e h e c p l e e e f l b e l w h e r e C h e n u b e r f p b l e l b e l v l u e W e c n e r l b e l r b u n l e r n n g L D L p r b l e w h e r e f r e c h n p u p l e x xe2\\\\x88\\\\x88 X h e r e l b e l r b u n x 1 x 2 x C xe2\\\\x88\\\\x88 R C H e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e P C c n r n h x c xe2\\\\x88\\\\x88 0 1 n c 1 x c 1 T h e g l f h e L D L p r b l e l e r n p p n g f u n c n g x xe2\\\\x86\\\\x92 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n H e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l T A e c n r e e c n f e f p l n e N n e f l e f n e L E c h p l n e n xe2\\\\x88\\\\x88 N e f n e p l f u n c n n xc2\\\\xb7 xce\\\\x98 X xe2\\\\x86\\\\x92 0 1 p r e e r z e b xce\\\\x98 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e E c h l e f n e xe2\\\\x88\\\\x88 L h l r b u n q q 1 q 2 q C P C v e r Y e q c xe2\\\\x88\\\\x88 0 1 n c 1 q c 1 T b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x xce\\\\x98 xcf\\\\x83 f xcf\\\\x95 n x xce\\\\x98 w h e r e xcf\\\\x83 xc2\\\\xb7 g f u n c n xcf\\\\x95 xc2\\\\xb7 n n e x f u n c n b r n g h e xcf\\\\x95 n h u p u f f u n c n f x xce\\\\x98 n c r r e p n e n c e w h p l n e n n f x xe2\\\\x86\\\\x92 R M r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r xce\\\\x98 n c n k e n f r F r p l e f r c n b e l n e r r n f r n f x w h e r e xce\\\\x98 h e r n f r n r x F r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n xce\\\\x98 h e n e w r k p r e e r T h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b xcf\\\\x95 xc2\\\\xb7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r xe2\\\\x80\\\\x9c f xe2\\\\x80\\\\x9d r e u e f r c n r u c n g r e e e e r n e r n l A n e x p l e e n r e xcf\\\\x95 xc2\\\\xb7 h w n n F g 2 T h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b Y l r p x xce\\\\x98 n x xce\\\\x98 1 xe2\\\\x88\\\\x88 L n 1 xe2\\\\x88\\\\x92 n x xce\\\\x98 1 xe2\\\\x88\\\\x88 L n 1 n xe2\\\\x88\\\\x88 N w h e r e 1 xc2\\\\xb7 n n c r f u n c n n L l n n L r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n T n l n T n r r e p e c v e l T h e u p u f h e r e e T w r x e h e p p n g f u n c n g e f n e b X g x xce\\\\x98 T p x xce\\\\x98 q 2 xe2\\\\x88\\\\x88 L 3 2 T r e e O p z n G v e n r n n g e S x N 1 u r g l l e r n e c n r e e T e c r b e n S e c 3 1 w h c h c n u p u r b u n g x xce\\\\x98 T l r f r e c h p l e x T h e n r g h f r w r w n z e h e K u l l b c k L e b l e r K L v e r g e n c e b e w e e n e c h g x xce\\\\x98 T n r e q u v l e n l n z e h e f l l w n g c r e n r p l R q xce\\\\x98 S xe2\\\\x88\\\\x92 N C N C x10 X x11 1 X X c 1 X X c x l g g c x xce\\\\x98 T xe2\\\\x88\\\\x92 x l g p x xce\\\\x98 q c 3 N 1 c 1 N 1 c 1 xe2\\\\x88\\\\x88 L 4 x0c w h e r e q e n e h e r b u n h e l b l l h e l e f n e L n g c x xce\\\\x98 T h e c h u p u u n f g x xce\\\\x98 T L e r n n g h e r e e T r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r xce\\\\x98 n 2 h e r b u n q h e l b h e l e f n e T h e b e p r e e r xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 r e e e r n e b xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 r g n R q xce\\\\x98 S 4 xce\\\\x98 q T l v e E q n 4 w e c n e r n l e r n n g p z n r e g F r w e f x q n p z e xce\\\\x98 T h e n w e f x xce\\\\x98 n p z e q T h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 L e r n n g S p l N e I n h e c n w e e c r b e h w l e r n h e p r e e r xce\\\\x98 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e W e c p u e h e g r e n f h e l R q xce\\\\x98 S w r xce\\\\x98 b h e c h n r u l e N xe2\\\\x88\\\\x82 R q xce\\\\x98 S X X xe2\\\\x88\\\\x82 R q xce\\\\x98 S xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 5 xe2\\\\x88\\\\x82 xce\\\\x98 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 n xe2\\\\x88\\\\x88 N w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f xcf\\\\x95 n T h e f r e r g v e n b C x01 g c x xce\\\\x98 T n l x11 g c x xce\\\\x98 T n r 1 X c x10 xe2\\\\x88\\\\x82 R q xce\\\\x98 S x n x xce\\\\x98 xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 n x xce\\\\x98 6 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 N c 1 g c x xce\\\\x98 T g c x xce\\\\x98 T P P w h e r e g c x xce\\\\x98 T n l xe2\\\\x88\\\\x88 L l n p x xce\\\\x98 q c n g c x xce\\\\x98 T n r xe2\\\\x88\\\\x88 L r n p x xce\\\\x98 q c N e h l e T n b e h e r e e r e h e n e n h e n w e h v e g c x xce\\\\x98 T n g c x xce\\\\x98 T n l g c x xce\\\\x98 T n r T h e n h e g r e n c p u n n E q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r T h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 L e r n n g L e f N e N w f x n g h e p r e e r xce\\\\x98 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n R q xce\\\\x98 S xe2\\\\x88\\\\x80 q C X q c 1 7 c 1 H e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q I n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r A u p p e r b u n f r h e l f u n c n R q xce\\\\x98 S c n b e b n e b J e n e n xe2\\\\x80\\\\x99 n e q u l R q xce\\\\x98 S xe2\\\\x88\\\\x92 N C x10 X x11 1 X X c x l g p x xce\\\\x98 q c N 1 c 1 xe2\\\\x88\\\\x88 L xe2\\\\x89\\\\xa4 xe2\\\\x88\\\\x92 w h e r e xce\\\\xbe q c x 1 N N X C X 1 c 1 p x xce\\\\x98 q c g c x xce\\\\x98 T xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x88\\\\x92 x c X xce\\\\xbe q xcc\\\\x84 c x l g xe2\\\\x88\\\\x88 L x10 p x xce\\\\x98 q x11 c xce\\\\xbe q xcc\\\\x84 c x 8 W e e f n e N C x10 p x xce\\\\x98 q x11 1 X X c X c x xce\\\\xbe q xcc\\\\x84 c x l g N 1 c 1 xce\\\\xbe q xcc\\\\x84 c x 9 xe2\\\\x88\\\\x88 L T h e n xcf\\\\x86 q q xcc\\\\x84 n u p p e r b u n f r R q xce\\\\x98 S w h c h h h e p r p e r h f r n q n q xcc\\\\x84 xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x89\\\\xa5 R q xce\\\\x98 S n xcf\\\\x86 q q R q xce\\\\x98 S A u e h w e r e p n q c r r e p n n g h e h e r n h e n xcf\\\\x86 q q n u p p e r b u n f r R q xce\\\\x98 S I n h e n e x e r n q 1 c h e n u c h h xcf\\\\x86 q 1 q xe2\\\\x89\\\\xa4 R q xce\\\\x98 S w h c h p l e R q 1 xce\\\\x98 S xe2\\\\x89\\\\xa4 R q xce\\\\x98 S 5 x0c C n e q u e n l w e c n n z e xcf\\\\x86 q q xcc\\\\x84 n e f R q xce\\\\x98 S f e r e n u r n g h R q xce\\\\x98 S xcf\\\\x86 q q xcc\\\\x84 e q xcc\\\\x84 q S w e h v e q 1 r g n xcf\\\\x86 q q xe2\\\\x88\\\\x80 q C X q c 1 1 0 c 1 w h c h l e n z n g h e L g r n g n e f n e b xcf\\\\x95 q q xcf\\\\x86 q q X xce\\\\xbb xe2\\\\x88\\\\x88 L w h e r e xce\\\\xbb h e L g r n g e u l p l e r B e n g xce\\\\xbb 1 N e h q c 1 xe2\\\\x88\\\\x88 0 1 n r b u n h e l b h e l e f n e T h e r n g 0 r b u n q c C 1 3 3 q c xe2\\\\x88\\\\x92 1 1 1 c 1 xe2\\\\x88\\\\x82 xcf\\\\x95 q q xe2\\\\x88\\\\x82 q c N C 1 X X c 1 xce\\\\xbe q c x n q c N 1 c 1 x f e h q c C X 0 w e h v e P N c x xce\\\\xbe q c x P C 1 P N c xce\\\\xbe q x x c 1 1 c 1 2 1 1 E q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r P C L e r n n g F r e A f r e n e n e b l e f e c n r e e F T 1 T K I n h e r n n g g e l l r e e n h e f r e F u e h e e p r e e r xce\\\\x98 f r f e u r e l e r n n g f u n c n f xc2\\\\xb7 xce\\\\x98 b u c r r e p n f f e r e n u p u u n f f g n e b xcf\\\\x95 e e F g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q T h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e P K 1 R F K k 1 R T k w h e r e R T k h e l f u n c n f r r e e T k e f n e b E q n 3 T l e r n xce\\\\x98 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e F b e n h e e r v n n S e c 3 2 n r e f e r r n g F g 2 w e h v e N K xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x xce\\\\x98 1 X X X xe2\\\\x88\\\\x82 R F xe2\\\\x88\\\\x82 R T k xe2\\\\x88\\\\x82 xce\\\\x98 K 1 xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 3 k 1 n xe2\\\\x88\\\\x88 N k w h e r e N k n xcf\\\\x95 k xc2\\\\xb7 r e h e p l n e e n h e n e x f u n c n f T k r e p e c v e l N e h h e n e x f u n c n xcf\\\\x95 k xc2\\\\xb7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f T h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g A f r q n c e e c h r e e n h e f r e F h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b E q n 1 2 g v e n b xce\\\\x98 F r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e S b u n e f n b c h e B T h e r n n g p r c e u r e f L D L F h w n n A l g r h 1 A l g r h 1 T h e r n n g p r c e u r e f L D L F R e q u r e S r n n g e n B h e n u b e r f n b c h e u p e q I n l z e xce\\\\x98 r n l n q u n f r l e B xe2\\\\x88\\\\x85 w h l e N c n v e r g e w h l e B n B R n l e l e c n b c h B f r S U p e S xce\\\\x98 b c p u n g g r e n E q n 1 3 n B B B B e n w h l e U p e q b e r n g E q n 1 2 n B B xe2\\\\x88\\\\x85 e n w h l e I n h e e n g g e h e u p u f h e f r e F g v e n b v e r g n g h e p r e c n f r l l h e P K 1 n v u l r e e g x xce\\\\x98 F K k 1 g x xce\\\\x98 T k 6 x0c 4 E x p e r e n l R e u l O u r r e l z n f L D L F b e n xe2\\\\x80\\\\x9c C f f e xe2\\\\x80\\\\x9d 1 8 I u l r n p l e e n e n r n e u r l n e w r k l e r W e c n e h e r u e h l l w n l n e e l L D L F r n e g r e w h n e e p n e w r k L D L F W e e v l u e L D L F n f f e r e n L D L k n c p r e w h h e r n l n e L D L e h A L D L F c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f L D L F n c p u e r v n p p l c n e f c l g e e n T h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 C p r n f L D L F S n l n e L D L M e h W e c p r e u r h l l w e l L D L F w h h e r e f h e r n l n e L D L e h F r L D L F h e f e u r e l e r n n g f u n c n f x xce\\\\x98 l n e r r n f r n f x e h e h u p u u n f x xce\\\\xb8 xce\\\\xb8 x w h e r e xce\\\\xb8 h e h c l u n f h e r n f r n r x xce\\\\x98 W e u e 3 p p u l r L D L e n 6 M v e H u n G e n e n N u r l S c e n e 1 T h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l T h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n F g 1 b F l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f L D L e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e K L E u c l e n S xcf\\\\x86 r e n e n S q u r e xcf\\\\x87 2 n w l r e u r e F e l I n e r e c n W e e v l u e u r h l l w e l L D L F n h e e 3 e n c p r e w h h e r e f h e r n l n e L D L e h T h e r e u l f L D L F n h e c p e r r e u r z e n T b l e 1 F r M v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e F r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e I n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b xe2\\\\x80\\\\x9c e n xc2\\\\xb1 n r e v n xe2\\\\x80\\\\x9d n e r l e h w r n n g n e n g g e v e A c n b e e e n f r T b l e 1 L D L F p e r f r b e n l l f h e x e u r e T b l e 1 C p r n r e u l n h r e e L D L e 6 xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x91 xe2\\\\x80\\\\x9d n xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x93 xe2\\\\x80\\\\x9d n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l D e M e h K L xe2\\\\x86\\\\x93 E u c l e n xe2\\\\x86\\\\x93 S xcf\\\\x86 r e n e n xe2\\\\x86\\\\x93 S q u r e xcf\\\\x87 2 xe2\\\\x86\\\\x93 F e l xe2\\\\x86\\\\x91 I n e r e c n xe2\\\\x86\\\\x91 M v e L D L F u r A O S O L D L g B 2 7 L D L g B 2 7 L D S V R 7 B F G S L D L 6 I I S L D L 1 1 0 0 7 3 xc2\\\\xb1 0 0 0 5 0 0 8 6 xc2\\\\xb1 0 0 0 4 0 0 9 0 xc2\\\\xb1 0 0 0 4 0 0 9 2 xc2\\\\xb1 0 0 0 5 0 0 9 9 xc2\\\\xb1 0 0 0 4 0 1 2 9 xc2\\\\xb1 0 0 0 7 0 1 3 3 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 9 xc2\\\\xb1 0 0 0 3 0 1 5 8 xc2\\\\xb1 0 0 0 4 0 1 6 7 xc2\\\\xb1 0 0 0 4 0 1 8 7 xc2\\\\xb1 0 0 0 4 0 1 3 0 xc2\\\\xb1 0 0 0 3 0 1 5 2 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 6 xc2\\\\xb1 0 0 0 4 0 1 6 4 xc2\\\\xb1 0 0 0 3 0 1 8 3 xc2\\\\xb1 0 0 0 4 0 0 7 0 xc2\\\\xb1 0 0 0 4 0 0 8 4 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 4 0 0 9 6 xc2\\\\xb1 0 0 0 4 0 1 2 0 xc2\\\\xb1 0 0 0 5 0 9 8 1 xc2\\\\xb1 0 0 0 1 0 9 7 8 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 4 xc2\\\\xb1 0 0 0 1 0 9 6 7 xc2\\\\xb1 0 0 0 1 0 8 7 0 xc2\\\\xb1 0 0 0 3 0 8 4 8 xc2\\\\xb1 0 0 0 3 0 8 4 5 xc2\\\\xb1 0 0 0 3 0 8 4 4 xc2\\\\xb1 0 0 0 4 0 8 3 6 xc2\\\\xb1 0 0 0 3 0 8 1 7 xc2\\\\xb1 0 0 0 4 L D L F u r L D S V R 7 B F G S L D L 6 I I S L D L 1 1 0 2 2 8 xc2\\\\xb1 0 0 0 6 0 2 4 5 xc2\\\\xb1 0 0 1 9 0 2 3 1 xc2\\\\xb1 0 0 2 1 0 2 3 9 xc2\\\\xb1 0 0 1 8 0 0 8 5 xc2\\\\xb1 0 0 0 2 0 0 9 9 xc2\\\\xb1 0 0 0 5 0 0 7 6 xc2\\\\xb1 0 0 0 6 0 0 8 9 xc2\\\\xb1 0 0 0 6 0 2 1 2 xc2\\\\xb1 0 0 0 2 0 2 2 9 xc2\\\\xb1 0 0 1 5 0 2 3 1 xc2\\\\xb1 0 0 1 2 0 2 5 3 xc2\\\\xb1 0 0 0 9 0 1 7 9 xc2\\\\xb1 0 0 0 4 0 1 8 9 xc2\\\\xb1 0 0 2 1 0 2 1 1 xc2\\\\xb1 0 0 1 8 0 2 0 5 xc2\\\\xb1 0 0 1 2 0 9 4 8 xc2\\\\xb1 0 0 0 1 0 9 4 0 xc2\\\\xb1 0 0 0 6 0 9 3 8 xc2\\\\xb1 0 0 0 8 0 9 4 4 xc2\\\\xb1 0 0 0 3 0 7 8 8 xc2\\\\xb1 0 0 0 2 0 7 7 1 xc2\\\\xb1 0 0 1 5 0 7 6 9 xc2\\\\xb1 0 0 1 2 0 7 4 7 xc2\\\\xb1 0 0 0 9 L D L F u r L D S V R 7 B F G S L D L 6 I I S L D L 1 1 0 5 3 4 xc2\\\\xb1 0 0 1 3 0 8 5 2 xc2\\\\xb1 0 0 2 3 0 8 5 6 xc2\\\\xb1 0 0 6 1 0 8 7 9 xc2\\\\xb1 0 0 2 3 0 3 1 7 xc2\\\\xb1 0 0 1 4 0 5 1 1 xc2\\\\xb1 0 0 2 1 0 4 7 5 xc2\\\\xb1 0 0 2 9 0 4 5 8 xc2\\\\xb1 0 0 1 4 0 3 3 6 xc2\\\\xb1 0 0 1 0 0 4 9 2 xc2\\\\xb1 0 0 1 6 0 5 0 8 xc2\\\\xb1 0 0 2 6 0 5 3 9 xc2\\\\xb1 0 0 1 1 0 4 4 8 xc2\\\\xb1 0 0 1 7 0 5 9 5 xc2\\\\xb1 0 0 2 6 0 7 1 6 xc2\\\\xb1 0 0 4 1 0 7 9 2 xc2\\\\xb1 0 0 1 9 0 8 2 4 xc2\\\\xb1 0 0 0 8 0 8 1 3 xc2\\\\xb1 0 0 0 8 0 7 2 2 xc2\\\\xb1 0 0 2 1 0 6 8 6 xc2\\\\xb1 0 0 0 9 0 6 6 4 xc2\\\\xb1 0 0 1 0 0 5 0 9 xc2\\\\xb1 0 0 1 6 0 4 9 2 xc2\\\\xb1 0 0 2 6 0 4 6 1 xc2\\\\xb1 0 0 1 1 H u n G e n e N u r l S c e n e 4 2 E v l u n f L D L F n F c l A g e E n I n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e L D L p r b l e W e c n u c f c l g e e n e x p e r e n n M r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e E c h f c l g e n n e w h c h r n l g c l g e T g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e G u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e F g 1 T h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 W e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g L D L n e x h 7 x0c l b e l r b u n T h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r M A E b e w e e n p r e c e g e n c h r n l g c l g e A h e c u r r e n e f h e r r e u l n M r p h b n b f n e u n n g D L D L 5 n V G G F c e 2 3 w e l b u l L D L F n V G G F c e b r e p l c n g h e f x l e r n V G G N e b L D L F F l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n T b l e 2 w h c h h w L D L F c h e v e h e e f h e r p e r f r n c e n M r p h N e h h e g n f c n p e r f r n c e g n b e w e e n e e p L D L e l D L D L n L D L F n n n e e p L D L e l I I S L D L C P N N B F G S L D L n h e u p e r r f L D L F c p r e w h D L D L v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r L D L r e p e c v e l T b l e 2 M A E f g e e n c p r n n M r p h 2 4 M e h I I S L D L 1 1 C P N N 1 1 B F G S L D L 6 D L D L V G G F c e 5 L D L F V G G F c e u r M A E 5 6 7 xc2\\\\xb1 0 1 5 4 8 7 xc2\\\\xb1 0 3 1 3 9 4 xc2\\\\xb1 0 0 5 2 4 2 xc2\\\\xb1 0 0 1 2 2 4 xc2\\\\xb1 0 0 2 A h e r b u n f g e n e r n e h n c v e r u n b l n c e n M r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f M r p h c l l e M r p h S u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n T h e b e p e r f r n c e r e p r e n M r p h S u b g v e n b D 2 L D L 1 5 e p e n e n L D L e h A D 2 L D L u e h e u p u f h e xe2\\\\x80\\\\x9c f c 7 xe2\\\\x80\\\\x9d l e r n A l e x N e 2 1 h e f c e g e f e u r e h e r e w e n e g r e L D L F w h A l e x N e F l l w n g h e e x p e r e n e n g u e n D 2 L D L w e e v l u e u r L D L F n h e c p e r n c l u n g b h S L L n L D L b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 A l l f h e c p e r r e r n e n h e e e e p f e u r e u e b D 2 L D L A c n b e e e n f r T b l e 3 u r L D L F g n f c n l u p e r f r h e r f r l l r n n g e r N e h h e g e n e r e g e r F g u r e 3 M A E f g e e n c p r n n b u n r e u n l r b u n M r p h S u b n h e l b e l r b u n u e n T r n n g e r M e h S e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 T h e p r p e e h L D L F c h e v e A A S 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f L A R R 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 I I S A L D L 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l D 2 L D L 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l L D L F u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 T e C p l e x L e h n B b e h e r e e e p h n h e b c h z e r e p e c v e l E c h r e e h 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 p l n e n 2 h xe2\\\\x88\\\\x92 1 l e f n e L e D 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 F r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e O D D 1 xc3\\\\x97 C O D xc3\\\\x97 C n O D 1 xc3\\\\x97 C D xc3\\\\x97 C O D xc3\\\\x97 C r e p e c v e l S f r K r e e n n B b c h e h e c p l e x f f r w r n b c k w r p O D xc3\\\\x97 C xc3\\\\x97 K xc3\\\\x97 n B xc3\\\\x97 B T h e c p l e x f n e r n u p e l e f n e r e O n B xc3\\\\x97 B xc3\\\\x97 K xc3\\\\x97 C xc3\\\\x97 D 1 O D xc3\\\\x97 C xc3\\\\x97 K xc3\\\\x97 n B xc3\\\\x97 B T h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n B b c h e n h e e n g p r c e u r e n e p l e r e O D xc3\\\\x97 C xc3\\\\x97 K xc3\\\\x97 n B xc3\\\\x97 B n O D xc3\\\\x97 C xc3\\\\x97 K r e p e c v e l L D L F r e e f f c e n O n M r p h S u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 P r e e r D c u n N w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e W e r e p r h e r e u l f r n g p r e c n n M v e e u r e b K L n g e e n n M r p h S u b w h 6 0 r n n g e r e u r e b M A E f r f f e r e n p r e e r e n g n h e c n T r e e n u b e r A f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e N e h w e c u e n S e c 2 h e e n e b l e r e g l e r n f r e p r p e n N D F 2 0 f f e r e n f r u r T h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e T w r h e n w e r e p l c e u r e n e b l e r e g n L D L F b h e n e u e n N D F n n e h e h N D F L D L T h e c r r e p n n g h l l w e l n e b N D F L D L W e f x h e r p r e e r e r e e e p h n 8 x0c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g A h w n n F g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n N D F e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e O b e r v e f r F g 4 h e p e r f r n c e f L D L F c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r T h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n O n M v e h e n u b e r f r e e K 1 0 0 K L 0 0 7 0 v K 2 0 K L 0 0 7 1 N e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g S h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e T r e e e p h T r e e e p h n h e r p r n p r e e r f r e c n r e e I n L D L F h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n xcf\\\\x84 xcf\\\\x84 xe2\\\\x89\\\\xa5 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 T c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f L D L F w e e xcf\\\\x84 2 h xe2\\\\x88\\\\x92 1 n f x r e e n u b e r K 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n F g 4 b W e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h T h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l S u n g u c h l r g e r e p h l e b p e r f r n c e O n M v e r e e e p h h 1 8 K L 0 1 1 6 2 v h 9 K L 0 0 8 3 1 F g u r e 4 T h e p e r f r n c e c h n g e f g e e n n M r p h S u b n r n g p r e c n n M v e b v r n g r e e n u b e r n b r e e e p h O u r p p r c h L D L F L D L F c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n N D F N D F L D L N D F L D L e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 C n c l u n W e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e W e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r E x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l L D L k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n A c k n w l e g e e n T h w r k w u p p r e n p r b h e N n l N u r l S c e n c e F u n n f C h n N 6 1 6 7 2 3 3 6 n p r b xe2\\\\x80\\\\x9c C h e n G u n g xe2\\\\x80\\\\x9d p r j e c u p p r e b S h n g h M u n c p l E u c n C n n S h n g h E u c n D e v e l p e n F u n n N 1 5 C G 4 3 n n p r b O N R N 0 0 0 1 4 1 5 1 2 3 5 6 R e f e r e n c e 1 Y A n D G e n S h p e q u n z n n r e c g n n w h r n z e r e e N e u r l C p u n 9 7 1 5 4 5 xe2\\\\x80\\\\x93 1 5 8 8 1 9 9 7 2 A L B e r g e r S D P e r n V J D P e r A x u e n r p p p r c h n u r l l n g u g e p r c e n g C p u n l L n g u c 2 2 1 3 9 xe2\\\\x80\\\\x93 7 1 1 9 9 6 3 L B r e n R n f r e M c h n e L e r n n g 4 5 1 5 xe2\\\\x80\\\\x93 3 2 2 0 0 1 4 A C r n n J S h n D e c n F r e f r C p u e r V n n M e c l I g e A n l S p r n g e r 2 0 1 3 5 B B G C X n g C W X e J W u n X G e n g D e e p l b e l r b u n l e r n n g w h l b e l b g u r X v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 X G e n g L b e l r b u n l e r n n g I E E E T r n K n w l D E n g 2 8 7 1 7 3 4 xe2\\\\x80\\\\x93 1 7 4 8 2 0 1 6 9 x0c 7 X G e n g n P H u P r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g I n P r I J C A I p g e 3 5 1 1 xe2\\\\x80\\\\x93 3 5 1 7 2 0 1 5 8 X G e n g K S h M l e n Z Z h u F c l g e e n b l e r n n g f r l b e l r b u n I n P r c A A A I 2 0 1 0 9 X G e n g Q W n g n Y X F c l g e e n b p v e l b e l r b u n l e r n n g I n P r c I C P R p g e 4 4 6 5 xe2\\\\x80\\\\x93 4 4 7 0 2 0 1 4 1 0 X G e n g n Y X H e p e e n b e n u l v r e l b e l r b u n I n P r c C V P R p g e 1 8 3 7 xe2\\\\x80\\\\x93 1 8 4 2 2 0 1 4 1 1 X G e n g C Y n n Z Z h u F c l g e e n b l e r n n g f r l b e l r b u n I E E E T r n P e r n A n l M c h I n e l l 3 5 1 0 2 4 0 1 xe2\\\\x80\\\\x93 2 4 1 2 2 0 1 3 1 2 G G u Y F u C R D e r n T S H u n g I g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n I E E E T r n I g e P r c e n g 1 7 7 1 1 7 8 xe2\\\\x80\\\\x93 1 1 8 8 2 0 0 8 1 3 G G u n G M u H u n g e e n W h h e n f l u e n c e c r r c e n g e n e r I n C V P R W r k h p p g e 7 1 xe2\\\\x80\\\\x93 7 8 2 0 1 0 1 4 G G u n C Z h n g A u n c r p p u l n g e e n I n P r c C V P R p g e 4 2 5 7 xe2\\\\x80\\\\x93 4 2 6 3 2 0 1 4 1 5 Z H e X L Z Z h n g F W u X G e n g Y Z h n g M H Y n g n Y Z h u n g D e p e n e n l b e l r b u n l e r n n g f r g e e n I E E E T r n n I g e P r c e n g 2 0 1 7 1 6 T K H R n e c n f r e I n P r c I C D A R p g e 2 7 8 xe2\\\\x80\\\\x93 2 8 2 1 9 9 5 1 7 T K H T h e r n u b p c e e h f r c n r u c n g e c n f r e I E E E T r n P e r n A n l M c h I n e l l 2 0 8 8 3 2 xe2\\\\x80\\\\x93 8 4 4 1 9 9 8 1 8 Y J E S h e l h e r J D n h u e S K r e v J L n g R G r h c k S G u r r n T D r r e l l C f f e C n v l u n l r c h e c u r e f r f f e u r e e b e n g r X v p r e p r n r X v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 M I J r n Z G h h r n T S J k k l n L K S u l A n n r u c n v r n l e h f r g r p h c l e l M c h n e L e r n n g 3 7 2 1 8 3 xe2\\\\x80\\\\x93 2 3 3 1 9 9 9 2 0 P K n c h e e r M F e r u A C r n n S R B u l xc3\\\\xb2 D e e p n e u r l e c n f r e I n P r c I C C V p g e 1 4 6 7 xe2\\\\x80\\\\x93 1 4 7 5 2 0 1 5 2 1 A K r z h e v k I S u k e v e r n G E H n n I g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k I n P r c N I P S p g e 1 1 0 6 xe2\\\\x80\\\\x93 1 1 1 4 2 0 1 2 2 2 A L n C D r g n v n C C h r u l u C p r n g f f e r e n c l f e r f r u c g e e n I E E E T r n n C b e r n e c 3 4 1 6 2 1 xe2\\\\x80\\\\x93 6 2 8 2 0 0 4 2 3 O M P r k h A V e l n A Z e r n D e e p f c e r e c g n n I n P r c B M V C p g e 4 1 1 xe2\\\\x80\\\\x93 4 1 1 2 2 0 1 5 2 4 K R c n e k n T T e f e M O R P H A l n g u n l g e b e f n r l u l g e p r g r e n I n P r c F G p g e 3 4 1 xe2\\\\x80\\\\x93 3 4 5 2 0 0 6 2 5 J S h n A W F z g b b n M C k T S h r p M F n c c h R M r e A K p n n A B l k e R e l e h u n p e r e c g n n n p r f r n g l e e p h g e I n P r c C V P R p g e 1 2 9 7 xe2\\\\x80\\\\x93 1 3 0 4 2 0 1 1 2 6 G T u k n I K k M u l l b e l c l f c n A n v e r v e w I n e r n n l J u r n l f D W r e h u n g n M n n g 3 3 1 xe2\\\\x80\\\\x93 1 3 2 0 0 7 2 7 C X n g X G e n g n H X u e L g c b n g r e g r e n f r l b e l r b u n l e r n n g I n P r c C V P R p g e 4 4 8 9 xe2\\\\x80\\\\x93 4 4 9 7 2 0 1 6 2 8 X Y n g X G e n g n D Z h u S p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n I n P r c I J C A I p g e 2 2 5 9 xe2\\\\x80\\\\x93 2 2 6 5 2 0 1 6 2 9 A L Y u l l e n A R n g r j n T h e c n c v e c n v e x p r c e u r e N e u r l C p u n 1 5 4 9 1 5 xe2\\\\x80\\\\x93 9 3 6 2 0 0 3 3 0 Y Z h u H X u e n X G e n g E n r b u n r e c g n n f r f c l e x p r e n I n P r c M M p g e 1 2 4 7 xe2\\\\x80\\\\x93 1 2 5 0 2 0 1 5 1 0 x0c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.36\n",
      "RandomForest Accuracy on Test set -> 0.3\n",
      "DecisionTree Accuracy on Test set -> 0.16\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s w e i s h e n 1 2 k a i z h a o 1 y i l u g u o 1 a l a n y u i l l e 2 k e y l a b o r a t o r y o f s p e c i a l t y f i b e r o p t i c s a n d o p t i c a l a c c e s s n e t w o r k s s h a n g h a i i n s t i t u t e f o r a d v a n c e d c o m m u n i c a t i o n a n d d a t a s c i e n c e s c h o o l o f c o m m u n i c a t i o n a n d i n f o r m a t i o n e n g i n e e r i n g s h a n g h a i u n i v e r s i t y 2 d e p a r t m e n t o f c o m p u t e r s c i e n c e j o h n s h o p k i n s u n i v e r s i t y a r x i v 1 7 0 2 0 6 0 8 6 v 4 c s l g 1 6 o c t 2 0 1 7 1 s h e n w e i 1 2 3 1 z h a o k 1 2 0 6 g y l l u a n 0 a l a n l y u i l l e g m a i l c o m a b s t r a c t l a b e l d i s t r i b u t i o n l e a r n i n g l d l i s a g e n e r a l l e a r n i n g f r a m e w o r k w h i c h a s s i g n s t o a n i n s t a n c e a d i s t r i b u t i o n o v e r a s e t o f l a b e l s r a t h e r t h a n a s i n g l e l a b e l o r m u l t i p l e l a b e l s c u r r e n t l d l m e t h o d s h a v e e i t h e r r e s t r i c t e d a s s u m p t i o n s o n t h e e x p r e s s i o n f o r m o f t h e l a b e l d i s t r i b u t i o n o r l i m i t a t i o n s i n r e p r e s e n t a t i o n l e a r n i n g e g t o l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r t h i s p a p e r p r e s e n t s l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s l d l f s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m b a s e d o n d i f f e r e n t i a b l e d e c i s i o n t r e e s w h i c h h a v e s e v e r a l a d v a n t a g e s 1 d e c i s i o n t r e e s h a v e t h e p o t e n t i a l t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s b y a m i x t u r e o f l e a f n o d e p r e d i c t i o n s 2 t h e l e a r n i n g o f d i f f e r e n t i a b l e d e c i s i o n t r e e s c a n b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g w e d e f i n e a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r a f o r e s t e n a b l i n g a l l t h e t r e e s t o b e l e a r n e d j o i n t l y a n d s h o w t h a t a n u p d a t e f u n c t i o n f o r l e a f n o d e p r e d i c t i o n s w h i c h g u a r a n t e e s a s t r i c t d e c r e a s e o f t h e l o s s f u n c t i o n c a n b e d e r i v e d b y v a r i a t i o n a l b o u n d i n g t h e e f f e c t i v e n e s s o f t h e p r o p o s e d l d l f s i s v e r i f i e d o n s e v e r a l l d l t a s k s a n d a c o m p u t e r v i s i o n a p p l i c a t i o n s h o w i n g s i g n i f i c a n t i m p r o v e m e n t s t o t h e s t a t e o f t h e a r t l d l m e t h o d s 1 i n t r o d u c t i o n l a b e l d i s t r i b u t i o n l e a r n i n g l d l 6 1 1 i s a l e a r n i n g f r a m e w o r k t o d e a l w i t h p r o b l e m s o f l a b e l a m b i g u i t y u n l i k e s i n g l e l a b e l l e a r n i n g s l l a n d m u l t i l a b e l l e a r n i n g m l l 2 6 w h i c h a s s u m e a n i n s t a n c e i s a s s i g n e d t o a s i n g l e l a b e l o r m u l t i p l e l a b e l s l d l a i m s a t l e a r n i n g t h e r e l a t i v e i m p o r t a n c e o f e a c h l a b e l i n v o l v e d i n t h e d e s c r i p t i o n o f a n i n s t a n c e i e a d i s t r i b u t i o n o v e r t h e s e t o f l a b e l s s u c h a l e a r n i n g s t r a t e g y i s s u i t a b l e f o r m a n y r e a l w o r l d p r o b l e m s w h i c h h a v e l a b e l a m b i g u i t y a n e x a m p l e i s f a c i a l a g e e s t i m a t i o n 8 e v e n h u m a n s c a n n o t p r e d i c t t h e p r e c i s e a g e f r o m a s i n g l e f a c i a l i m a g e t h e y m a y s a y t h a t t h e p e r s o n i s p r o b a b l y i n o n e a g e g r o u p a n d l e s s l i k e l y t o b e i n a n o t h e r h e n c e i t i s m o r e n a t u r a l t o a s s i g n a d i s t r i b u t i o n o f a g e l a b e l s t o e a c h f a c i a l i m a g e f i g 1 a i n s t e a d o f u s i n g a s i n g l e a g e l a b e l a n o t h e r e x a m p l e i s m o v i e r a t i n g p r e d i c t i o n 7 m a n y f a m o u s m o v i e r e v i e w w e b s i t e s s u c h a s n e t f l i x i m d b a n d d o u b a n p r o v i d e a c r o w d o p i n i o n f o r e a c h m o v i e s p e c i f i e d b y t h e d i s t r i b u t i o n o f r a t i n g s c o l l e c t e d f r o m t h e i r u s e r s f i g 1 b i f a s y s t e m c o u l d p r e c i s e l y p r e d i c t s u c h a r a t i n g d i s t r i b u t i o n f o r e v e r y m o v i e b e f o r e i t i s r e l e a s e d m o v i e p r o d u c e r s c a n r e d u c e t h e i r i n v e s t m e n t r i s k a n d t h e a u d i e n c e c a n b e t t e r c h o o s e w h i c h m o v i e s t o w a t c h m a n y l d l m e t h o d s a s s u m e t h e l a b e l d i s t r i b u t i o n c a n b e r e p r e s e n t e d b y a m a x i m u m e n t r o p y m o d e l 2 a n d l e a r n i t b y o p t i m i z i n g a n e n e r g y f u n c t i o n b a s e d o n t h e m o d e l 8 1 1 2 8 6 b u t t h e e x p o n e n t i a l p a r t o f t h i s m o d e l r e s t r i c t s t h e g e n e r a l i t y o f t h e d i s t r i b u t i o n f o r m e g i t h a s d i f f i c u l t y i n r e p r e s e n t i n g m i x t u r e d i s t r i b u t i o n s s o m e o t h e r l d l m e t h o d s e x t e n d t h e e x i s t i n g l e a r n i n g a l g o r i t h m s e g b y b o o s t i n g a n d s u p p o r t v e c t o r r e g r e s s i o n t o d e a l w i t h l a b e l d i s t r i b u t i o n s 7 2 7 w h i c h a v o i d m a k i n g t h i s a s s u m p t i o n b u t h a v e l i m i t a t i o n s i n r e p r e s e n t a t i o n l e a r n i n g e g t h e y d o n o t l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r 3 1 s t c o n f e r e n c e o n n e u r a l i n f o r m a t i o n p r o c e s s i n g s y s t e m s n i p s 2 0 1 7 l o n g b e a c h c a u s a x0c f i g u r e 1 t h e r e a l w o r l d d a t a w h i c h a r e s u i t a b l e t o b e m o d e l e d b y l a b e l d i s t r i b u t i o n l e a r n i n g a e s t i m a t e d f a c i a l a g e s a u n i m o d a l d i s t r i b u t i o n b r a t i n g d i s t r i b u t i o n o f c r o w d o p i n i o n o n a m o v i e a m u l t i m o d a l d i s t r i b u t i o n i n t h i s p a p e r w e p r e s e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s l d l f s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s 2 0 e x t e n d i n g d i f f e r e n t i a b l e d e c i s i o n t r e e s t o d e a l w i t h t h e l d l t a s k h a s t w o a d v a n t a g e s o n e i s t h a t d e c i s i o n t r e e s h a v e t h e p o t e n t i a l t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s b y m i x t u r e o f t h e l e a f n o d e p r e d i c t i o n s w h i c h a v o i d m a k i n g s t r o n g a s s u m p t i o n o n t h e f o r m o f t h e l a b e l d i s t r i b u t i o n s t h e s e c o n d i s t h a t t h e s p l i t n o d e p a r a m e t e r s i n d i f f e r e n t i a b l e d e c i s i o n t r e e s c a n b e l e a r n e d b y b a c k p r o p a g a t i o n w h i c h e n a b l e s a c o m b i n a t i o n o f t r e e l e a r n i n g a n d r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r w e d e f i n e a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r a t r e e b y t h e k u l l b a c k l e i b l e r d i v e r g e n c e k l b e t w e e n t h e g r o u n d t r u t h l a b e l d i s t r i b u t i o n a n d t h e d i s t r i b u t i o n p r e d i c t e d b y t h e t r e e b y f i x i n g s p l i t n o d e s w e s h o w t h a t t h e o p t i m i z a t i o n o f l e a f n o d e p r e d i c t i o n s t o m i n i m i z e t h e l o s s f u n c t i o n o f t h e t r e e c a n b e a d d r e s s e d b y v a r i a t i o n a l b o u n d i n g 1 9 2 9 i n w h i c h t h e o r i g i n a l l o s s f u n c t i o n t o b e m i n i m i z e d g e t s i t e r a t i v e l y r e p l a c e d b y a d e c r e a s i n g s e q u e n c e o f u p p e r b o u n d s f o l l o w i n g t h i s o p t i m i z a t i o n s t r a t e g y w e d e r i v e a d i s c r e t e i t e r a t i v e f u n c t i o n t o u p d a t e t h e l e a f n o d e p r e d i c t i o n s t o l e a r n a f o r e s t w e a v e r a g e t h e l o s s e s o f a l l t h e i n d i v i d u a l t r e e s t o b e t h e l o s s f o r t h e f o r e s t a n d a l l o w t h e s p l i t n o d e s f r o m d i f f e r e n t t r e e s t o b e c o n n e c t e d t o t h e s a m e o u t p u t u n i t o f t h e f e a t u r e l e a r n i n g f u n c t i o n i n t h i s w a y t h e s p l i t n o d e p a r a m e t e r s o f a l l t h e i n d i v i d u a l t r e e s c a n b e l e a r n e d j o i n t l y o u r l d l f s c a n b e u s e d a s a s h a l l o w s t a n d a l o n e m o d e l a n d c a n a l s o b e i n t e g r a t e d w i t h a n y d e e p n e t w o r k s i e t h e f e a t u r e l e a r n i n g f u n c t i o n c a n b e a l i n e a r t r a n s f o r m a t i o n a n d a d e e p n e t w o r k r e s p e c t i v e l y f i g 2 i l l u s t r a t e s a s k e t c h c h a r t o f o u r l d l f s w h e r e a f o r e s t c o n s i s t s o f t w o t r e e s i s s h o w n w e v e r i f y t h e e f f e c t i v e n e s s o f o u r m o d e l o n s e v e r a l l d l t a s k s s u c h a s c r o w d o p i n i o n p r e d i c t i o n o n m o v i e s a n d d i s e a s e p r e d i c t i o n b a s e d o n h u m a n g e n e s a s w e l l a s o n e c o m p u t e r v i s i o n a p p l i c a t i o n i e f a c i a l a g e e s t i m a t i o n s h o w i n g s i g n i f i c a n t i m p r o v e m e n t s t o t h e s t a t e o f t h e a r t l d l m e t h o d s t h e l a b e l d i s t r i b u t i o n s f o r t h e s e t a s k s i n c l u d e b o t h u n i m o d a l d i s t r i b u t i o n s e g t h e a g e d i s t r i b u t i o n i n f i g 1 a a n d m i x t u r e d i s t r i b u t i o n s t h e r a t i n g d i s t r i b u t i o n o n a m o v i e i n f i g 1 b t h e s u p e r i o r i t y o f o u r m o d e l o n b o t h o f t h e m v e r i f i e s i t s a b i l i t y t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s f i g u r e 2 i l l u s t r a t i o n o f a l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t t h e t o p c i r c l e s d e n o t e t h e o u t p u t u n i t s o f t h e f u n c t i o n f p a r a m e t e r i z e d b y xce\\\\x98 w h i c h c a n b e a f e a t u r e v e c t o r o r a f u l l y c o n n e c t e d l a y e r o f a d e e p n e t w o r k t h e b l u e a n d g r e e n c i r c l e s a r e s p l i t n o d e s a n d l e a f n o d e s r e s p e c t i v e l y t w o i n d e x f u n c t i o n xcf\\\\x95 1 a n d xcf\\\\x95 2 a r e a s s i g n e d t o t h e s e t w o t r e e s r e s p e c t i v e l y t h e b l a c k d a s h a r r o w s i n d i c a t e t h e c o r r e s p o n d e n c e b e t w e e n t h e s p l i t n o d e s o f t h e s e t w o t r e e s a n d t h e o u t p u t u n i t s o f f u n c t i o n f n o t e t h a t o n e o u t p u t u n i t m a y c o r r e s p o n d t o t h e s p l i t n o d e s b e l o n g i n g t o d i f f e r e n t t r e e s e a c h t r e e h a s i n d e p e n d e n t l e a f n o d e p r e d i c t i o n s q d e n o t e d b y h i s t o g r a m s i n l e a f n o d e s t h e o u t p u t o f t h e f o r e s t i s a m i x t u r e o f t h e t r e e p r e d i c t i o n s f xc2\\\\xb7 xce\\\\x98 a n d q a r e l e a r n e d j o i n t l y i n a n e n d t o e n d m a n n e r 2 x0c 2 r e l a t e d w o r k s i n c e o u r l d l a l g o r i t h m i s i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s i t i s n e c e s s a r y t o f i r s t r e v i e w s o m e t y p i c a l t e c h n i q u e s o f d e c i s i o n t r e e s t h e n w e d i s c u s s c u r r e n t l d l m e t h o d s d e c i s i o n t r e e s r a n d o m f o r e s t s o r r a n d o m i z e d d e c i s i o n t r e e s 1 6 1 3 4 a r e a p o p u l a r e n s e m b l e p r e d i c t i v e m o d e l s u i t a b l e f o r m a n y m a c h i n e l e a r n i n g t a s k s i n t h e p a s t l e a r n i n g o f a d e c i s i o n t r e e w a s b a s e d o n h e u r i s t i c s s u c h a s a g r e e d y a l g o r i t h m w h e r e l o c a l l y o p t i m a l h a r d d e c i s i o n s a r e m a d e a t e a c h s p l i t n o d e 1 a n d t h u s c a n n o t b e i n t e g r a t e d i n t o i n a d e e p l e a r n i n g f r a m e w o r k i e b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r t h e n e w l y p r o p o s e d d e e p n e u r a l d e c i s i o n f o r e s t s d n d f s 2 0 o v e r c o m e s t h i s p r o b l e m b y i n t r o d u c i n g a s o f t d i f f e r e n t i a b l e d e c i s i o n f u n c t i o n a t t h e s p l i t n o d e s a n d a g l o b a l l o s s f u n c t i o n d e f i n e d o n a t r e e t h i s e n s u r e s t h a t t h e s p l i t n o d e p a r a m e t e r s c a n b e l e a r n e d b y b a c k p r o p a g a t i o n a n d l e a f n o d e p r e d i c t i o n s c a n b e u p d a t e d b y a d i s c r e t e i t e r a t i v e f u n c t i o n o u r m e t h o d e x t e n d s d n d f s t o a d d r e s s l d l p r o b l e m s b u t t h i s e x t e n s i o n i s n o n t r i v i a l b e c a u s e l e a r n i n g l e a f n o d e p r e d i c t i o n s i s a c o n s t r a i n e d c o n v e x o p t i m i z a t i o n p r o b l e m a l t h o u g h a s t e p s i z e f r e e u p d a t e f u n c t i o n w a s g i v e n i n d n d f s t o u p d a t e l e a f n o d e p r e d i c t i o n s i t w a s o n l y p r o v e d t o c o n v e r g e f o r a c l a s s i f i c a t i o n l o s s c o n s e q u e n t l y i t w a s u n c l e a r h o w t o o b t a i n s u c h a n u p d a t e f u n c t i o n f o r o t h e r l o s s e s w e o b s e r v e d h o w e v e r t h a t t h e u p d a t e f u n c t i o n i n d n d f s c a n b e d e r i v e d f r o m v a r i a t i o n a l b o u n d i n g w h i c h a l l o w s u s t o e x t e n d i t t o o u r l d l l o s s i n a d d i t i o n t h e s t r a t e g i e s u s e d i n l d l f s a n d d n d f s t o l e a r n i n g t h e e n s e m b l e o f m u l t i p l e t r e e s f o r e s t s a r e d i f f e r e n t 1 w e e x p l i c i t l y d e f i n e a l o s s f u n c t i o n f o r f o r e s t s w h i l e o n l y t h e l o s s f u n c t i o n f o r a s i n g l e t r e e w a s d e f i n e d i n d n d f s 2 w e a l l o w t h e s p l i t n o d e s f r o m d i f f e r e n t t r e e s t o b e c o n n e c t e d t o t h e s a m e o u t p u t u n i t o f t h e f e a t u r e l e a r n i n g f u n c t i o n w h i l e d n d f s d i d n o t 3 a l l t r e e s i n l d l f s c a n b e l e a r n e d j o i n t l y w h i l e t r e e s i n d n d f s w e r e l e a r n e d a l t e r n a t i v e l y t h e s e c h a n g e s i n t h e e n s e m b l e l e a r n i n g a r e i m p o r t a n t b e c a u s e a s s h o w n i n o u r e x p e r i m e n t s s e c 4 4 l d l f s c a n g e t b e t t e r r e s u l t s b y u s i n g m o r e t r e e s b u t b y u s i n g t h e e n s e m b l e s t r a t e g y p r o p o s e d i n d n d f s t h e r e s u l t s o f f o r e s t s a r e e v e n w o r s e t h a n t h o s e f o r a s i n g l e t r e e t o s u m u p w r t d n d f s 2 0 t h e c o n t r i b u t i o n s o f l d l f s a r e f i r s t w e e x t e n d f r o m c l a s s i f i c a t i o n 2 0 t o d i s t r i b u t i o n l e a r n i n g b y p r o p o s i n g a d i s t r i b u t i o n b a s e d l o s s f o r t h e f o r e s t s a n d d e r i v e t h e g r a d i e n t t o l e a r n s p l i t s n o d e s w r t t h i s l o s s s e c o n d w e d e r i v e d t h e u p d a t e f u n c t i o n f o r l e a f n o d e s b y v a r i a t i o n a l b o u n d i n g h a v i n g o b s e r v e d t h a t t h e u p d a t e f u n c t i o n i n 2 0 w a s a s p e c i a l c a s e o f v a r i a t i o n a l b o u n d i n g l a s t b u t n o t t h e l e a s t w e p r o p o s e a b o v e t h r e e s t r a t e g i e s t o l e a r n i n g t h e e n s e m b l e o f m u l t i p l e t r e e s w h i c h a r e d i f f e r e n t f r o m 2 0 b u t w e s h o w a r e e f f e c t i v e l a b e l d i s t r i b u t i o n l e a r n i n g a n u m b e r o f s p e c i a l i z e d a l g o r i t h m s h a v e b e e n p r o p o s e d t o a d d r e s s t h e l d l t a s k a n d h a v e s h o w n t h e i r e f f e c t i v e n e s s i n m a n y c o m p u t e r v i s i o n a p p l i c a t i o n s s u c h a s f a c i a l a g e e s t i m a t i o n 8 1 1 2 8 e x p r e s s i o n r e c o g n i t i o n 3 0 a n d h a n d o r i e n t a t i o n e s t i m a t i o n 1 0 g e n g e t a l 8 d e f i n e d t h e l a b e l d i s t r i b u t i o n f o r a n i n s t a n c e a s a v e c t o r c o n t a i n i n g t h e p r o b a b i l i t i e s o f t h e i n s t a n c e h a v i n g e a c h l a b e l t h e y a l s o g a v e a s t r a t e g y t o a s s i g n a p r o p e r l a b e l d i s t r i b u t i o n t o a n i n s t a n c e w i t h a s i n g l e l a b e l i e a s s i g n i n g a g a u s s i a n o r t r i a n g l e d i s t r i b u t i o n w h o s e p e a k i s t h e s i n g l e l a b e l a n d p r o p o s e d a n a l g o r i t h m c a l l e d i i s l l d w h i c h i s a n i t e r a t i v e o p t i m i z a t i o n p r o c e s s b a s e d o n a t w o l a y e r e n e r g y b a s e d m o d e l y a n g e t a l 2 8 t h e n d e f i n e d a t h r e e l a y e r e n e r g y b a s e d m o d e l c a l l e d s c e l d l i n w h i c h t h e a b i l i t y t o p e r f o r m f e a t u r e l e a r n i n g i s i m p r o v e d b y a d d i n g t h e e x t r a h i d d e n l a y e r a n d s p a r s i t y c o n s t r a i n t s a r e a l s o i n c o r p o r a t e d t o a m e l i o r a t e t h e m o d e l g e n g 6 d e v e l o p e d a n a c c e l e r a t e d v e r s i o n o f i i s l l d c a l l e d b f g s l d l b y u s i n g q u a s i n e w t o n o p t i m i z a t i o n a l l t h e a b o v e l d l m e t h o d s a s s u m e t h a t t h e l a b e l d i s t r i b u t i o n c a n b e r e p r e s e n t e d b y a m a x i m u m e n t r o p y m o d e l 2 b u t t h e e x p o n e n t i a l p a r t o f t h i s m o d e l r e s t r i c t s t h e g e n e r a l i t y o f t h e d i s t r i b u t i o n f o r m a n o t h e r w a y t o a d d r e s s t h e l d l t a s k i s t o e x t e n d e x i s t i n g l e a r n i n g a l g o r i t h m s t o d e a l w i t h l a b e l d i s t r i b u t i o n s g e n g a n d h o u 7 p r o p o s e d l d s v r a l d l m e t h o d b y e x t e n d i n g s u p p o r t v e c t o r r e g r e s s o r w h i c h f i t a s i g m o i d f u n c t i o n t o e a c h c o m p o n e n t o f t h e d i s t r i b u t i o n s i m u l t a n e o u s l y b y a s u p p o r t v e c t o r m a c h i n e x i n g e t a l 2 7 t h e n e x t e n d e d b o o s t i n g t o a d d r e s s t h e l d l t a s k b y a d d i t i v e w e i g h t e d r e g r e s s o r s t h e y s h o w e d t h a t u s i n g t h e v e c t o r t r e e m o d e l a s t h e w e a k r e g r e s s o r c a n l e a d t o b e t t e r p e r f o r m a n c e a n d n a m e d t h i s m e t h o d a o s o l d l l o g i t b o o s t a s t h e l e a r n i n g o f t h i s t r e e m o d e l i s b a s e d o n l o c a l l y o p t i m a l h a r d d a t a p a r t i t i o n f u n c t i o n s a t e a c h s p l i t n o d e a o s o l d l l o g i t b o o s t i s u n a b l e t o b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g e x t e n d i n g c u r r e n t d e e p l e a r n i n g a l g o r i t h m s t o 3 x0c a d d r e s s t h e l d l t a s k i s a n i n t e r e s t i n g t o p i c b u t t h e e x i s t i n g s u c h a m e t h o d c a l l e d d l d l 5 s t i l l f o c u s e s o n m a x i m u m e n t r o p y m o d e l b a s e d l d l o u r m e t h o d l d l f s e x t e n d s d i f f e r e n t i a b l e d e c i s i o n t r e e s t o a d d r e s s l d l t a s k s i n w h i c h t h e p r e d i c t e d l a b e l d i s t r i b u t i o n f o r a s a m p l e c a n b e e x p r e s s e d b y a l i n e a r c o m b i n a t i o n o f t h e l a b e l d i s t r i b u t i o n s o f t h e t r a i n i n g d a t a a n d t h u s h a v e n o r e s t r i c t i o n s o n t h e d i s t r i b u t i o n s e g n o r e q u i r e m e n t o f t h e m a x i m u m e n t r o p y m o d e l i n a d d i t i o n t h a n k s t o t h e i n t r o d u c t i o n o f d i f f e r e n t i a b l e d e c i s i o n f u n c t i o n s l d l f s c a n b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g e g t o l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r 3 l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s a f o r e s t i s a n e n s e m b l e o f d e c i s i o n t r e e s w e f i r s t i n t r o d u c e h o w t o l e a r n a s i n g l e d e c i s i o n t r e e b y l a b e l d i s t r i b u t i o n l e a r n i n g t h e n d e s c r i b e t h e l e a r n i n g o f a f o r e s t 3 1 p r o b l e m f o r m u l a t i o n l e t x r m d e n o t e t h e i n p u t s p a c e a n d y y 1 y 2 y c d e n o t e t h e c o m p l e t e s e t o f l a b e l s w h e r e c i s t h e n u m b e r o f p o s s i b l e l a b e l v a l u e s w e c o n s i d e r a l a b e l d i s t r i b u t i o n l e a r n i n g l d l p r o b l e m w h e r e f o r e a c h i n p u t s a m p l e x xe2\\\\x88\\\\x88 x t h e r e i s a l a b e l d i s t r i b u t i o n d d x y 1 d y x 2 d y x c xe2\\\\x88\\\\x88 r c h e r e d y x c e x p r e s s e s t h e p r o b a b i l i t y o f t h e s a m p l e x h a v i n g t h e c t h l a b e l y c a n d t h u s h a s t h e p c c o n s t r a i n t s t h a t d y x c xe2\\\\x88\\\\x88 0 1 a n d c 1 d y x c 1 t h e g o a l o f t h e l d l p r o b l e m i s t o l e a r n a m a p p i n g f u n c t i o n g x xe2\\\\x86\\\\x92 d b e t w e e n a n i n p u t s a m p l e x a n d i t s c o r r e s p o n d i n g l a b e l d i s t r i b u t i o n d h e r e w e w a n t t o l e a r n t h e m a p p i n g f u n c t i o n g x b y a d e c i s i o n t r e e b a s e d m o d e l t a d e c i s i o n t r e e c o n s i s t s o f a s e t o f s p l i t n o d e s n a n d a s e t o f l e a f n o d e s l e a c h s p l i t n o d e n xe2\\\\x88\\\\x88 n d e f i n e s a s p l i t f u n c t i o n s n xc2\\\\xb7 xce\\\\x98 x xe2\\\\x86\\\\x92 0 1 p a r a m e t e r i z e d b y xce\\\\x98 t o d e t e r m i n e w h e t h e r a s a m p l e i s s e n t t o t h e l e f t o r r i g h t s u b t r e e e a c h l e a f n o d e xe2\\\\x88\\\\x88 l h o l d s a d i s t r i b u t i o n q q 1 q 2 q c p c o v e r y i e q c xe2\\\\x88\\\\x88 0 1 a n d c 1 q c 1 t o b u i l d a d i f f e r e n t i a b l e d e c i s i o n t r e e f o l l o w i n g 2 0 w e u s e a p r o b a b i l i s t i c s p l i t f u n c t i o n s n x xce\\\\x98 xcf\\\\x83 f xcf\\\\x95 n x xce\\\\x98 w h e r e xcf\\\\x83 xc2\\\\xb7 i s a s i g m o i d f u n c t i o n xcf\\\\x95 xc2\\\\xb7 i s a n i n d e x f u n c t i o n t o b r i n g t h e xcf\\\\x95 n t h o u t p u t o f f u n c t i o n f x xce\\\\x98 i n c o r r e s p o n d e n c e w i t h s p l i t n o d e n a n d f x xe2\\\\x86\\\\x92 r m i s a r e a l v a l u e d f e a t u r e l e a r n i n g f u n c t i o n d e p e n d i n g o n t h e s a m p l e x a n d t h e p a r a m e t e r xce\\\\x98 a n d c a n t a k e a n y f o r m f o r a s i m p l e f o r m i t c a n b e a l i n e a r t r a n s f o r m a t i o n o f x w h e r e xce\\\\x98 i s t h e t r a n s f o r m a t i o n m a t r i x f o r a c o m p l e x f o r m i t c a n b e a d e e p n e t w o r k t o p e r f o r m r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r t h e n xce\\\\x98 i s t h e n e t w o r k p a r a m e t e r t h e c o r r e s p o n d e n c e b e t w e e n t h e s p l i t n o d e s a n d t h e o u t p u t u n i t s o f f u n c t i o n f i n d i c a t e d b y xcf\\\\x95 xc2\\\\xb7 t h a t i s r a n d o m l y g e n e r a t e d b e f o r e t r e e l e a r n i n g i e w h i c h o u t p u t u n i t s f r o m xe2\\\\x80\\\\x9c f xe2\\\\x80\\\\x9d a r e u s e d f o r c o n s t r u c t i n g a t r e e i s d e t e r m i n e d r a n d o m l y a n e x a m p l e t o d e m o n s t r a t e xcf\\\\x95 xc2\\\\xb7 i s s h o w n i n f i g 2 t h e n t h e p r o b a b i l i t y o f t h e s a m p l e x f a l l i n g i n t o l e a f n o d e i s g i v e n b y y l r p x xce\\\\x98 s n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 xe2\\\\x88\\\\x92 s n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 n xe2\\\\x88\\\\x88 n w h e r e 1 xc2\\\\xb7 i s a n i n d i c a t o r f u n c t i o n a n d l l n a n d l r n d e n o t e t h e s e t s o f l e a f n o d e s h e l d b y t h e l e f t a n d r i g h t s u b t r e e s o f n o d e n t n l a n d t n r r e s p e c t i v e l y t h e o u t p u t o f t h e t r e e t w r t x i e t h e m a p p i n g f u n c t i o n g i s d e f i n e d b y x g x xce\\\\x98 t p x xce\\\\x98 q 2 xe2\\\\x88\\\\x88 l 3 2 t r e e o p t i m i z a t i o n g i v e n a t r a i n i n g s e t s x i d i n i 1 o u r g o a l i s t o l e a r n a d e c i s i o n t r e e t d e s c r i b e d i n s e c 3 1 w h i c h c a n o u t p u t a d i s t r i b u t i o n g x i xce\\\\x98 t s i m i l a r t o d i f o r e a c h s a m p l e x i t o t h i s e n d a s t r a i g h t f o r w a r d w a y i s t o m i n i m i z e t h e k u l l b a c k l e i b l e r k l d i v e r g e n c e b e t w e e n e a c h g x i xce\\\\x98 t a n d d i o r e q u i v a l e n t l y t o m i n i m i z e t h e f o l l o w i n g c r o s s e n t r o p y l o s s r q xce\\\\x98 s xe2\\\\x88\\\\x92 n c n c x10 x x11 1 x x y c 1 x x y c d x i l o g g c x i xce\\\\x98 t xe2\\\\x88\\\\x92 d x i l o g p x i xce\\\\x98 q c 3 n i 1 c 1 n i 1 c 1 xe2\\\\x88\\\\x88 l 4 x0c w h e r e q d e n o t e t h e d i s t r i b u t i o n s h e l d b y a l l t h e l e a f n o d e s l a n d g c x i xce\\\\x98 t i s t h e c t h o u t p u t u n i t o f g x i xce\\\\x98 t l e a r n i n g t h e t r e e t r e q u i r e s t h e e s t i m a t i o n o f t w o p a r a m e t e r s 1 t h e s p l i t n o d e p a r a m e t e r xce\\\\x98 a n d 2 t h e d i s t r i b u t i o n s q h e l d b y t h e l e a f n o d e s t h e b e s t p a r a m e t e r s xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 a r e d e t e r m i n e d b y xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 a r g m i n r q xce\\\\x98 s 4 xce\\\\x98 q t o s o l v e e q n 4 w e c o n s i d e r a n a l t e r n a t i n g o p t i m i z a t i o n s t r a t e g y f i r s t w e f i x q a n d o p t i m i z e xce\\\\x98 t h e n w e f i x xce\\\\x98 a n d o p t i m i z e q t h e s e t w o l e a r n i n g s t e p s a r e a l t e r n a t i v e l y p e r f o r m e d u n t i l c o n v e r g e n c e o r a m a x i m u m n u m b e r o f i t e r a t i o n s i s r e a c h e d d e f i n e d i n t h e e x p e r i m e n t s 3 2 1 l e a r n i n g s p l i t n o d e s i n t h i s s e c t i o n w e d e s c r i b e h o w t o l e a r n t h e p a r a m e t e r xce\\\\x98 f o r s p l i t n o d e s w h e n t h e d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s q a r e f i x e d w e c o m p u t e t h e g r a d i e n t o f t h e l o s s r q xce\\\\x98 s w r t xce\\\\x98 b y t h e c h a i n r u l e n xe2\\\\x88\\\\x82 r q xce\\\\x98 s x x xe2\\\\x88\\\\x82 r q xce\\\\x98 s xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 5 xe2\\\\x88\\\\x82 xce\\\\x98 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 i 1 n xe2\\\\x88\\\\x88 n w h e r e o n l y t h e f i r s t t e r m d e p e n d s o n t h e t r e e a n d t h e s e c o n d t e r m d e p e n d s o n t h e s p e c i f i c t y p e o f t h e f u n c t i o n f xcf\\\\x95 n t h e f i r s t t e r m i s g i v e n b y c x01 g c x i xce\\\\x98 t n l x11 g c x i xce\\\\x98 t n r 1 x y c x10 xe2\\\\x88\\\\x82 r q xce\\\\x98 s d x i s n x i xce\\\\x98 xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 s n x i xce\\\\x98 6 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 n c 1 g c x i xce\\\\x98 t g c x i xce\\\\x98 t p p w h e r e g c x i xce\\\\x98 t n l xe2\\\\x88\\\\x88 l l n p x i xce\\\\x98 q c a n d g c x i xce\\\\x98 t n r xe2\\\\x88\\\\x88 l r n p x i xce\\\\x98 q c n o t e t h a t l e t t n b e t h e t r e e r o o t e d a t t h e n o d e n t h e n w e h a v e g c x i xce\\\\x98 t n g c x i xce\\\\x98 t n l g c x i xce\\\\x98 t n r t h i s m e a n s t h e g r a d i e n t c o m p u t a t i o n i n e q n 6 c a n b e s t a r t e d a t t h e l e a f n o d e s a n d c a r r i e d o u t i n a b o t t o m u p m a n n e r t h u s t h e s p l i t n o d e p a r a m e t e r s c a n b e l e a r n e d b y s t a n d a r d b a c k p r o p a g a t i o n 3 2 2 l e a r n i n g l e a f n o d e s n o w f i x i n g t h e p a r a m e t e r xce\\\\x98 w e s h o w h o w t o l e a r n t h e d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s q w h i c h i s a c o n s t r a i n e d o p t i m i z a t i o n p r o b l e m m i n r q xce\\\\x98 s s t xe2\\\\x88\\\\x80 q c x q c 1 7 c 1 h e r e w e p r o p o s e t o a d d r e s s t h i s c o n s t r a i n e d c o n v e x o p t i m i z a t i o n p r o b l e m b y v a r i a t i o n a l b o u n d i n g 1 9 2 9 w h i c h l e a d s t o a s t e p s i z e f r e e a n d f a s t c o n v e r g e d u p d a t e r u l e f o r q i n v a r i a t i o n a l b o u n d i n g a n o r i g i n a l o b j e c t i v e f u n c t i o n t o b e m i n i m i z e d g e t s r e p l a c e d b y i t s b o u n d i n a n i t e r a t i v e m a n n e r a u p p e r b o u n d f o r t h e l o s s f u n c t i o n r q xce\\\\x98 s c a n b e o b t a i n e d b y j e n s e n xe2\\\\x80\\\\x99 s i n e q u a l i t y r q xce\\\\x98 s xe2\\\\x88\\\\x92 n c x10 x x11 1 x x y c d x i l o g p x i xce\\\\x98 q c n i 1 c 1 xe2\\\\x88\\\\x88 l xe2\\\\x89\\\\xa4 xe2\\\\x88\\\\x92 w h e r e xce\\\\xb q c x i 1 n n x c x i 1 c 1 p x i xce\\\\x98 q c g c x i xce\\\\x98 t xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x88\\\\x92 d y x c i x xce\\\\xb q xcc\\\\x84 c x i l o g xe2\\\\x88\\\\x88 l x10 p x xce\\\\x98 q x11 i c xce\\\\xb q xcc\\\\x84 c x i 8 w e d e f i n e n c x10 p x xce\\\\x98 q x11 1 x x y c x i c d x i xce\\\\xb q xcc\\\\x84 c x i l o g n i 1 c 1 xce\\\\xb q xcc\\\\x84 c x i 9 xe2\\\\x88\\\\x88 l t h e n xcf\\\\x86 q q xcc\\\\x84 i s a n u p p e r b o u n d f o r r q xce\\\\x98 s w h i c h h a s t h e p r o p e r t y t h a t f o r a n y q a n d q xcc\\\\x84 xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x89\\\\xa5 r q xce\\\\x98 s a n d xcf\\\\x86 q q r q xce\\\\x98 s a s s u m e t h a t w e a r e a t a p o i n t q t c o r r e s p o n d i n g t o t h e t t h i t e r a t i o n t h e n xcf\\\\x86 q q t i s a n u p p e r b o u n d f o r r q xce\\\\x98 s i n t h e n e x t i t e r a t i o n q t 1 i s c h o s e n s u c h t h a t xcf\\\\x86 q t 1 q xe2\\\\x89\\\\xa4 r q t xce\\\\x98 s w h i c h i m p l i e s r q t 1 xce\\\\x98 s xe2\\\\x89\\\\xa4 r q t xce\\\\x98 s 5 x0c c o n s e q u e n t l y w e c a n m i n i m i z e xcf\\\\x86 q q xcc\\\\x84 i n s t e a d o f r q xce\\\\x98 s a f t e r e n s u r i n g t h a t r q t xce\\\\x98 s xcf\\\\x86 q t q xcc\\\\x84 i e q xcc\\\\x84 q t s o w e h a v e q t 1 a r g m i n xcf\\\\x86 q q t s t xe2\\\\x88\\\\x80 q c x q c 1 1 0 c 1 w h i c h l e a d s t o m i n i m i z i n g t h e l a g r a n g i a n d e f i n e d b y xcf\\\\x95 q q t xcf\\\\x86 q q t x xce\\\\xbb xe2\\\\x88\\\\x88 l w h e r e xce\\\\xbb i s t h e l a g r a n g e m u l t i p l i e r b y s e t t i n g xce\\\\xbb t 1 n o t e t h a t q c t 1 xe2\\\\x88\\\\x88 0 1 a n d d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s t h e s t a r t i n g 0 d i s t r i b u t i o n q c c 1 3 3 q c xe2\\\\x88\\\\x92 1 1 1 c 1 xe2\\\\x88\\\\x82 xcf\\\\x95 q q t xe2\\\\x88\\\\x82 q c n c 1 x x y c t t 1 d xce\\\\xb q c x i a n d q c n i 1 c 1 x i s a t i s f i e s t h a t q c c x 0 w e h a v e p n y c t d x i xce\\\\xb q c x i p c i 1 p n y c t xce\\\\xb q x d x i i c 1 i 1 c 1 2 t 1 1 e q n 1 2 i s t h e u p d a t e s c h e m e f o r c 1 q c 0 p o i n t q c a n b e s i m p l y i n i t i a l i z e d b y t h e u n i f o r m p c l e a r n i n g a f o r e s t a f o r e s t i s a n e n s e m b l e o f d e c i s i o n t r e e s f t 1 t k i n t h e t r a i n i n g s t a g e a l l t r e e s i n t h e f o r e s t f u s e t h e s a m e p a r a m e t e r s xce\\\\x98 f o r f e a t u r e l e a r n i n g f u n c t i o n f xc2\\\\xb7 xce\\\\x98 b u t c o r r e s p o n d t o d i f f e r e n t o u t p u t u n i t s o f f a s s i g n e d b y xcf\\\\x95 s e e f i g 2 b u t e a c h t r e e h a s i n d e p e n d e n t l e a f n o d e p r e d i c t i o n s q t h e l o s s f u n c t i o n f o r a f o r e s t i s g i v e n b y a v e r a g i n g t h e l o s s f u n c t i o n s f o r a l l i n d i v i d u a l t r e e s p k 1 r f k k 1 r t k w h e r e r t k i s t h e l o s s f u n c t i o n f o r t r e e t k d e f i n e d b y e q n 3 t o l e a r n xce\\\\x98 b y f i x i n g t h e l e a f n o d e p r e d i c t i o n s q o f a l l t h e t r e e s i n t h e f o r e s t f b a s e d o n t h e d e r i v a t i o n i n s e c 3 2 a n d r e f e r r i n g t o f i g 2 w e h a v e n k xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x i xce\\\\x98 1 x x x xe2\\\\x88\\\\x82 r f xe2\\\\x88\\\\x82 r t k xe2\\\\x88\\\\x82 xce\\\\x98 k i 1 xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x i xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 3 k 1 n xe2\\\\x88\\\\x88 n k w h e r e n k a n d xcf\\\\x95 k xc2\\\\xb7 a r e t h e s p l i t n o d e s e t a n d t h e i n d e x f u n c t i o n o f t k r e s p e c t i v e l y n o t e t h a t t h e i n d e x f u n c t i o n xcf\\\\x95 k xc2\\\\xb7 f o r e a c h t r e e i s r a n d o m l y a s s i g n e d b e f o r e t r e e l e a r n i n g a n d t h u s s p l i t n o d e s c o r r e s p o n d t o a s u b s e t o f o u t p u t u n i t s o f f t h i s s t r a t e g y i s s i m i l a r t o t h e r a n d o m s u b s p a c e m e t h o d 1 7 w h i c h i n c r e a s e s t h e r a n d o m n e s s i n t r a i n i n g t o r e d u c e t h e r i s k o f o v e r f i t t i n g a s f o r q s i n c e e a c h t r e e i n t h e f o r e s t f h a s i t s o w n l e a f n o d e p r e d i c t i o n s q w e c a n u p d a t e t h e m i n d e p e n d e n t l y b y e q n 1 2 g i v e n b y xce\\\\x98 f o r i m p l e m e n t a t i o n a l c o n v e n i e n c e w e d o n o t c o n d u c t t h i s u p d a t e s c h e m e o n t h e w h o l e d a t a s e t s b u t o n a s e t o f m i n i b a t c h e s b t h e t r a i n i n g p r o c e d u r e o f a l d l f i s s h o w n i n a l g o r i t h m 1 a l g o r i t h m 1 t h e t r a i n i n g p r o c e d u r e o f a l d l f r e q u i r e s t r a i n i n g s e t n b t h e n u m b e r o f m i n i b a t c h e s t o u p d a t e q i n i t i a l i z e xce\\\\x98 r a n d o m l y a n d q u n i f o r m l y s e t b xe2\\\\x88\\\\x85 w h i l e n o t c o n v e r g e d o w h i l e b n b d o r a n d o m l y s e l e c t a m i n i b a t c h b f r o m s u p d a t e s xce\\\\x98 b y c o m p u t i n g g r a d i e n t e q n 1 3 o n b b b b e n d w h i l e u p d a t e q b y i t e r a t i n g e q n 1 2 o n b b xe2\\\\x88\\\\x85 e n d w h i l e i n t h e t e s t i n g s t a g e t h e o u t p u t o f t h e f o r e s t f i s g i v e n b y a v e r a g i n g t h e p r e d i c t i o n s f r o m a l l t h e p k 1 i n d i v i d u a l t r e e s g x xce\\\\x98 f k k 1 g x xce\\\\x98 t k 6 x0c 4 e x p e r i m e n t a l r e s u l t s o u r r e a l i z a t i o n o f l d l f s i s b a s e d o n xe2\\\\x80\\\\x9c c a f f e xe2\\\\x80\\\\x9d 1 8 i t i s m o d u l a r a n d i m p l e m e n t e d a s a s t a n d a r d n e u r a l n e t w o r k l a y e r w e c a n e i t h e r u s e i t a s a s h a l l o w s t a n d a l o n e m o d e l s l d l f s o r i n t e g r a t e i t w i t h a n y d e e p n e t w o r k s d l d l f s w e e v a l u a t e s l d l f s o n d i f f e r e n t l d l t a s k s a n d c o m p a r e i t w i t h o t h e r s t a n d a l o n e l d l m e t h o d s a s d l d l f s c a n b e l e a r n e d f r o m r a w i m a g e d a t a i n a n e n d t o e n d m a n n e r w e v e r i f y d l d l f s o n a c o m p u t e r v i s i o n a p p l i c a t i o n i e f a c i a l a g e e s t i m a t i o n t h e d e f a u l t s e t t i n g s f o r t h e p a r a m e t e r s o f o u r f o r e s t s a r e t r e e n u m b e r 5 t r e e d e p t h 7 o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n 6 4 i t e r a t i o n t i m e s t o u p d a t e l e a f n o d e p r e d i c t i o n s 2 0 t h e n u m b e r o f m i n i b a t c h e s t o u p d a t e l e a f n o d e p r e d i c t i o n s 1 0 0 m a x i m u m i t e r a t i o n 2 5 0 0 0 4 1 c o m p a r i s o n o f s l d l f s t o s t a n d a l o n e l d l m e t h o d s w e c o m p a r e o u r s h a l l o w m o d e l s l d l f s w i t h o t h e r s t a t e o f t h e a r t s t a n d a l o n e l d l m e t h o d s f o r s l d l f s t h e f e a t u r e l e a r n i n g f u n c t i o n f x xce\\\\x98 i s a l i n e a r t r a n s f o r m a t i o n o f x i e t h e i t h o u t p u t u n i t f i x xce\\\\xb8 i xce\\\\xb8 i x w h e r e xce\\\\xb8 i i s t h e i t h c o l u m n o f t h e t r a n s f o r m a t i o n m a t r i x xce\\\\x98 w e u s e d 3 p o p u l a r l d l d a t a s e t s i n 6 m o v i e h u m a n g e n e a n d n a t u r a l s c e n e 1 t h e s a m p l e s i n t h e s e 3 d a t a s e t s a r e r e p r e s e n t e d b y n u m e r i c a l d e s c r i p t o r s a n d t h e g r o u n d t r u t h s f o r t h e m a r e t h e r a t i n g d i s t r i b u t i o n s o f c r o w d o p i n i o n o n m o v i e s t h e d i s e a s e s d i s t r i b u t i o n s r e l a t e d t o h u m a n g e n e s a n d l a b e l d i s t r i b u t i o n s o n s c e n e s s u c h a s p l a n t s k y a n d c l o u d r e s p e c t i v e l y t h e l a b e l d i s t r i b u t i o n s o f t h e s e 3 d a t a s e t s a r e m i x t u r e d i s t r i b u t i o n s s u c h a s t h e r a t i n g d i s t r i b u t i o n s h o w n i n f i g 1 b f o l l o w i n g 7 2 7 w e u s e 6 m e a s u r e s t o e v a l u a t e t h e p e r f o r m a n c e s o f l d l m e t h o d s w h i c h c o m p u t e t h e a v e r a g e s i m i l a r i t y d i s t a n c e b e t w e e n t h e p r e d i c t e d r a t i n g d i s t r i b u t i o n s a n d t h e r e a l r a t i n g d i s t r i b u t i o n s i n c l u d i n g 4 d i s t a n c e m e a s u r e s k l e u c l i d e a n s xcf\\\\x86 r e n s e n s q u a r e d xcf\\\\x87 2 a n d t w o s i m i l a r i t y m e a s u r e s f i d e l i t y i n t e r s e c t i o n w e e v a l u a t e o u r s h a l l o w m o d e l s l d l f s o n t h e s e 3 d a t a s e t s a n d c o m p a r e i t w i t h o t h e r s t a t e o f t h e a r t s t a n d a l o n e l d l m e t h o d s t h e r e s u l t s o f s l d l f s a n d t h e c o m p e t i t o r s a r e s u m m a r i z e d i n t a b l e 1 f o r m o v i e w e q u o t e t h e r e s u l t s r e p o r t e d i n 2 7 a s t h e c o d e o f 2 7 i s n o t p u b l i c l y a v a i l a b l e f o r t h e r e s u l t s o f t h e o t h e r s t w o w e r u n c o d e t h a t t h e a u t h o r s h a d m a d e a v a i l a b l e i n a l l c a s e f o l l o w i n g 2 7 6 w e s p l i t e a c h d a t a s e t i n t o 1 0 f i x e d f o l d s a n d d o s t a n d a r d t e n f o l d c r o s s v a l i d a t i o n w h i c h r e p r e s e n t s t h e r e s u l t b y xe2\\\\x80\\\\x9c m e a n xc2\\\\xb1 s t a n d a r d d e v i a t i o n xe2\\\\x80\\\\x9d a n d m a t t e r s l e s s h o w t r a i n i n g a n d t e s t i n g d a t a g e t d i v i d e d a s c a n b e s e e n f r o m t a b l e 1 s l d l f s p e r f o r m b e s t o n a l l o f t h e s i x m e a s u r e s t a b l e 1 c o m p a r i s o n r e s u l t s o n t h r e e l d l d a t a s e t s 6 xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x91 xe2\\\\x80\\\\x9d a n d xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x93 xe2\\\\x80\\\\x9d i n d i c a t e t h e l a r g e r a n d t h e s m a l l e r t h e b e t t e r r e s p e c t i v e l y d a t a s e t m e t h o d k l xe2\\\\x86\\\\x93 e u c l i d e a n xe2\\\\x86\\\\x93 s xcf\\\\x86 r e n s e n xe2\\\\x86\\\\x93 s q u a r e d xcf\\\\x87 2 xe2\\\\x86\\\\x93 f i d e l i t y xe2\\\\x86\\\\x91 i n t e r s e c t i o n xe2\\\\x86\\\\x91 m o v i e s l d l f o u r s a o s o l d l o g i t b o o s t 2 7 l d l o g i t b o o s t 2 7 l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 0 7 3 xc2\\\\xb1 0 0 0 5 0 0 8 6 xc2\\\\xb1 0 0 0 4 0 0 9 0 xc2\\\\xb1 0 0 0 4 0 0 9 2 xc2\\\\xb1 0 0 0 5 0 0 9 9 xc2\\\\xb1 0 0 0 4 0 1 2 9 xc2\\\\xb1 0 0 0 7 0 1 3 3 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 9 xc2\\\\xb1 0 0 0 3 0 1 5 8 xc2\\\\xb1 0 0 0 4 0 1 6 7 xc2\\\\xb1 0 0 0 4 0 1 8 7 xc2\\\\xb1 0 0 0 4 0 1 3 0 xc2\\\\xb1 0 0 0 3 0 1 5 2 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 6 xc2\\\\xb1 0 0 0 4 0 1 6 4 xc2\\\\xb1 0 0 0 3 0 1 8 3 xc2\\\\xb1 0 0 0 4 0 0 7 0 xc2\\\\xb1 0 0 0 4 0 0 8 4 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 4 0 0 9 6 xc2\\\\xb1 0 0 0 4 0 1 2 0 xc2\\\\xb1 0 0 0 5 0 9 8 1 xc2\\\\xb1 0 0 0 1 0 9 7 8 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 4 xc2\\\\xb1 0 0 0 1 0 9 6 7 xc2\\\\xb1 0 0 0 1 0 8 7 0 xc2\\\\xb1 0 0 0 3 0 8 4 8 xc2\\\\xb1 0 0 0 3 0 8 4 5 xc2\\\\xb1 0 0 0 3 0 8 4 4 xc2\\\\xb1 0 0 0 4 0 8 3 6 xc2\\\\xb1 0 0 0 3 0 8 1 7 xc2\\\\xb1 0 0 0 4 s l d l f o u r s l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 2 2 8 xc2\\\\xb1 0 0 0 6 0 2 4 5 xc2\\\\xb1 0 0 1 9 0 2 3 1 xc2\\\\xb1 0 0 2 1 0 2 3 9 xc2\\\\xb1 0 0 1 8 0 0 8 5 xc2\\\\xb1 0 0 0 2 0 0 9 9 xc2\\\\xb1 0 0 0 5 0 0 7 6 xc2\\\\xb1 0 0 0 6 0 0 8 9 xc2\\\\xb1 0 0 0 6 0 2 1 2 xc2\\\\xb1 0 0 0 2 0 2 2 9 xc2\\\\xb1 0 0 1 5 0 2 3 1 xc2\\\\xb1 0 0 1 2 0 2 5 3 xc2\\\\xb1 0 0 0 9 0 1 7 9 xc2\\\\xb1 0 0 0 4 0 1 8 9 xc2\\\\xb1 0 0 2 1 0 2 1 1 xc2\\\\xb1 0 0 1 8 0 2 0 5 xc2\\\\xb1 0 0 1 2 0 9 4 8 xc2\\\\xb1 0 0 0 1 0 9 4 0 xc2\\\\xb1 0 0 0 6 0 9 3 8 xc2\\\\xb1 0 0 0 8 0 9 4 4 xc2\\\\xb1 0 0 0 3 0 7 8 8 xc2\\\\xb1 0 0 0 2 0 7 7 1 xc2\\\\xb1 0 0 1 5 0 7 6 9 xc2\\\\xb1 0 0 1 2 0 7 4 7 xc2\\\\xb1 0 0 0 9 s l d l f o u r s l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 5 3 4 xc2\\\\xb1 0 0 1 3 0 8 5 2 xc2\\\\xb1 0 0 2 3 0 8 5 6 xc2\\\\xb1 0 0 6 1 0 8 7 9 xc2\\\\xb1 0 0 2 3 0 3 1 7 xc2\\\\xb1 0 0 1 4 0 5 1 1 xc2\\\\xb1 0 0 2 1 0 4 7 5 xc2\\\\xb1 0 0 2 9 0 4 5 8 xc2\\\\xb1 0 0 1 4 0 3 3 6 xc2\\\\xb1 0 0 1 0 0 4 9 2 xc2\\\\xb1 0 0 1 6 0 5 0 8 xc2\\\\xb1 0 0 2 6 0 5 3 9 xc2\\\\xb1 0 0 1 1 0 4 4 8 xc2\\\\xb1 0 0 1 7 0 5 9 5 xc2\\\\xb1 0 0 2 6 0 7 1 6 xc2\\\\xb1 0 0 4 1 0 7 9 2 xc2\\\\xb1 0 0 1 9 0 8 2 4 xc2\\\\xb1 0 0 0 8 0 8 1 3 xc2\\\\xb1 0 0 0 8 0 7 2 2 xc2\\\\xb1 0 0 2 1 0 6 8 6 xc2\\\\xb1 0 0 0 9 0 6 6 4 xc2\\\\xb1 0 0 1 0 0 5 0 9 xc2\\\\xb1 0 0 1 6 0 4 9 2 xc2\\\\xb1 0 0 2 6 0 4 6 1 xc2\\\\xb1 0 0 1 1 h u m a n g e n e n a t u r a l s c e n e 4 2 e v a l u a t i o n o f d l d l f s o n f a c i a l a g e e s t i m a t i o n i n s o m e l i t e r a t u r e 8 1 1 2 8 1 5 5 a g e e s t i m a t i o n i s f o r m u l a t e d a s a l d l p r o b l e m w e c o n d u c t f a c i a l a g e e s t i m a t i o n e x p e r i m e n t s o n m o r p h 2 4 w h i c h c o n t a i n s m o r e t h a n 5 0 0 0 0 f a c i a l i m a g e s f r o m a b o u t 1 3 0 0 0 p e o p l e o f d i f f e r e n t r a c e s e a c h f a c i a l i m a g e i s a n n o t a t e d w i t h a c h r o n o l o g i c a l a g e t o g e n e r a t e a n a g e d i s t r i b u t i o n f o r e a c h f a c e i m a g e w e f o l l o w t h e s a m e s t r a t e g y u s e d i n 8 2 8 5 w h i c h u s e s a g a u s s i a n d i s t r i b u t i o n w h o s e m e a n i s t h e c h r o n o l o g i c a l a g e o f t h e f a c e i m a g e f i g 1 a t h e p r e d i c t e d a g e f o r a f a c e i m a g e i s s i m p l y t h e a g e h a v i n g t h e h i g h e s t p r o b a b i l i t y i n t h e p r e d i c t e d 1 w e d o w n l o a d t h e s e d a t a s e t s f r o m h t t p c s e s e u e d u c n p e o p l e x g e n g l d l i n d e x h t m 7 x0c l a b e l d i s t r i b u t i o n t h e p e r f o r m a n c e o f a g e e s t i m a t i o n i s e v a l u a t e d b y t h e m e a n a b s o l u t e e r r o r m a e b e t w e e n p r e d i c t e d a g e s a n d c h r o n o l o g i c a l a g e s a s t h e c u r r e n t s t a t e o f t h e a r t r e s u l t o n m o r p h i s o b t a i n b y f i n e t u n i n g d l d l 5 o n v g g f a c e 2 3 w e a l s o b u i l d a d l d l f o n v g g f a c e b y r e p l a c i n g t h e s o f t m a x l a y e r i n v g g n e t b y a l d l f f o l l o w i n g 5 w e d o s t a n d a r d 1 0 t e n f o l d c r o s s v a l i d a t i o n a n d t h e r e s u l t s a r e s u m m a r i z e d i n t a b l e 2 w h i c h s h o w s d l d l f a c h i e v e t h e s t a t e o f t h e a r t p e r f o r m a n c e o n m o r p h n o t e t h a t t h e s i g n i f i c a n t p e r f o r m a n c e g a i n b e t w e e n d e e p l d l m o d e l s d l d l a n d d l d l f a n d n o n d e e p l d l m o d e l s i i s l d l c p n n b f g s l d l a n d t h e s u p e r i o r i t y o f d l d l f c o m p a r e d w i t h d l d l v e r i f i e s t h e e f f e c t i v e n e s s o f e n d t o e n d l e a r n i n g a n d o u r t r e e b a s e d m o d e l f o r l d l r e s p e c t i v e l y t a b l e 2 m a e o f a g e e s t i m a t i o n c o m p a r i s o n o n m o r p h 2 4 m e t h o d i i s l d l 1 1 c p n n 1 1 b f g s l d l 6 d l d l v g g f a c e 5 d l d l f v g g f a c e o u r s m a e 5 6 7 xc2\\\\xb1 0 1 5 4 8 7 xc2\\\\xb1 0 3 1 3 9 4 xc2\\\\xb1 0 0 5 2 4 2 xc2\\\\xb1 0 0 1 2 2 4 xc2\\\\xb1 0 0 2 a s t h e d i s t r i b u t i o n o f g e n d e r a n d e t h n i c i t y i s v e r y u n b a l a n c e d i n m o r p h m a n y a g e e s t i m a t i o n m e t h o d s 1 3 1 4 1 5 a r e e v a l u a t e d o n a s u b s e t o f m o r p h c a l l e d m o r p h s u b f o r s h o r t w h i c h c o n s i s t s o f 2 0 1 6 0 s e l e c t e d f a c i a l i m a g e s t o a v o i d t h e i n f l u e n c e o f u n b a l a n c e d d i s t r i b u t i o n t h e b e s t p e r f o r m a n c e r e p o r t e d o n m o r p h s u b i s g i v e n b y d 2 l d l 1 5 a d a t a d e p e n d e n t l d l m e t h o d a s d 2 l d l u s e d t h e o u t p u t o f t h e xe2\\\\x80\\\\x9c f c 7 xe2\\\\x80\\\\x9d l a y e r i n a l e x n e t 2 1 a s t h e f a c e i m a g e f e a t u r e s h e r e w e i n t e g r a t e a l d l f w i t h a l e x n e t f o l l o w i n g t h e e x p e r i m e n t s e t t i n g u s e d i n d 2 l d l w e e v a l u a t e o u r d l d l f a n d t h e c o m p e t i t o r s i n c l u d i n g b o t h s l l a n d l d l b a s e d m e t h o d s u n d e r s i x d i f f e r e n t t r a i n i n g s e t r a t i o s 1 0 t o 6 0 a l l o f t h e c o m p e t i t o r s a r e t r a i n e d o n t h e s a m e d e e p f e a t u r e s u s e d b y d 2 l d l a s c a n b e s e e n f r o m t a b l e 3 o u r d l d l f s s i g n i f i c a n t l y o u t p e r f o r m o t h e r s f o r a l l t r a i n i n g s e t r a t i o s n o t e t h a t t h e g e n e r a t e d a g e d i s t r i f i g u r e 3 m a e o f a g e e s t i m a t i o n c o m p a r i s o n o n b u t i o n s a r e u n i m o d a l d i s t r i b u t i o n s m o r p h s u b a n d t h e l a b e l d i s t r i b u t i o n s u s e d i n t r a i n i n g s e t r a t i o m e t h o d s e c 4 1 a r e m i x t u r e d i s t r i b u t i o n s 1 0 2 0 3 0 4 0 5 0 6 0 t h e p r o p o s e d m e t h o d l d l f s a c h i e v e a a s 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 t h e s t a t e o f t h e a r t r e s u l t s o n b o t h o f l a r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 i i s a l d l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 t h e m w h i c h v e r i f i e s t h a t o u r m o d e l d 2 l d l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h a s t h e a b i l i t y t o m o d e l a n y g e n e r a l d l d l f o u r s 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f o r m o f l a b e l d i s t r i b u t i o n s 4 3 t i m e c o m p l e x i t y l e t h a n d s b b e t h e t r e e d e p t h a n d t h e b a t c h s i z e r e s p e c t i v e l y e a c h t r e e h a s 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 s p l i t n o d e s a n d 2 h xe2\\\\x88\\\\x92 1 l e a f n o d e s l e t d 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 f o r o n e t r e e a n d o n e s a m p l e t h e c o m p l e x i t y o f a f o r w a r d p a s s a n d a b a c k w a r d p a s s a r e o d d 1 xc3\\\\x97 c o d xc3\\\\x97 c a n d o d 1 xc3\\\\x97 c d xc3\\\\x97 c o d xc3\\\\x97 c r e s p e c t i v e l y s o f o r k t r e e s a n d n b b a t c h e s t h e c o m p l e x i t y o f a f o r w a r d a n d b a c k w a r d p a s s i s o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b t h e c o m p l e x i t y o f a n i t e r a t i o n t o u p d a t e l e a f n o d e s a r e o n b xc3\\\\x97 s b xc3\\\\x97 k xc3\\\\x97 c xc3\\\\x97 d 1 o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b t h u s t h e c o m p l e x i t y f o r t h e t r a i n i n g p r o c e d u r e o n e e p o c h n b b a t c h e s a n d t h e t e s t i n g p r o c e d u r e o n e s a m p l e a r e o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b a n d o d xc3\\\\x97 c xc3\\\\x97 k r e s p e c t i v e l y l d l f s a r e e f f i c i e n t o n m o r p h s u b 1 2 6 3 6 t r a i n i n g i m a g e s 8 4 2 4 t e s t i n g i m a g e s o u r m o d e l o n l y t a k e s 5 2 5 0 s f o r t r a i n i n g 2 5 0 0 0 i t e r a t i o n s a n d 8 s f o r t e s t i n g a l l 8 4 2 4 i m a g e s 4 4 p a r a m e t e r d i s c u s s i o n n o w w e d i s c u s s t h e i n f l u e n c e o f p a r a m e t e r s e t t i n g s o n p e r f o r m a n c e w e r e p o r t t h e r e s u l t s o f r a t i n g p r e d i c t i o n o n m o v i e m e a s u r e d b y k l a n d a g e e s t i m a t i o n o n m o r p h s u b w i t h 6 0 t r a i n i n g s e t r a t i o m e a s u r e d b y m a e f o r d i f f e r e n t p a r a m e t e r s e t t i n g s i n t h i s s e c t i o n t r e e n u m b e r a s a f o r e s t i s a n e n s e m b l e m o d e l i t i s n e c e s s a r y t o i n v e s t i g a t e h o w p e r f o r m a n c e s c h a n g e b y v a r y i n g t h e t r e e n u m b e r u s e d i n a f o r e s t n o t e t h a t a s w e d i s c u s s e d i n s e c 2 t h e e n s e m b l e s t r a t e g y t o l e a r n a f o r e s t p r o p o s e d i n d n d f s 2 0 i s d i f f e r e n t f r o m o u r s t h e r e f o r e i t i s n e c e s s a r y t o s e e w h i c h e n s e m b l e s t r a t e g y i s b e t t e r t o l e a r n a f o r e s t t o w a r d s t h i s e n d w e r e p l a c e o u r e n s e m b l e s t r a t e g y i n d l d l f s b y t h e o n e u s e d i n d n d f s a n d n a m e t h i s m e t h o d d n d f s l d l t h e c o r r e s p o n d i n g s h a l l o w m o d e l i s n a m e d b y s n d f s l d l w e f i x o t h e r p a r a m e t e r s i e t r e e d e p t h a n d 8 x0c o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n a s t h e d e f a u l t s e t t i n g a s s h o w n i n f i g 4 a o u r e n s e m b l e s t r a t e g y c a n i m p r o v e t h e p e r f o r m a n c e b y u s i n g m o r e t r e e s w h i l e t h e o n e u s e d i n d n d f s e v e n l e a d s t o a w o r s e p e r f o r m a n c e t h a n o n e f o r a s i n g l e t r e e o b s e r v e d f r o m f i g 4 t h e p e r f o r m a n c e o f l d l f s c a n b e i m p r o v e d b y u s i n g m o r e t r e e s b u t t h e i m p r o v e m e n t b e c o m e s i n c r e a s i n g l y s m a l l e r a n d s m a l l e r t h e r e f o r e u s i n g m u c h l a r g e r e n s e m b l e s d o e s n o t y i e l d a b i g i m p r o v e m e n t o n m o v i e t h e n u m b e r o f t r e e s k 1 0 0 k l 0 0 7 0 v s k 2 0 k l 0 0 7 1 n o t e t h a t n o t a l l r a n d o m f o r e s t s b a s e d m e t h o d s u s e a l a r g e n u m b e r o f t r e e s e g s h o t t o n e t a l 2 5 o b t a i n e d v e r y g o o d p o s e e s t i m a t i o n r e s u l t s f r o m d e p t h i m a g e s b y o n l y 3 d e c i s i o n t r e e s t r e e d e p t h t r e e d e p t h i s a n o t h e r i m p o r t a n t p a r a m e t e r f o r d e c i s i o n t r e e s i n l d l f s t h e r e i s a n i m p l i c i t c o n s t r a i n t b e t w e e n t r e e d e p t h h a n d o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n xcf\\\\x84 xcf\\\\x84 xe2\\\\x89\\\\xa5 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 t o d i s c u s s t h e i n f l u e n c e o f t r e e d e p t h t o t h e p e r f o r m a n c e o f d l d l f s w e s e t xcf\\\\x84 2 h xe2\\\\x88\\\\x92 1 a n d f i x t r e e n u m b e r k 1 a n d t h e p e r f o r m a n c e c h a n g e b y v a r y i n g t r e e d e p t h i s s h o w n i n f i g 4 b w e s e e t h a t t h e p e r f o r m a n c e f i r s t i m p r o v e s t h e n d e c r e a s e s w i t h t h e i n c r e a s e o f t h e t r e e d e p t h t h e r e a s o n i s a s t h e t r e e d e p t h i n c r e a s e s t h e d i m e n s i o n o f l e a r n e d f e a t u r e s i n c r e a s e s e x p o n e n t i a l l y w h i c h g r e a t l y i n c r e a s e s t h e t r a i n i n g d i f f i c u l t y s o u s i n g m u c h l a r g e r d e p t h s m a y l e a d t o b a d p e r f o r m a n c e o n m o v i e t r e e d e p t h h 1 8 k l 0 1 1 6 2 v s h 9 k l 0 0 8 3 1 f i g u r e 4 t h e p e r f o r m a n c e c h a n g e o f a g e e s t i m a t i o n o n m o r p h s u b a n d r a t i n g p r e d i c t i o n o n m o v i e b y v a r y i n g a t r e e n u m b e r a n d b t r e e d e p t h o u r a p p r o a c h d l d l f s s l d l f s c a n i m p r o v e t h e p e r f o r m a n c e b y u s i n g m o r e t r e e s w h i l e u s i n g t h e e n s e m b l e s t r a t e g y p r o p o s e d i n d n d f s d n d f s l d l s n d f s l d l e v e n l e a d s t o a w o r s e p e r f o r m a n c e t h a n o n e f o r a s i n g l e t r e e 5 c o n c l u s i o n w e p r e s e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s w e d e f i n e d a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r t h e f o r e s t s a n d f o u n d t h a t t h e l e a f n o d e p r e d i c t i o n s c a n b e o p t i m i z e d v i a v a r i a t i o n a l b o u n d i n g w h i c h e n a b l e s a l l t h e t r e e s a n d t h e f e a t u r e t h e y u s e t o b e l e a r n e d j o i n t l y i n a n e n d t o e n d m a n n e r e x p e r i m e n t a l r e s u l t s s h o w e d t h e s u p e r i o r i t y o f o u r a l g o r i t h m f o r s e v e r a l l d l t a s k s a n d a r e l a t e d c o m p u t e r v i s i o n a p p l i c a t i o n a n d v e r i f i e d o u r m o d e l h a s t h e a b i l i t y t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s a c k n o w l e d g e m e n t t h i s w o r k w a s s u p p o r t e d i n p a r t b y t h e n a t i o n a l n a t u r a l s c i e n c e f o u n d a t i o n o f c h i n a n o 6 1 6 7 2 3 3 6 i n p a r t b y xe2\\\\x80\\\\x9c c h e n g u a n g xe2\\\\x80\\\\x9d p r o j e c t s u p p o r t e d b y s h a n g h a i m u n i c i p a l e d u c a t i o n c o m m i s s i o n a n d s h a n g h a i e d u c a t i o n d e v e l o p m e n t f o u n d a t i o n n o 1 5 c g 4 3 a n d i n p a r t b y o n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e s 1 y a m i t a n d d g e m a n s h a p e q u a n t i z a t i o n a n d r e c o g n i t i o n w i t h r a n d o m i z e d t r e e s n e u r a l c o m p u t a t i o n 9 7 1 5 4 5 xe2\\\\x80\\\\x93 1 5 8 8 1 9 9 7 2 a l b e r g e r s d p i e t r a a n d v j d p i e t r a a m a x i m u m e n t r o p y a p p r o a c h t o n a t u r a l l a n g u a g e p r o c e s s i n g c o m p u t a t i o n a l l i n g u i s t i c s 2 2 1 3 9 xe2\\\\x80\\\\x93 7 1 1 9 9 6 3 l b r e i m a n r a n d o m f o r e s t s m a c h i n e l e a r n i n g 4 5 1 5 xe2\\\\x80\\\\x93 3 2 2 0 0 1 4 a c r i m i n i s i a n d j s h o t t o n d e c i s i o n f o r e s t s f o r c o m p u t e r v i s i o n a n d m e d i c a l i m a g e a n a l y s i s s p r i n g e r 2 0 1 3 5 b b g a o c x i n g c w x i e j w u a n d x g e n g d e e p l a b e l d i s t r i b u t i o n l e a r n i n g w i t h l a b e l a m b i g u i t y a r x i v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l a b e l d i s t r i b u t i o n l e a r n i n g i e e e t r a n s k n o w l d a t a e n g 2 8 7 1 7 3 4 xe2\\\\x80\\\\x93 1 7 4 8 2 0 1 6 9 x0c 7 x g e n g a n d p h o u p r e r e l e a s e p r e d i c t i o n o f c r o w d o p i n i o n o n m o v i e s b y l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o i j c a i p a g e s 3 5 1 1 xe2\\\\x80\\\\x93 3 5 1 7 2 0 1 5 8 x g e n g k s m i t h m i l e s a n d z z h o u f a c i a l a g e e s t i m a t i o n b y l e a r n i n g f r o m l a b e l d i s t r i b u t i o n s i n p r o c a a a i 2 0 1 0 9 x g e n g q w a n g a n d y x i a f a c i a l a g e e s t i m a t i o n b y a d a p t i v e l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o c i c p r p a g e s 4 4 6 5 xe2\\\\x80\\\\x93 4 4 7 0 2 0 1 4 1 0 x g e n g a n d y x i a h e a d p o s e e s t i m a t i o n b a s e d o n m u l t i v a r i a t e l a b e l d i s t r i b u t i o n i n p r o c c v p r p a g e s 1 8 3 7 xe2\\\\x80\\\\x93 1 8 4 2 2 0 1 4 1 1 x g e n g c y i n a n d z z h o u f a c i a l a g e e s t i m a t i o n b y l e a r n i n g f r o m l a b e l d i s t r i b u t i o n s i e e e t r a n s p a t t e r n a n a l m a c h i n t e l l 3 5 1 0 2 4 0 1 xe2\\\\x80\\\\x93 2 4 1 2 2 0 1 3 1 2 g g u o y f u c r d y e r a n d t s h u a n g i m a g e b a s e d h u m a n a g e e s t i m a t i o n b y m a n i f o l d l e a r n i n g a n d l o c a l l y a d j u s t e d r o b u s t r e g r e s s i o n i e e e t r a n s i m a g e p r o c e s s i n g 1 7 7 1 1 7 8 xe2\\\\x80\\\\x93 1 1 8 8 2 0 0 8 1 3 g g u o a n d g m u h u m a n a g e e s t i m a t i o n w h a t i s t h e i n f l u e n c e a c r o s s r a c e a n d g e n d e r i n c v p r w o r k s h o p s p a g e s 7 1 xe2\\\\x80\\\\x93 7 8 2 0 1 0 1 4 g g u o a n d c z h a n g a s t u d y o n c r o s s p o p u l a t i o n a g e e s t i m a t i o n i n p r o c c v p r p a g e s 4 2 5 7 xe2\\\\x80\\\\x93 4 2 6 3 2 0 1 4 1 5 z h e x l i z z h a n g f w u x g e n g y z h a n g m h y a n g a n d y z h u a n g d a t a d e p e n d e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r a g e e s t i m a t i o n i e e e t r a n s o n i m a g e p r o c e s s i n g 2 0 1 7 1 6 t k h o r a n d o m d e c i s i o n f o r e s t s i n p r o c i c d a r p a g e s 2 7 8 xe2\\\\x80\\\\x93 2 8 2 1 9 9 5 1 7 t k h o t h e r a n d o m s u b s p a c e m e t h o d f o r c o n s t r u c t i n g d e c i s i o n f o r e s t s i e e e t r a n s p a t t e r n a n a l m a c h i n t e l l 2 0 8 8 3 2 xe2\\\\x80\\\\x93 8 4 4 1 9 9 8 1 8 y j i a e s h e l h a m e r j d o n a h u e s k a r a y e v j l o n g r g i r s h i c k s g u a d a r r a m a a n d t d a r r e l l c a f f e c o n v o l u t i o n a l a r c h i t e c t u r e f o r f a s t f e a t u r e e m b e d d i n g a r x i v p r e p r i n t a r x i v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 m i j o r d a n z g h a h r a m a n i t s j a a k k o l a a n d l k s a u l a n i n t r o d u c t i o n t o v a r i a t i o n a l m e t h o d s f o r g r a p h i c a l m o d e l s m a c h i n e l e a r n i n g 3 7 2 1 8 3 xe2\\\\x80\\\\x93 2 3 3 1 9 9 9 2 0 p k o n t s c h i e d e r m f i t e r a u a c r i m i n i s i a n d s r b u l xc3\\\\xb2 d e e p n e u r a l d e c i s i o n f o r e s t s i n p r o c i c c v p a g e s 1 4 6 7 xe2\\\\x80\\\\x93 1 4 7 5 2 0 1 5 2 1 a k r i z h e v s k y i s u t s k e v e r a n d g e h i n t o n i m a g e n e t c l a s s i f i c a t i o n w i t h d e e p c o n v o l u t i o n a l n e u r a l n e t w o r k s i n p r o c n i p s p a g e s 1 1 0 6 xe2\\\\x80\\\\x93 1 1 1 4 2 0 1 2 2 2 a l a n i t i s c d r a g a n o v a a n d c c h r i s t o d o u l o u c o m p a r i n g d i f f e r e n t c l a s s i f i e r s f o r a u t o m a t i c a g e e s t i m a t i o n i e e e t r a n s o n c y b e r n e t i c s 3 4 1 6 2 1 xe2\\\\x80\\\\x93 6 2 8 2 0 0 4 2 3 o m p a r k h i a v e d a l d i a n d a z i s s e r m a n d e e p f a c e r e c o g n i t i o n i n p r o c b m v c p a g e s 4 1 1 xe2\\\\x80\\\\x93 4 1 1 2 2 0 1 5 2 4 k r i c a n e k a n d t t e s a f a y e m o r p h a l o n g i t u d i n a l i m a g e d a t a b a s e o f n o r m a l a d u l t a g e p r o g r e s s i o n i n p r o c f g p a g e s 3 4 1 xe2\\\\x80\\\\x93 3 4 5 2 0 0 6 2 5 j s h o t t o n a w f i t z g i b b o n m c o o k t s h a r p m f i n o c c h i o r m o o r e a k i p m a n a n d a b l a k e r e a l t i m e h u m a n p o s e r e c o g n i t i o n i n p a r t s f r o m s i n g l e d e p t h i m a g e s i n p r o c c v p r p a g e s 1 2 9 7 xe2\\\\x80\\\\x93 1 3 0 4 2 0 1 1 2 6 g t s o u m a k a s a n d i k a t a k i s m u l t i l a b e l c l a s s i f i c a t i o n a n o v e r v i e w i n t e r n a t i o n a l j o u r n a l o f d a t a w a r e h o u s i n g a n d m i n i n g 3 3 1 xe2\\\\x80\\\\x93 1 3 2 0 0 7 2 7 c x i n g x g e n g a n d h x u e l o g i s t i c b o o s t i n g r e g r e s s i o n f o r l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o c c v p r p a g e s 4 4 8 9 xe2\\\\x80\\\\x93 4 4 9 7 2 0 1 6 2 8 x y a n g x g e n g a n d d z h o u s p a r s i t y c o n d i t i o n a l e n e r g y l a b e l d i s t r i b u t i o n l e a r n i n g f o r a g e e s t i m a t i o n i n p r o c i j c a i p a g e s 2 2 5 9 xe2\\\\x80\\\\x93 2 2 6 5 2 0 1 6 2 9 a l y u i l l e a n d a r a n g a r a j a n t h e c o n c a v e c o n v e x p r o c e d u r e n e u r a l c o m p u t a t i o n 1 5 4 9 1 5 xe2\\\\x80\\\\x93 9 3 6 2 0 0 3 3 0 y z h o u h x u e a n d x g e n g e m o t i o n d i s t r i b u t i o n r e c o g n i t i o n f r o m f a c i a l e x p r e s s i o n s i n p r o c m m p a g e s 1 2 4 7 xe2\\\\x80\\\\x93 1 2 5 0 2 0 1 5 1 0 x0c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.1\n",
      "XGBoost Accuracy on Test set -> 0.26\n",
      "RandomForest Accuracy on Test set -> 0.3\n",
      "DecisionTree Accuracy on Test set -> 0.26\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l r b u n l e r n n g f r e w e h e n 1 2 k z h 1 l u g u 1 l n u l l e 2 k e l b r r f p e c l f b e r p c n p c l c c e n e w r k h n g h n u e f r v n c e c u n c n n c e n c e c h l f c u n c n n n f r n e n g n e e r n g h n g h u n v e r 2 e p r e n f c p u e r c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c b r c l b e l r b u n l e r n n g l l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r h p p e r p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g h e e f f e c v e n e f h e p r p e l l f v e r f e n e v e r l l l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l l e h 1 n r u c n l b e l r b u n l e r n n g l l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g l l n u l l b e l l e r n n g l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l n h e r e x p l e v e r n g p r e c n 7 n f u v e r e v e w w e b e u c h n e f l x b n u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h n l l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n e h e r l l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l n f r n p r c e n g e n p 2 0 1 7 l n g b e c h c u x0c f g u r e 1 h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l l k h w v n g e n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l u r l l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l l e h h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 l l u r n f l b e l r b u n l e r n n g f r e h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b xce\\\\x98 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l w n e x f u n c n xcf\\\\x95 1 n xcf\\\\x95 2 r e g n e h e e w r e e r e p e c v e l h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e h e u p u f h e f r e x u r e f h e r e e p r e c n f xc2\\\\xb7 xce\\\\x98 n q r e l e r n e j n l n n e n e n n n e r 2 x0c 2 r e l e w r k n c e u r l l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e h e n w e c u c u r r e n l l e h e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r h e n e w l p r p e e e p n e u r l e c n f r e n f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n u r e h e x e n n f r e l l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e l h u g h e p z e f r e e u p e f u n c n w g v e n n n f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l l l n n h e r e g e u e n l l f n n f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n f n 3 l l r e e n l l f c n b e l e r n e j n l w h l e r e e n n f w e r e l e r n e l e r n v e l h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n e c 4 4 l l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e u u p w r n f 2 0 h e c n r b u n f l l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e l l w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e c e l l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f l l c l l e b f g l l b u n g q u n e w n p z n l l h e b v e l l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r n h e r w r e h e l l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l v r l l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l l k b v e w e g h e r e g r e r h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h l l l g b h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e l l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x0c r e h e l l k n n e r e n g p c b u h e e x n g u c h e h c l l e l l 5 l l f c u e n x u e n r p e l b e l l u r e h l l f e x e n f f e r e n b l e e c n r e e r e l l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l r b u n l e r n n g f r e f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l l p r b l e w h e r e f r e c h n p u p l e x xe2\\\\x88\\\\x88 x h e r e l b e l r b u n x 1 x 2 x c xe2\\\\x88\\\\x88 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c xe2\\\\x88\\\\x88 0 1 n c 1 x c 1 h e g l f h e l l p r b l e l e r n p p n g f u n c n g x xe2\\\\x86\\\\x92 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n xe2\\\\x88\\\\x88 n e f n e p l f u n c n n xc2\\\\xb7 xce\\\\x98 x xe2\\\\x86\\\\x92 0 1 p r e e r z e b xce\\\\x98 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e xe2\\\\x88\\\\x88 l h l r b u n q q 1 q 2 q c p c v e r e q c xe2\\\\x88\\\\x88 0 1 n c 1 q c 1 b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x xce\\\\x98 xcf\\\\x83 f xcf\\\\x95 n x xce\\\\x98 w h e r e xcf\\\\x83 xc2\\\\xb7 g f u n c n xcf\\\\x95 xc2\\\\xb7 n n e x f u n c n b r n g h e xcf\\\\x95 n h u p u f f u n c n f x xce\\\\x98 n c r r e p n e n c e w h p l n e n n f x xe2\\\\x86\\\\x92 r r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r xce\\\\x98 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e xce\\\\x98 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n xce\\\\x98 h e n e w r k p r e e r h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b xcf\\\\x95 xc2\\\\xb7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r xe2\\\\x80\\\\x9c f xe2\\\\x80\\\\x9d r e u e f r c n r u c n g r e e e e r n e r n l n e x p l e e n r e xcf\\\\x95 xc2\\\\xb7 h w n n f g 2 h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b l r p x xce\\\\x98 n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 xe2\\\\x88\\\\x92 n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 n xe2\\\\x88\\\\x88 n w h e r e 1 xc2\\\\xb7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n n l n n r r e p e c v e l h e u p u f h e r e e w r x e h e p p n g f u n c n g e f n e b x g x xce\\\\x98 p x xce\\\\x98 q 2 xe2\\\\x88\\\\x88 l 3 2 r e e p z n g v e n r n n g e x n 1 u r g l l e r n e c n r e e e c r b e n e c 3 1 w h c h c n u p u r b u n g x xce\\\\x98 l r f r e c h p l e x h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x xce\\\\x98 n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q xce\\\\x98 xe2\\\\x88\\\\x92 n c n c x10 x x11 1 x x c 1 x x c x l g g c x xce\\\\x98 xe2\\\\x88\\\\x92 x l g p x xce\\\\x98 q c 3 n 1 c 1 n 1 c 1 xe2\\\\x88\\\\x88 l 4 x0c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x xce\\\\x98 h e c h u p u u n f g x xce\\\\x98 l e r n n g h e r e e r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r xce\\\\x98 n 2 h e r b u n q h e l b h e l e f n e h e b e p r e e r xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 r e e e r n e b xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 r g n r q xce\\\\x98 4 xce\\\\x98 q l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e xce\\\\x98 h e n w e f x xce\\\\x98 n p z e q h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g p l n e n h e c n w e e c r b e h w l e r n h e p r e e r xce\\\\x98 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q xce\\\\x98 w r xce\\\\x98 b h e c h n r u l e n xe2\\\\x88\\\\x82 r q xce\\\\x98 x x xe2\\\\x88\\\\x82 r q xce\\\\x98 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 5 xe2\\\\x88\\\\x82 xce\\\\x98 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 n xe2\\\\x88\\\\x88 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f xcf\\\\x95 n h e f r e r g v e n b c x01 g c x xce\\\\x98 n l x11 g c x xce\\\\x98 n r 1 x c x10 xe2\\\\x88\\\\x82 r q xce\\\\x98 x n x xce\\\\x98 xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 n x xce\\\\x98 6 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x xce\\\\x98 n c 1 g c x xce\\\\x98 g c x xce\\\\x98 p p w h e r e g c x xce\\\\x98 n l xe2\\\\x88\\\\x88 l l n p x xce\\\\x98 q c n g c x xce\\\\x98 n r xe2\\\\x88\\\\x88 l r n p x xce\\\\x98 q c n e h l e n b e h e r e e r e h e n e n h e n w e h v e g c x xce\\\\x98 n g c x xce\\\\x98 n l g c x xce\\\\x98 n r h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r xce\\\\x98 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q xce\\\\x98 xe2\\\\x88\\\\x80 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r u p p e r b u n f r h e l f u n c n r q xce\\\\x98 c n b e b n e b j e n e n xe2\\\\x80\\\\x99 n e q u l r q xce\\\\x98 xe2\\\\x88\\\\x92 n c x10 x x11 1 x x c x l g p x xce\\\\x98 q c n 1 c 1 xe2\\\\x88\\\\x88 l xe2\\\\x89\\\\xa4 xe2\\\\x88\\\\x92 w h e r e xce\\\\xbe q c x 1 n n x c x 1 c 1 p x xce\\\\x98 q c g c x xce\\\\x98 xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x88\\\\x92 x c x xce\\\\xbe q xcc\\\\x84 c x l g xe2\\\\x88\\\\x88 l x10 p x xce\\\\x98 q x11 c xce\\\\xbe q xcc\\\\x84 c x 8 w e e f n e n c x10 p x xce\\\\x98 q x11 1 x x c x c x xce\\\\xbe q xcc\\\\x84 c x l g n 1 c 1 xce\\\\xbe q xcc\\\\x84 c x 9 xe2\\\\x88\\\\x88 l h e n xcf\\\\x86 q q xcc\\\\x84 n u p p e r b u n f r r q xce\\\\x98 w h c h h h e p r p e r h f r n q n q xcc\\\\x84 xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x89\\\\xa5 r q xce\\\\x98 n xcf\\\\x86 q q r q xce\\\\x98 u e h w e r e p n q c r r e p n n g h e h e r n h e n xcf\\\\x86 q q n u p p e r b u n f r r q xce\\\\x98 n h e n e x e r n q 1 c h e n u c h h xcf\\\\x86 q 1 q xe2\\\\x89\\\\xa4 r q xce\\\\x98 w h c h p l e r q 1 xce\\\\x98 xe2\\\\x89\\\\xa4 r q xce\\\\x98 5 x0c c n e q u e n l w e c n n z e xcf\\\\x86 q q xcc\\\\x84 n e f r q xce\\\\x98 f e r e n u r n g h r q xce\\\\x98 xcf\\\\x86 q q xcc\\\\x84 e q xcc\\\\x84 q w e h v e q 1 r g n xcf\\\\x86 q q xe2\\\\x88\\\\x80 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b xcf\\\\x95 q q xcf\\\\x86 q q x xce\\\\xbb xe2\\\\x88\\\\x88 l w h e r e xce\\\\xbb h e l g r n g e u l p l e r b e n g xce\\\\xbb 1 n e h q c 1 xe2\\\\x88\\\\x88 0 1 n r b u n h e l b h e l e f n e h e r n g 0 r b u n q c c 1 3 3 q c xe2\\\\x88\\\\x92 1 1 1 c 1 xe2\\\\x88\\\\x82 xcf\\\\x95 q q xe2\\\\x88\\\\x82 q c n c 1 x x c 1 xce\\\\xbe q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x xce\\\\xbe q c x p c 1 p n c xce\\\\xbe q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e f r e n e n e b l e f e c n r e e f 1 k n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r xce\\\\x98 f r f e u r e l e r n n g f u n c n f xc2\\\\xb7 xce\\\\x98 b u c r r e p n f f e r e n u p u u n f f g n e b xcf\\\\x95 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r k w h e r e r k h e l f u n c n f r r e e k e f n e b e q n 3 l e r n xce\\\\x98 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n e c 3 2 n r e f e r r n g f g 2 w e h v e n k xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x xce\\\\x98 1 x x x xe2\\\\x88\\\\x82 r f xe2\\\\x88\\\\x82 r k xe2\\\\x88\\\\x82 xce\\\\x98 k 1 xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 3 k 1 n xe2\\\\x88\\\\x88 n k w h e r e n k n xcf\\\\x95 k xc2\\\\xb7 r e h e p l n e e n h e n e x f u n c n f k r e p e c v e l n e h h e n e x f u n c n xcf\\\\x95 k xc2\\\\xb7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b xce\\\\x98 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e b u n e f n b c h e b h e r n n g p r c e u r e f l l f h w n n l g r h 1 l g r h 1 h e r n n g p r c e u r e f l l f r e q u r e r n n g e n b h e n u b e r f n b c h e u p e q n l z e xce\\\\x98 r n l n q u n f r l e b xe2\\\\x88\\\\x85 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r u p e xce\\\\x98 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b xe2\\\\x88\\\\x85 e n w h l e n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x xce\\\\x98 f k k 1 g x xce\\\\x98 k 6 x0c 4 e x p e r e n l r e u l u r r e l z n f l l f b e n xe2\\\\x80\\\\x9c c f f e xe2\\\\x80\\\\x9d 1 8 u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l l f r n e g r e w h n e e p n e w r k l l f w e e v l u e l l f n f f e r e n l l k n c p r e w h h e r n l n e l l e h l l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l l f n c p u e r v n p p l c n e f c l g e e n h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l l f n l n e l l e h w e c p r e u r h l l w e l l l f w h h e r e f h e r n l n e l l e h f r l l f h e f e u r e l e r n n g f u n c n f x xce\\\\x98 l n e r r n f r n f x e h e h u p u u n f x xce\\\\xb8 xce\\\\xb8 x w h e r e xce\\\\xb8 h e h c l u n f h e r n f r n r x xce\\\\x98 w e u e 3 p p u l r l l e n 6 v e h u n g e n e n n u r l c e n e 1 h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n xcf\\\\x86 r e n e n q u r e xcf\\\\x87 2 n w l r e u r e f e l n e r e c n w e e v l u e u r h l l w e l l l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l l e h h e r e u l f l l f n h e c p e r r e u r z e n b l e 1 f r v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b xe2\\\\x80\\\\x9c e n xc2\\\\xb1 n r e v n xe2\\\\x80\\\\x9d n e r l e h w r n n g n e n g g e v e c n b e e e n f r b l e 1 l l f p e r f r b e n l l f h e x e u r e b l e 1 c p r n r e u l n h r e e l l e 6 xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x91 xe2\\\\x80\\\\x9d n xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x93 xe2\\\\x80\\\\x9d n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l e e h k l xe2\\\\x86\\\\x93 e u c l e n xe2\\\\x86\\\\x93 xcf\\\\x86 r e n e n xe2\\\\x86\\\\x93 q u r e xcf\\\\x87 2 xe2\\\\x86\\\\x93 f e l xe2\\\\x86\\\\x91 n e r e c n xe2\\\\x86\\\\x91 v e l l f u r l l g b 2 7 l l g b 2 7 l v r 7 b f g l l 6 l l 1 1 0 0 7 3 xc2\\\\xb1 0 0 0 5 0 0 8 6 xc2\\\\xb1 0 0 0 4 0 0 9 0 xc2\\\\xb1 0 0 0 4 0 0 9 2 xc2\\\\xb1 0 0 0 5 0 0 9 9 xc2\\\\xb1 0 0 0 4 0 1 2 9 xc2\\\\xb1 0 0 0 7 0 1 3 3 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 9 xc2\\\\xb1 0 0 0 3 0 1 5 8 xc2\\\\xb1 0 0 0 4 0 1 6 7 xc2\\\\xb1 0 0 0 4 0 1 8 7 xc2\\\\xb1 0 0 0 4 0 1 3 0 xc2\\\\xb1 0 0 0 3 0 1 5 2 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 6 xc2\\\\xb1 0 0 0 4 0 1 6 4 xc2\\\\xb1 0 0 0 3 0 1 8 3 xc2\\\\xb1 0 0 0 4 0 0 7 0 xc2\\\\xb1 0 0 0 4 0 0 8 4 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 4 0 0 9 6 xc2\\\\xb1 0 0 0 4 0 1 2 0 xc2\\\\xb1 0 0 0 5 0 9 8 1 xc2\\\\xb1 0 0 0 1 0 9 7 8 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 4 xc2\\\\xb1 0 0 0 1 0 9 6 7 xc2\\\\xb1 0 0 0 1 0 8 7 0 xc2\\\\xb1 0 0 0 3 0 8 4 8 xc2\\\\xb1 0 0 0 3 0 8 4 5 xc2\\\\xb1 0 0 0 3 0 8 4 4 xc2\\\\xb1 0 0 0 4 0 8 3 6 xc2\\\\xb1 0 0 0 3 0 8 1 7 xc2\\\\xb1 0 0 0 4 l l f u r l v r 7 b f g l l 6 l l 1 1 0 2 2 8 xc2\\\\xb1 0 0 0 6 0 2 4 5 xc2\\\\xb1 0 0 1 9 0 2 3 1 xc2\\\\xb1 0 0 2 1 0 2 3 9 xc2\\\\xb1 0 0 1 8 0 0 8 5 xc2\\\\xb1 0 0 0 2 0 0 9 9 xc2\\\\xb1 0 0 0 5 0 0 7 6 xc2\\\\xb1 0 0 0 6 0 0 8 9 xc2\\\\xb1 0 0 0 6 0 2 1 2 xc2\\\\xb1 0 0 0 2 0 2 2 9 xc2\\\\xb1 0 0 1 5 0 2 3 1 xc2\\\\xb1 0 0 1 2 0 2 5 3 xc2\\\\xb1 0 0 0 9 0 1 7 9 xc2\\\\xb1 0 0 0 4 0 1 8 9 xc2\\\\xb1 0 0 2 1 0 2 1 1 xc2\\\\xb1 0 0 1 8 0 2 0 5 xc2\\\\xb1 0 0 1 2 0 9 4 8 xc2\\\\xb1 0 0 0 1 0 9 4 0 xc2\\\\xb1 0 0 0 6 0 9 3 8 xc2\\\\xb1 0 0 0 8 0 9 4 4 xc2\\\\xb1 0 0 0 3 0 7 8 8 xc2\\\\xb1 0 0 0 2 0 7 7 1 xc2\\\\xb1 0 0 1 5 0 7 6 9 xc2\\\\xb1 0 0 1 2 0 7 4 7 xc2\\\\xb1 0 0 0 9 l l f u r l v r 7 b f g l l 6 l l 1 1 0 5 3 4 xc2\\\\xb1 0 0 1 3 0 8 5 2 xc2\\\\xb1 0 0 2 3 0 8 5 6 xc2\\\\xb1 0 0 6 1 0 8 7 9 xc2\\\\xb1 0 0 2 3 0 3 1 7 xc2\\\\xb1 0 0 1 4 0 5 1 1 xc2\\\\xb1 0 0 2 1 0 4 7 5 xc2\\\\xb1 0 0 2 9 0 4 5 8 xc2\\\\xb1 0 0 1 4 0 3 3 6 xc2\\\\xb1 0 0 1 0 0 4 9 2 xc2\\\\xb1 0 0 1 6 0 5 0 8 xc2\\\\xb1 0 0 2 6 0 5 3 9 xc2\\\\xb1 0 0 1 1 0 4 4 8 xc2\\\\xb1 0 0 1 7 0 5 9 5 xc2\\\\xb1 0 0 2 6 0 7 1 6 xc2\\\\xb1 0 0 4 1 0 7 9 2 xc2\\\\xb1 0 0 1 9 0 8 2 4 xc2\\\\xb1 0 0 0 8 0 8 1 3 xc2\\\\xb1 0 0 0 8 0 7 2 2 xc2\\\\xb1 0 0 2 1 0 6 8 6 xc2\\\\xb1 0 0 0 9 0 6 6 4 xc2\\\\xb1 0 0 1 0 0 5 0 9 xc2\\\\xb1 0 0 1 6 0 4 9 2 xc2\\\\xb1 0 0 2 6 0 4 6 1 xc2\\\\xb1 0 0 1 1 h u n g e n e n u r l c e n e 4 2 e v l u n f l l f n f c l g e e n n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l l p r b l e w e c n u c f c l g e e n e x p e r e n n r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l l n e x h 7 x0c l b e l r b u n h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r e b e w e e n p r e c e g e n c h r n l g c l g e h e c u r r e n e f h e r r e u l n r p h b n b f n e u n n g l l 5 n v g g f c e 2 3 w e l b u l l l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n b l e 2 w h c h h w l l f c h e v e h e e f h e r p e r f r n c e n r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l l e l l l n l l f n n n e e p l l e l l l c p n n b f g l l n h e u p e r r f l l f c p r e w h l l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l l r e p e c v e l b l e 2 e f g e e n c p r n n r p h 2 4 e h l l 1 1 c p n n 1 1 b f g l l 6 l l v g g f c e 5 l l f v g g f c e u r e 5 6 7 xc2\\\\xb1 0 1 5 4 8 7 xc2\\\\xb1 0 3 1 3 9 4 xc2\\\\xb1 0 0 5 2 4 2 xc2\\\\xb1 0 0 1 2 2 4 xc2\\\\xb1 0 0 2 h e r b u n f g e n e r n e h n c v e r u n b l n c e n r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f r p h c l l e r p h u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n h e b e p e r f r n c e r e p r e n r p h u b g v e n b 2 l l 1 5 e p e n e n l l e h 2 l l u e h e u p u f h e xe2\\\\x80\\\\x9c f c 7 xe2\\\\x80\\\\x9d l e r n l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l l f w h l e x n e f l l w n g h e e x p e r e n e n g u e n 2 l l w e e v l u e u r l l f n h e c p e r n c l u n g b h l l n l l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 l l f h e c p e r r e r n e n h e e e e p f e u r e u e b 2 l l c n b e e e n f r b l e 3 u r l l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 e f g e e n c p r n n b u n r e u n l r b u n r p h u b n h e l b e l r b u n u e n r n n g e r e h e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 h e p r p e e h l l f c h e v e 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 l l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l 2 l l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 p l n e n 2 h xe2\\\\x88\\\\x92 1 l e f n e l e 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e 1 xc3\\\\x97 c xc3\\\\x97 c n 1 xc3\\\\x97 c xc3\\\\x97 c xc3\\\\x97 c r e p e c v e l f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 b h e c p l e x f n e r n u p e l e f n e r e n b xc3\\\\x97 b xc3\\\\x97 k xc3\\\\x97 c xc3\\\\x97 1 xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 b h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 b n xc3\\\\x97 c xc3\\\\x97 k r e p e c v e l l l f r e e f f c e n n r p h u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n v e e u r e b k l n g e e n n r p h u b w h 6 0 r n n g e r e u r e b e f r f f e r e n p r e e r e n g n h e c n r e e n u b e r f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n e c 2 h e e n e b l e r e g l e r n f r e p r p e n n f 2 0 f f e r e n f r u r h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e w r h e n w e r e p l c e u r e n e b l e r e g n l l f b h e n e u e n n f n n e h e h n f l l h e c r r e p n n g h l l w e l n e b n f l l w e f x h e r p r e e r e r e e e p h n 8 x0c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e b e r v e f r f g 4 h e p e r f r n c e f l l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n n v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e r e e e p h r e e e p h n h e r p r n p r e e r f r e c n r e e n l l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n xcf\\\\x84 xcf\\\\x84 xe2\\\\x89\\\\xa5 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l l f w e e xcf\\\\x84 2 h xe2\\\\x88\\\\x92 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l u n g u c h l r g e r e p h l e b p e r f r n c e n v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 h e p e r f r n c e c h n g e f g e e n n r p h u b n r n g p r e c n n v e b v r n g r e e n u b e r n b r e e e p h u r p p r c h l l f l l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n f n f l l n f l l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n c k n w l e g e e n h w r k w u p p r e n p r b h e n n l n u r l c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b xe2\\\\x80\\\\x9c c h e n g u n g xe2\\\\x80\\\\x9d p r j e c u p p r e b h n g h u n c p l e u c n c n n h n g h e u c n e v e l p e n f u n n n 1 5 c g 4 3 n n p r b n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 n g e n h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 xe2\\\\x80\\\\x93 1 5 8 8 1 9 9 7 2 l b e r g e r p e r n v j p e r x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 xe2\\\\x80\\\\x93 7 1 1 9 9 6 3 l b r e n r n f r e c h n e l e r n n g 4 5 1 5 xe2\\\\x80\\\\x93 3 2 2 0 0 1 4 c r n n j h n e c n f r e f r c p u e r v n n e c l g e n l p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g e e e r n k n w l e n g 2 8 7 1 7 3 4 xe2\\\\x80\\\\x93 1 7 4 8 2 0 1 6 9 x0c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g n p r j c p g e 3 5 1 1 xe2\\\\x80\\\\x93 3 5 1 7 2 0 1 5 8 x g e n g k h l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n n p r c 2 0 1 0 9 x g e n g q w n g n x f c l g e e n b p v e l b e l r b u n l e r n n g n p r c c p r p g e 4 4 6 5 xe2\\\\x80\\\\x93 4 4 7 0 2 0 1 4 1 0 x g e n g n x h e p e e n b e n u l v r e l b e l r b u n n p r c c v p r p g e 1 8 3 7 xe2\\\\x80\\\\x93 1 8 4 2 2 0 1 4 1 1 x g e n g c n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n e e e r n p e r n n l c h n e l l 3 5 1 0 2 4 0 1 xe2\\\\x80\\\\x93 2 4 1 2 2 0 1 3 1 2 g g u f u c r e r n h u n g g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n e e e r n g e p r c e n g 1 7 7 1 1 7 8 xe2\\\\x80\\\\x93 1 1 8 8 2 0 0 8 1 3 g g u n g u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r n c v p r w r k h p p g e 7 1 xe2\\\\x80\\\\x93 7 8 2 0 1 0 1 4 g g u n c z h n g u n c r p p u l n g e e n n p r c c v p r p g e 4 2 5 7 xe2\\\\x80\\\\x93 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g z h n g h n g n z h u n g e p e n e n l b e l r b u n l e r n n g f r g e e n e e e r n n g e p r c e n g 2 0 1 7 1 6 k h r n e c n f r e n p r c c r p g e 2 7 8 xe2\\\\x80\\\\x93 2 8 2 1 9 9 5 1 7 k h h e r n u b p c e e h f r c n r u c n g e c n f r e e e e r n p e r n n l c h n e l l 2 0 8 8 3 2 xe2\\\\x80\\\\x93 8 4 4 1 9 9 8 1 8 j e h e l h e r j n h u e k r e v j l n g r g r h c k g u r r n r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 j r n z g h h r n j k k l n l k u l n n r u c n v r n l e h f r g r p h c l e l c h n e l e r n n g 3 7 2 1 8 3 xe2\\\\x80\\\\x93 2 3 3 1 9 9 9 2 0 p k n c h e e r f e r u c r n n r b u l xc3\\\\xb2 e e p n e u r l e c n f r e n p r c c c v p g e 1 4 6 7 xe2\\\\x80\\\\x93 1 4 7 5 2 0 1 5 2 1 k r z h e v k u k e v e r n g e h n n g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k n p r c n p p g e 1 1 0 6 xe2\\\\x80\\\\x93 1 1 1 4 2 0 1 2 2 2 l n c r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n e e e r n n c b e r n e c 3 4 1 6 2 1 xe2\\\\x80\\\\x93 6 2 8 2 0 0 4 2 3 p r k h v e l n z e r n e e p f c e r e c g n n n p r c b v c p g e 4 1 1 xe2\\\\x80\\\\x93 4 1 1 2 2 0 1 5 2 4 k r c n e k n e f e r p h l n g u n l g e b e f n r l u l g e p r g r e n n p r c f g p g e 3 4 1 xe2\\\\x80\\\\x93 3 4 5 2 0 0 6 2 5 j h n w f z g b b n c k h r p f n c c h r r e k p n n b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e n p r c c v p r p g e 1 2 9 7 xe2\\\\x80\\\\x93 1 3 0 4 2 0 1 1 2 6 g u k n k k u l l b e l c l f c n n v e r v e w n e r n n l j u r n l f w r e h u n g n n n g 3 3 1 xe2\\\\x80\\\\x93 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g n p r c c v p r p g e 4 4 8 9 xe2\\\\x80\\\\x93 4 4 9 7 2 0 1 6 2 8 x n g x g e n g n z h u p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n n p r c j c p g e 2 2 5 9 xe2\\\\x80\\\\x93 2 2 6 5 2 0 1 6 2 9 l u l l e n r n g r j n h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 xe2\\\\x80\\\\x93 9 3 6 2 0 0 3 3 0 z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n n p r c p g e 1 2 4 7 xe2\\\\x80\\\\x93 1 2 5 0 2 0 1 5 1 0 x0c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.28\n",
      "RandomForest Accuracy on Test set -> 0.26\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s w e i s h e n 1 2 k a i z h a o 1 y i l u g u o 1 a l a n y u i l l e 2 k e y l a b o r a t o r y o f s p e c i a l t y f i b e r o p t i c s a n d o p t i c a l a c c e s s n e t w o r k s s h a n g h a i i n s t i t u t e f o r a d v a n c e d c o m m u n i c a t i o n a n d d a t a s c i e n c e s c h o o l o f c o m m u n i c a t i o n a n d i n f o r m a t i o n e n g i n e e r i n g s h a n g h a i u n i v e r s i t y 2 d e p a r t m e n t o f c o m p u t e r s c i e n c e j o h n s h o p k i n s u n i v e r s i t y a r x i v 1 7 0 2 0 6 0 8 6 v 4 c s l g 1 6 o c t 2 0 1 7 1 s h e n w e i 1 2 3 1 z h a o k 1 2 0 6 g y l l u a n 0 a l a n l y u i l l e g m a i l c o m a b s t r a c t l a b e l d i s t r i b u t i o n l e a r n i n g l d l i s a g e n e r a l l e a r n i n g f r a m e w o r k w h i c h a s s i g n s t o a n i n s t a n c e a d i s t r i b u t i o n o v e r a s e t o f l a b e l s r a t h e r t h a n a s i n g l e l a b e l o r m u l t i p l e l a b e l s c u r r e n t l d l m e t h o d s h a v e e i t h e r r e s t r i c t e d a s s u m p t i o n s o n t h e e x p r e s s i o n f o r m o f t h e l a b e l d i s t r i b u t i o n o r l i m i t a t i o n s i n r e p r e s e n t a t i o n l e a r n i n g e g t o l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r t h i s p a p e r p r e s e n t s l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s l d l f s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m b a s e d o n d i f f e r e n t i a b l e d e c i s i o n t r e e s w h i c h h a v e s e v e r a l a d v a n t a g e s 1 d e c i s i o n t r e e s h a v e t h e p o t e n t i a l t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s b y a m i x t u r e o f l e a f n o d e p r e d i c t i o n s 2 t h e l e a r n i n g o f d i f f e r e n t i a b l e d e c i s i o n t r e e s c a n b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g w e d e f i n e a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r a f o r e s t e n a b l i n g a l l t h e t r e e s t o b e l e a r n e d j o i n t l y a n d s h o w t h a t a n u p d a t e f u n c t i o n f o r l e a f n o d e p r e d i c t i o n s w h i c h g u a r a n t e e s a s t r i c t d e c r e a s e o f t h e l o s s f u n c t i o n c a n b e d e r i v e d b y v a r i a t i o n a l b o u n d i n g t h e e f f e c t i v e n e s s o f t h e p r o p o s e d l d l f s i s v e r i f i e d o n s e v e r a l l d l t a s k s a n d a c o m p u t e r v i s i o n a p p l i c a t i o n s h o w i n g s i g n i f i c a n t i m p r o v e m e n t s t o t h e s t a t e o f t h e a r t l d l m e t h o d s 1 i n t r o d u c t i o n l a b e l d i s t r i b u t i o n l e a r n i n g l d l 6 1 1 i s a l e a r n i n g f r a m e w o r k t o d e a l w i t h p r o b l e m s o f l a b e l a m b i g u i t y u n l i k e s i n g l e l a b e l l e a r n i n g s l l a n d m u l t i l a b e l l e a r n i n g m l l 2 6 w h i c h a s s u m e a n i n s t a n c e i s a s s i g n e d t o a s i n g l e l a b e l o r m u l t i p l e l a b e l s l d l a i m s a t l e a r n i n g t h e r e l a t i v e i m p o r t a n c e o f e a c h l a b e l i n v o l v e d i n t h e d e s c r i p t i o n o f a n i n s t a n c e i e a d i s t r i b u t i o n o v e r t h e s e t o f l a b e l s s u c h a l e a r n i n g s t r a t e g y i s s u i t a b l e f o r m a n y r e a l w o r l d p r o b l e m s w h i c h h a v e l a b e l a m b i g u i t y a n e x a m p l e i s f a c i a l a g e e s t i m a t i o n 8 e v e n h u m a n s c a n n o t p r e d i c t t h e p r e c i s e a g e f r o m a s i n g l e f a c i a l i m a g e t h e y m a y s a y t h a t t h e p e r s o n i s p r o b a b l y i n o n e a g e g r o u p a n d l e s s l i k e l y t o b e i n a n o t h e r h e n c e i t i s m o r e n a t u r a l t o a s s i g n a d i s t r i b u t i o n o f a g e l a b e l s t o e a c h f a c i a l i m a g e f i g 1 a i n s t e a d o f u s i n g a s i n g l e a g e l a b e l a n o t h e r e x a m p l e i s m o v i e r a t i n g p r e d i c t i o n 7 m a n y f a m o u s m o v i e r e v i e w w e b s i t e s s u c h a s n e t f l i x i m d b a n d d o u b a n p r o v i d e a c r o w d o p i n i o n f o r e a c h m o v i e s p e c i f i e d b y t h e d i s t r i b u t i o n o f r a t i n g s c o l l e c t e d f r o m t h e i r u s e r s f i g 1 b i f a s y s t e m c o u l d p r e c i s e l y p r e d i c t s u c h a r a t i n g d i s t r i b u t i o n f o r e v e r y m o v i e b e f o r e i t i s r e l e a s e d m o v i e p r o d u c e r s c a n r e d u c e t h e i r i n v e s t m e n t r i s k a n d t h e a u d i e n c e c a n b e t t e r c h o o s e w h i c h m o v i e s t o w a t c h m a n y l d l m e t h o d s a s s u m e t h e l a b e l d i s t r i b u t i o n c a n b e r e p r e s e n t e d b y a m a x i m u m e n t r o p y m o d e l 2 a n d l e a r n i t b y o p t i m i z i n g a n e n e r g y f u n c t i o n b a s e d o n t h e m o d e l 8 1 1 2 8 6 b u t t h e e x p o n e n t i a l p a r t o f t h i s m o d e l r e s t r i c t s t h e g e n e r a l i t y o f t h e d i s t r i b u t i o n f o r m e g i t h a s d i f f i c u l t y i n r e p r e s e n t i n g m i x t u r e d i s t r i b u t i o n s s o m e o t h e r l d l m e t h o d s e x t e n d t h e e x i s t i n g l e a r n i n g a l g o r i t h m s e g b y b o o s t i n g a n d s u p p o r t v e c t o r r e g r e s s i o n t o d e a l w i t h l a b e l d i s t r i b u t i o n s 7 2 7 w h i c h a v o i d m a k i n g t h i s a s s u m p t i o n b u t h a v e l i m i t a t i o n s i n r e p r e s e n t a t i o n l e a r n i n g e g t h e y d o n o t l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r 3 1 s t c o n f e r e n c e o n n e u r a l i n f o r m a t i o n p r o c e s s i n g s y s t e m s n i p s 2 0 1 7 l o n g b e a c h c a u s a x0c f i g u r e 1 t h e r e a l w o r l d d a t a w h i c h a r e s u i t a b l e t o b e m o d e l e d b y l a b e l d i s t r i b u t i o n l e a r n i n g a e s t i m a t e d f a c i a l a g e s a u n i m o d a l d i s t r i b u t i o n b r a t i n g d i s t r i b u t i o n o f c r o w d o p i n i o n o n a m o v i e a m u l t i m o d a l d i s t r i b u t i o n i n t h i s p a p e r w e p r e s e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s l d l f s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s 2 0 e x t e n d i n g d i f f e r e n t i a b l e d e c i s i o n t r e e s t o d e a l w i t h t h e l d l t a s k h a s t w o a d v a n t a g e s o n e i s t h a t d e c i s i o n t r e e s h a v e t h e p o t e n t i a l t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s b y m i x t u r e o f t h e l e a f n o d e p r e d i c t i o n s w h i c h a v o i d m a k i n g s t r o n g a s s u m p t i o n o n t h e f o r m o f t h e l a b e l d i s t r i b u t i o n s t h e s e c o n d i s t h a t t h e s p l i t n o d e p a r a m e t e r s i n d i f f e r e n t i a b l e d e c i s i o n t r e e s c a n b e l e a r n e d b y b a c k p r o p a g a t i o n w h i c h e n a b l e s a c o m b i n a t i o n o f t r e e l e a r n i n g a n d r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r w e d e f i n e a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r a t r e e b y t h e k u l l b a c k l e i b l e r d i v e r g e n c e k l b e t w e e n t h e g r o u n d t r u t h l a b e l d i s t r i b u t i o n a n d t h e d i s t r i b u t i o n p r e d i c t e d b y t h e t r e e b y f i x i n g s p l i t n o d e s w e s h o w t h a t t h e o p t i m i z a t i o n o f l e a f n o d e p r e d i c t i o n s t o m i n i m i z e t h e l o s s f u n c t i o n o f t h e t r e e c a n b e a d d r e s s e d b y v a r i a t i o n a l b o u n d i n g 1 9 2 9 i n w h i c h t h e o r i g i n a l l o s s f u n c t i o n t o b e m i n i m i z e d g e t s i t e r a t i v e l y r e p l a c e d b y a d e c r e a s i n g s e q u e n c e o f u p p e r b o u n d s f o l l o w i n g t h i s o p t i m i z a t i o n s t r a t e g y w e d e r i v e a d i s c r e t e i t e r a t i v e f u n c t i o n t o u p d a t e t h e l e a f n o d e p r e d i c t i o n s t o l e a r n a f o r e s t w e a v e r a g e t h e l o s s e s o f a l l t h e i n d i v i d u a l t r e e s t o b e t h e l o s s f o r t h e f o r e s t a n d a l l o w t h e s p l i t n o d e s f r o m d i f f e r e n t t r e e s t o b e c o n n e c t e d t o t h e s a m e o u t p u t u n i t o f t h e f e a t u r e l e a r n i n g f u n c t i o n i n t h i s w a y t h e s p l i t n o d e p a r a m e t e r s o f a l l t h e i n d i v i d u a l t r e e s c a n b e l e a r n e d j o i n t l y o u r l d l f s c a n b e u s e d a s a s h a l l o w s t a n d a l o n e m o d e l a n d c a n a l s o b e i n t e g r a t e d w i t h a n y d e e p n e t w o r k s i e t h e f e a t u r e l e a r n i n g f u n c t i o n c a n b e a l i n e a r t r a n s f o r m a t i o n a n d a d e e p n e t w o r k r e s p e c t i v e l y f i g 2 i l l u s t r a t e s a s k e t c h c h a r t o f o u r l d l f s w h e r e a f o r e s t c o n s i s t s o f t w o t r e e s i s s h o w n w e v e r i f y t h e e f f e c t i v e n e s s o f o u r m o d e l o n s e v e r a l l d l t a s k s s u c h a s c r o w d o p i n i o n p r e d i c t i o n o n m o v i e s a n d d i s e a s e p r e d i c t i o n b a s e d o n h u m a n g e n e s a s w e l l a s o n e c o m p u t e r v i s i o n a p p l i c a t i o n i e f a c i a l a g e e s t i m a t i o n s h o w i n g s i g n i f i c a n t i m p r o v e m e n t s t o t h e s t a t e o f t h e a r t l d l m e t h o d s t h e l a b e l d i s t r i b u t i o n s f o r t h e s e t a s k s i n c l u d e b o t h u n i m o d a l d i s t r i b u t i o n s e g t h e a g e d i s t r i b u t i o n i n f i g 1 a a n d m i x t u r e d i s t r i b u t i o n s t h e r a t i n g d i s t r i b u t i o n o n a m o v i e i n f i g 1 b t h e s u p e r i o r i t y o f o u r m o d e l o n b o t h o f t h e m v e r i f i e s i t s a b i l i t y t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s f i g u r e 2 i l l u s t r a t i o n o f a l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t t h e t o p c i r c l e s d e n o t e t h e o u t p u t u n i t s o f t h e f u n c t i o n f p a r a m e t e r i z e d b y xce\\\\x98 w h i c h c a n b e a f e a t u r e v e c t o r o r a f u l l y c o n n e c t e d l a y e r o f a d e e p n e t w o r k t h e b l u e a n d g r e e n c i r c l e s a r e s p l i t n o d e s a n d l e a f n o d e s r e s p e c t i v e l y t w o i n d e x f u n c t i o n xcf\\\\x95 1 a n d xcf\\\\x95 2 a r e a s s i g n e d t o t h e s e t w o t r e e s r e s p e c t i v e l y t h e b l a c k d a s h a r r o w s i n d i c a t e t h e c o r r e s p o n d e n c e b e t w e e n t h e s p l i t n o d e s o f t h e s e t w o t r e e s a n d t h e o u t p u t u n i t s o f f u n c t i o n f n o t e t h a t o n e o u t p u t u n i t m a y c o r r e s p o n d t o t h e s p l i t n o d e s b e l o n g i n g t o d i f f e r e n t t r e e s e a c h t r e e h a s i n d e p e n d e n t l e a f n o d e p r e d i c t i o n s q d e n o t e d b y h i s t o g r a m s i n l e a f n o d e s t h e o u t p u t o f t h e f o r e s t i s a m i x t u r e o f t h e t r e e p r e d i c t i o n s f xc2\\\\xb7 xce\\\\x98 a n d q a r e l e a r n e d j o i n t l y i n a n e n d t o e n d m a n n e r 2 x0c 2 r e l a t e d w o r k s i n c e o u r l d l a l g o r i t h m i s i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s i t i s n e c e s s a r y t o f i r s t r e v i e w s o m e t y p i c a l t e c h n i q u e s o f d e c i s i o n t r e e s t h e n w e d i s c u s s c u r r e n t l d l m e t h o d s d e c i s i o n t r e e s r a n d o m f o r e s t s o r r a n d o m i z e d d e c i s i o n t r e e s 1 6 1 3 4 a r e a p o p u l a r e n s e m b l e p r e d i c t i v e m o d e l s u i t a b l e f o r m a n y m a c h i n e l e a r n i n g t a s k s i n t h e p a s t l e a r n i n g o f a d e c i s i o n t r e e w a s b a s e d o n h e u r i s t i c s s u c h a s a g r e e d y a l g o r i t h m w h e r e l o c a l l y o p t i m a l h a r d d e c i s i o n s a r e m a d e a t e a c h s p l i t n o d e 1 a n d t h u s c a n n o t b e i n t e g r a t e d i n t o i n a d e e p l e a r n i n g f r a m e w o r k i e b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r t h e n e w l y p r o p o s e d d e e p n e u r a l d e c i s i o n f o r e s t s d n d f s 2 0 o v e r c o m e s t h i s p r o b l e m b y i n t r o d u c i n g a s o f t d i f f e r e n t i a b l e d e c i s i o n f u n c t i o n a t t h e s p l i t n o d e s a n d a g l o b a l l o s s f u n c t i o n d e f i n e d o n a t r e e t h i s e n s u r e s t h a t t h e s p l i t n o d e p a r a m e t e r s c a n b e l e a r n e d b y b a c k p r o p a g a t i o n a n d l e a f n o d e p r e d i c t i o n s c a n b e u p d a t e d b y a d i s c r e t e i t e r a t i v e f u n c t i o n o u r m e t h o d e x t e n d s d n d f s t o a d d r e s s l d l p r o b l e m s b u t t h i s e x t e n s i o n i s n o n t r i v i a l b e c a u s e l e a r n i n g l e a f n o d e p r e d i c t i o n s i s a c o n s t r a i n e d c o n v e x o p t i m i z a t i o n p r o b l e m a l t h o u g h a s t e p s i z e f r e e u p d a t e f u n c t i o n w a s g i v e n i n d n d f s t o u p d a t e l e a f n o d e p r e d i c t i o n s i t w a s o n l y p r o v e d t o c o n v e r g e f o r a c l a s s i f i c a t i o n l o s s c o n s e q u e n t l y i t w a s u n c l e a r h o w t o o b t a i n s u c h a n u p d a t e f u n c t i o n f o r o t h e r l o s s e s w e o b s e r v e d h o w e v e r t h a t t h e u p d a t e f u n c t i o n i n d n d f s c a n b e d e r i v e d f r o m v a r i a t i o n a l b o u n d i n g w h i c h a l l o w s u s t o e x t e n d i t t o o u r l d l l o s s i n a d d i t i o n t h e s t r a t e g i e s u s e d i n l d l f s a n d d n d f s t o l e a r n i n g t h e e n s e m b l e o f m u l t i p l e t r e e s f o r e s t s a r e d i f f e r e n t 1 w e e x p l i c i t l y d e f i n e a l o s s f u n c t i o n f o r f o r e s t s w h i l e o n l y t h e l o s s f u n c t i o n f o r a s i n g l e t r e e w a s d e f i n e d i n d n d f s 2 w e a l l o w t h e s p l i t n o d e s f r o m d i f f e r e n t t r e e s t o b e c o n n e c t e d t o t h e s a m e o u t p u t u n i t o f t h e f e a t u r e l e a r n i n g f u n c t i o n w h i l e d n d f s d i d n o t 3 a l l t r e e s i n l d l f s c a n b e l e a r n e d j o i n t l y w h i l e t r e e s i n d n d f s w e r e l e a r n e d a l t e r n a t i v e l y t h e s e c h a n g e s i n t h e e n s e m b l e l e a r n i n g a r e i m p o r t a n t b e c a u s e a s s h o w n i n o u r e x p e r i m e n t s s e c 4 4 l d l f s c a n g e t b e t t e r r e s u l t s b y u s i n g m o r e t r e e s b u t b y u s i n g t h e e n s e m b l e s t r a t e g y p r o p o s e d i n d n d f s t h e r e s u l t s o f f o r e s t s a r e e v e n w o r s e t h a n t h o s e f o r a s i n g l e t r e e t o s u m u p w r t d n d f s 2 0 t h e c o n t r i b u t i o n s o f l d l f s a r e f i r s t w e e x t e n d f r o m c l a s s i f i c a t i o n 2 0 t o d i s t r i b u t i o n l e a r n i n g b y p r o p o s i n g a d i s t r i b u t i o n b a s e d l o s s f o r t h e f o r e s t s a n d d e r i v e t h e g r a d i e n t t o l e a r n s p l i t s n o d e s w r t t h i s l o s s s e c o n d w e d e r i v e d t h e u p d a t e f u n c t i o n f o r l e a f n o d e s b y v a r i a t i o n a l b o u n d i n g h a v i n g o b s e r v e d t h a t t h e u p d a t e f u n c t i o n i n 2 0 w a s a s p e c i a l c a s e o f v a r i a t i o n a l b o u n d i n g l a s t b u t n o t t h e l e a s t w e p r o p o s e a b o v e t h r e e s t r a t e g i e s t o l e a r n i n g t h e e n s e m b l e o f m u l t i p l e t r e e s w h i c h a r e d i f f e r e n t f r o m 2 0 b u t w e s h o w a r e e f f e c t i v e l a b e l d i s t r i b u t i o n l e a r n i n g a n u m b e r o f s p e c i a l i z e d a l g o r i t h m s h a v e b e e n p r o p o s e d t o a d d r e s s t h e l d l t a s k a n d h a v e s h o w n t h e i r e f f e c t i v e n e s s i n m a n y c o m p u t e r v i s i o n a p p l i c a t i o n s s u c h a s f a c i a l a g e e s t i m a t i o n 8 1 1 2 8 e x p r e s s i o n r e c o g n i t i o n 3 0 a n d h a n d o r i e n t a t i o n e s t i m a t i o n 1 0 g e n g e t a l 8 d e f i n e d t h e l a b e l d i s t r i b u t i o n f o r a n i n s t a n c e a s a v e c t o r c o n t a i n i n g t h e p r o b a b i l i t i e s o f t h e i n s t a n c e h a v i n g e a c h l a b e l t h e y a l s o g a v e a s t r a t e g y t o a s s i g n a p r o p e r l a b e l d i s t r i b u t i o n t o a n i n s t a n c e w i t h a s i n g l e l a b e l i e a s s i g n i n g a g a u s s i a n o r t r i a n g l e d i s t r i b u t i o n w h o s e p e a k i s t h e s i n g l e l a b e l a n d p r o p o s e d a n a l g o r i t h m c a l l e d i i s l l d w h i c h i s a n i t e r a t i v e o p t i m i z a t i o n p r o c e s s b a s e d o n a t w o l a y e r e n e r g y b a s e d m o d e l y a n g e t a l 2 8 t h e n d e f i n e d a t h r e e l a y e r e n e r g y b a s e d m o d e l c a l l e d s c e l d l i n w h i c h t h e a b i l i t y t o p e r f o r m f e a t u r e l e a r n i n g i s i m p r o v e d b y a d d i n g t h e e x t r a h i d d e n l a y e r a n d s p a r s i t y c o n s t r a i n t s a r e a l s o i n c o r p o r a t e d t o a m e l i o r a t e t h e m o d e l g e n g 6 d e v e l o p e d a n a c c e l e r a t e d v e r s i o n o f i i s l l d c a l l e d b f g s l d l b y u s i n g q u a s i n e w t o n o p t i m i z a t i o n a l l t h e a b o v e l d l m e t h o d s a s s u m e t h a t t h e l a b e l d i s t r i b u t i o n c a n b e r e p r e s e n t e d b y a m a x i m u m e n t r o p y m o d e l 2 b u t t h e e x p o n e n t i a l p a r t o f t h i s m o d e l r e s t r i c t s t h e g e n e r a l i t y o f t h e d i s t r i b u t i o n f o r m a n o t h e r w a y t o a d d r e s s t h e l d l t a s k i s t o e x t e n d e x i s t i n g l e a r n i n g a l g o r i t h m s t o d e a l w i t h l a b e l d i s t r i b u t i o n s g e n g a n d h o u 7 p r o p o s e d l d s v r a l d l m e t h o d b y e x t e n d i n g s u p p o r t v e c t o r r e g r e s s o r w h i c h f i t a s i g m o i d f u n c t i o n t o e a c h c o m p o n e n t o f t h e d i s t r i b u t i o n s i m u l t a n e o u s l y b y a s u p p o r t v e c t o r m a c h i n e x i n g e t a l 2 7 t h e n e x t e n d e d b o o s t i n g t o a d d r e s s t h e l d l t a s k b y a d d i t i v e w e i g h t e d r e g r e s s o r s t h e y s h o w e d t h a t u s i n g t h e v e c t o r t r e e m o d e l a s t h e w e a k r e g r e s s o r c a n l e a d t o b e t t e r p e r f o r m a n c e a n d n a m e d t h i s m e t h o d a o s o l d l l o g i t b o o s t a s t h e l e a r n i n g o f t h i s t r e e m o d e l i s b a s e d o n l o c a l l y o p t i m a l h a r d d a t a p a r t i t i o n f u n c t i o n s a t e a c h s p l i t n o d e a o s o l d l l o g i t b o o s t i s u n a b l e t o b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g e x t e n d i n g c u r r e n t d e e p l e a r n i n g a l g o r i t h m s t o 3 x0c a d d r e s s t h e l d l t a s k i s a n i n t e r e s t i n g t o p i c b u t t h e e x i s t i n g s u c h a m e t h o d c a l l e d d l d l 5 s t i l l f o c u s e s o n m a x i m u m e n t r o p y m o d e l b a s e d l d l o u r m e t h o d l d l f s e x t e n d s d i f f e r e n t i a b l e d e c i s i o n t r e e s t o a d d r e s s l d l t a s k s i n w h i c h t h e p r e d i c t e d l a b e l d i s t r i b u t i o n f o r a s a m p l e c a n b e e x p r e s s e d b y a l i n e a r c o m b i n a t i o n o f t h e l a b e l d i s t r i b u t i o n s o f t h e t r a i n i n g d a t a a n d t h u s h a v e n o r e s t r i c t i o n s o n t h e d i s t r i b u t i o n s e g n o r e q u i r e m e n t o f t h e m a x i m u m e n t r o p y m o d e l i n a d d i t i o n t h a n k s t o t h e i n t r o d u c t i o n o f d i f f e r e n t i a b l e d e c i s i o n f u n c t i o n s l d l f s c a n b e c o m b i n e d w i t h r e p r e s e n t a t i o n l e a r n i n g e g t o l e a r n d e e p f e a t u r e s i n a n e n d t o e n d m a n n e r 3 l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s a f o r e s t i s a n e n s e m b l e o f d e c i s i o n t r e e s w e f i r s t i n t r o d u c e h o w t o l e a r n a s i n g l e d e c i s i o n t r e e b y l a b e l d i s t r i b u t i o n l e a r n i n g t h e n d e s c r i b e t h e l e a r n i n g o f a f o r e s t 3 1 p r o b l e m f o r m u l a t i o n l e t x r m d e n o t e t h e i n p u t s p a c e a n d y y 1 y 2 y c d e n o t e t h e c o m p l e t e s e t o f l a b e l s w h e r e c i s t h e n u m b e r o f p o s s i b l e l a b e l v a l u e s w e c o n s i d e r a l a b e l d i s t r i b u t i o n l e a r n i n g l d l p r o b l e m w h e r e f o r e a c h i n p u t s a m p l e x xe2\\\\x88\\\\x88 x t h e r e i s a l a b e l d i s t r i b u t i o n d d x y 1 d y x 2 d y x c xe2\\\\x88\\\\x88 r c h e r e d y x c e x p r e s s e s t h e p r o b a b i l i t y o f t h e s a m p l e x h a v i n g t h e c t h l a b e l y c a n d t h u s h a s t h e p c c o n s t r a i n t s t h a t d y x c xe2\\\\x88\\\\x88 0 1 a n d c 1 d y x c 1 t h e g o a l o f t h e l d l p r o b l e m i s t o l e a r n a m a p p i n g f u n c t i o n g x xe2\\\\x86\\\\x92 d b e t w e e n a n i n p u t s a m p l e x a n d i t s c o r r e s p o n d i n g l a b e l d i s t r i b u t i o n d h e r e w e w a n t t o l e a r n t h e m a p p i n g f u n c t i o n g x b y a d e c i s i o n t r e e b a s e d m o d e l t a d e c i s i o n t r e e c o n s i s t s o f a s e t o f s p l i t n o d e s n a n d a s e t o f l e a f n o d e s l e a c h s p l i t n o d e n xe2\\\\x88\\\\x88 n d e f i n e s a s p l i t f u n c t i o n s n xc2\\\\xb7 xce\\\\x98 x xe2\\\\x86\\\\x92 0 1 p a r a m e t e r i z e d b y xce\\\\x98 t o d e t e r m i n e w h e t h e r a s a m p l e i s s e n t t o t h e l e f t o r r i g h t s u b t r e e e a c h l e a f n o d e xe2\\\\x88\\\\x88 l h o l d s a d i s t r i b u t i o n q q 1 q 2 q c p c o v e r y i e q c xe2\\\\x88\\\\x88 0 1 a n d c 1 q c 1 t o b u i l d a d i f f e r e n t i a b l e d e c i s i o n t r e e f o l l o w i n g 2 0 w e u s e a p r o b a b i l i s t i c s p l i t f u n c t i o n s n x xce\\\\x98 xcf\\\\x83 f xcf\\\\x95 n x xce\\\\x98 w h e r e xcf\\\\x83 xc2\\\\xb7 i s a s i g m o i d f u n c t i o n xcf\\\\x95 xc2\\\\xb7 i s a n i n d e x f u n c t i o n t o b r i n g t h e xcf\\\\x95 n t h o u t p u t o f f u n c t i o n f x xce\\\\x98 i n c o r r e s p o n d e n c e w i t h s p l i t n o d e n a n d f x xe2\\\\x86\\\\x92 r m i s a r e a l v a l u e d f e a t u r e l e a r n i n g f u n c t i o n d e p e n d i n g o n t h e s a m p l e x a n d t h e p a r a m e t e r xce\\\\x98 a n d c a n t a k e a n y f o r m f o r a s i m p l e f o r m i t c a n b e a l i n e a r t r a n s f o r m a t i o n o f x w h e r e xce\\\\x98 i s t h e t r a n s f o r m a t i o n m a t r i x f o r a c o m p l e x f o r m i t c a n b e a d e e p n e t w o r k t o p e r f o r m r e p r e s e n t a t i o n l e a r n i n g i n a n e n d t o e n d m a n n e r t h e n xce\\\\x98 i s t h e n e t w o r k p a r a m e t e r t h e c o r r e s p o n d e n c e b e t w e e n t h e s p l i t n o d e s a n d t h e o u t p u t u n i t s o f f u n c t i o n f i n d i c a t e d b y xcf\\\\x95 xc2\\\\xb7 t h a t i s r a n d o m l y g e n e r a t e d b e f o r e t r e e l e a r n i n g i e w h i c h o u t p u t u n i t s f r o m xe2\\\\x80\\\\x9c f xe2\\\\x80\\\\x9d a r e u s e d f o r c o n s t r u c t i n g a t r e e i s d e t e r m i n e d r a n d o m l y a n e x a m p l e t o d e m o n s t r a t e xcf\\\\x95 xc2\\\\xb7 i s s h o w n i n f i g 2 t h e n t h e p r o b a b i l i t y o f t h e s a m p l e x f a l l i n g i n t o l e a f n o d e i s g i v e n b y y l r p x xce\\\\x98 s n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 xe2\\\\x88\\\\x92 s n x xce\\\\x98 1 xe2\\\\x88\\\\x88 l n 1 n xe2\\\\x88\\\\x88 n w h e r e 1 xc2\\\\xb7 i s a n i n d i c a t o r f u n c t i o n a n d l l n a n d l r n d e n o t e t h e s e t s o f l e a f n o d e s h e l d b y t h e l e f t a n d r i g h t s u b t r e e s o f n o d e n t n l a n d t n r r e s p e c t i v e l y t h e o u t p u t o f t h e t r e e t w r t x i e t h e m a p p i n g f u n c t i o n g i s d e f i n e d b y x g x xce\\\\x98 t p x xce\\\\x98 q 2 xe2\\\\x88\\\\x88 l 3 2 t r e e o p t i m i z a t i o n g i v e n a t r a i n i n g s e t s x i d i n i 1 o u r g o a l i s t o l e a r n a d e c i s i o n t r e e t d e s c r i b e d i n s e c 3 1 w h i c h c a n o u t p u t a d i s t r i b u t i o n g x i xce\\\\x98 t s i m i l a r t o d i f o r e a c h s a m p l e x i t o t h i s e n d a s t r a i g h t f o r w a r d w a y i s t o m i n i m i z e t h e k u l l b a c k l e i b l e r k l d i v e r g e n c e b e t w e e n e a c h g x i xce\\\\x98 t a n d d i o r e q u i v a l e n t l y t o m i n i m i z e t h e f o l l o w i n g c r o s s e n t r o p y l o s s r q xce\\\\x98 s xe2\\\\x88\\\\x92 n c n c x10 x x11 1 x x y c 1 x x y c d x i l o g g c x i xce\\\\x98 t xe2\\\\x88\\\\x92 d x i l o g p x i xce\\\\x98 q c 3 n i 1 c 1 n i 1 c 1 xe2\\\\x88\\\\x88 l 4 x0c w h e r e q d e n o t e t h e d i s t r i b u t i o n s h e l d b y a l l t h e l e a f n o d e s l a n d g c x i xce\\\\x98 t i s t h e c t h o u t p u t u n i t o f g x i xce\\\\x98 t l e a r n i n g t h e t r e e t r e q u i r e s t h e e s t i m a t i o n o f t w o p a r a m e t e r s 1 t h e s p l i t n o d e p a r a m e t e r xce\\\\x98 a n d 2 t h e d i s t r i b u t i o n s q h e l d b y t h e l e a f n o d e s t h e b e s t p a r a m e t e r s xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 a r e d e t e r m i n e d b y xce\\\\x98 xe2\\\\x88\\\\x97 q xe2\\\\x88\\\\x97 a r g m i n r q xce\\\\x98 s 4 xce\\\\x98 q t o s o l v e e q n 4 w e c o n s i d e r a n a l t e r n a t i n g o p t i m i z a t i o n s t r a t e g y f i r s t w e f i x q a n d o p t i m i z e xce\\\\x98 t h e n w e f i x xce\\\\x98 a n d o p t i m i z e q t h e s e t w o l e a r n i n g s t e p s a r e a l t e r n a t i v e l y p e r f o r m e d u n t i l c o n v e r g e n c e o r a m a x i m u m n u m b e r o f i t e r a t i o n s i s r e a c h e d d e f i n e d i n t h e e x p e r i m e n t s 3 2 1 l e a r n i n g s p l i t n o d e s i n t h i s s e c t i o n w e d e s c r i b e h o w t o l e a r n t h e p a r a m e t e r xce\\\\x98 f o r s p l i t n o d e s w h e n t h e d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s q a r e f i x e d w e c o m p u t e t h e g r a d i e n t o f t h e l o s s r q xce\\\\x98 s w r t xce\\\\x98 b y t h e c h a i n r u l e n xe2\\\\x88\\\\x82 r q xce\\\\x98 s x x xe2\\\\x88\\\\x82 r q xce\\\\x98 s xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 5 xe2\\\\x88\\\\x82 xce\\\\x98 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 i 1 n xe2\\\\x88\\\\x88 n w h e r e o n l y t h e f i r s t t e r m d e p e n d s o n t h e t r e e a n d t h e s e c o n d t e r m d e p e n d s o n t h e s p e c i f i c t y p e o f t h e f u n c t i o n f xcf\\\\x95 n t h e f i r s t t e r m i s g i v e n b y c x01 g c x i xce\\\\x98 t n l x11 g c x i xce\\\\x98 t n r 1 x y c x10 xe2\\\\x88\\\\x82 r q xce\\\\x98 s d x i s n x i xce\\\\x98 xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 s n x i xce\\\\x98 6 xe2\\\\x88\\\\x82 f xcf\\\\x95 n x i xce\\\\x98 n c 1 g c x i xce\\\\x98 t g c x i xce\\\\x98 t p p w h e r e g c x i xce\\\\x98 t n l xe2\\\\x88\\\\x88 l l n p x i xce\\\\x98 q c a n d g c x i xce\\\\x98 t n r xe2\\\\x88\\\\x88 l r n p x i xce\\\\x98 q c n o t e t h a t l e t t n b e t h e t r e e r o o t e d a t t h e n o d e n t h e n w e h a v e g c x i xce\\\\x98 t n g c x i xce\\\\x98 t n l g c x i xce\\\\x98 t n r t h i s m e a n s t h e g r a d i e n t c o m p u t a t i o n i n e q n 6 c a n b e s t a r t e d a t t h e l e a f n o d e s a n d c a r r i e d o u t i n a b o t t o m u p m a n n e r t h u s t h e s p l i t n o d e p a r a m e t e r s c a n b e l e a r n e d b y s t a n d a r d b a c k p r o p a g a t i o n 3 2 2 l e a r n i n g l e a f n o d e s n o w f i x i n g t h e p a r a m e t e r xce\\\\x98 w e s h o w h o w t o l e a r n t h e d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s q w h i c h i s a c o n s t r a i n e d o p t i m i z a t i o n p r o b l e m m i n r q xce\\\\x98 s s t xe2\\\\x88\\\\x80 q c x q c 1 7 c 1 h e r e w e p r o p o s e t o a d d r e s s t h i s c o n s t r a i n e d c o n v e x o p t i m i z a t i o n p r o b l e m b y v a r i a t i o n a l b o u n d i n g 1 9 2 9 w h i c h l e a d s t o a s t e p s i z e f r e e a n d f a s t c o n v e r g e d u p d a t e r u l e f o r q i n v a r i a t i o n a l b o u n d i n g a n o r i g i n a l o b j e c t i v e f u n c t i o n t o b e m i n i m i z e d g e t s r e p l a c e d b y i t s b o u n d i n a n i t e r a t i v e m a n n e r a u p p e r b o u n d f o r t h e l o s s f u n c t i o n r q xce\\\\x98 s c a n b e o b t a i n e d b y j e n s e n xe2\\\\x80\\\\x99 s i n e q u a l i t y r q xce\\\\x98 s xe2\\\\x88\\\\x92 n c x10 x x11 1 x x y c d x i l o g p x i xce\\\\x98 q c n i 1 c 1 xe2\\\\x88\\\\x88 l xe2\\\\x89\\\\xa4 xe2\\\\x88\\\\x92 w h e r e xce\\\\xb q c x i 1 n n x c x i 1 c 1 p x i xce\\\\x98 q c g c x i xce\\\\x98 t xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x88\\\\x92 d y x c i x xce\\\\xb q xcc\\\\x84 c x i l o g xe2\\\\x88\\\\x88 l x10 p x xce\\\\x98 q x11 i c xce\\\\xb q xcc\\\\x84 c x i 8 w e d e f i n e n c x10 p x xce\\\\x98 q x11 1 x x y c x i c d x i xce\\\\xb q xcc\\\\x84 c x i l o g n i 1 c 1 xce\\\\xb q xcc\\\\x84 c x i 9 xe2\\\\x88\\\\x88 l t h e n xcf\\\\x86 q q xcc\\\\x84 i s a n u p p e r b o u n d f o r r q xce\\\\x98 s w h i c h h a s t h e p r o p e r t y t h a t f o r a n y q a n d q xcc\\\\x84 xcf\\\\x86 q q xcc\\\\x84 xe2\\\\x89\\\\xa5 r q xce\\\\x98 s a n d xcf\\\\x86 q q r q xce\\\\x98 s a s s u m e t h a t w e a r e a t a p o i n t q t c o r r e s p o n d i n g t o t h e t t h i t e r a t i o n t h e n xcf\\\\x86 q q t i s a n u p p e r b o u n d f o r r q xce\\\\x98 s i n t h e n e x t i t e r a t i o n q t 1 i s c h o s e n s u c h t h a t xcf\\\\x86 q t 1 q xe2\\\\x89\\\\xa4 r q t xce\\\\x98 s w h i c h i m p l i e s r q t 1 xce\\\\x98 s xe2\\\\x89\\\\xa4 r q t xce\\\\x98 s 5 x0c c o n s e q u e n t l y w e c a n m i n i m i z e xcf\\\\x86 q q xcc\\\\x84 i n s t e a d o f r q xce\\\\x98 s a f t e r e n s u r i n g t h a t r q t xce\\\\x98 s xcf\\\\x86 q t q xcc\\\\x84 i e q xcc\\\\x84 q t s o w e h a v e q t 1 a r g m i n xcf\\\\x86 q q t s t xe2\\\\x88\\\\x80 q c x q c 1 1 0 c 1 w h i c h l e a d s t o m i n i m i z i n g t h e l a g r a n g i a n d e f i n e d b y xcf\\\\x95 q q t xcf\\\\x86 q q t x xce\\\\xbb xe2\\\\x88\\\\x88 l w h e r e xce\\\\xbb i s t h e l a g r a n g e m u l t i p l i e r b y s e t t i n g xce\\\\xbb t 1 n o t e t h a t q c t 1 xe2\\\\x88\\\\x88 0 1 a n d d i s t r i b u t i o n s h e l d b y t h e l e a f n o d e s t h e s t a r t i n g 0 d i s t r i b u t i o n q c c 1 3 3 q c xe2\\\\x88\\\\x92 1 1 1 c 1 xe2\\\\x88\\\\x82 xcf\\\\x95 q q t xe2\\\\x88\\\\x82 q c n c 1 x x y c t t 1 d xce\\\\xb q c x i a n d q c n i 1 c 1 x i s a t i s f i e s t h a t q c c x 0 w e h a v e p n y c t d x i xce\\\\xb q c x i p c i 1 p n y c t xce\\\\xb q x d x i i c 1 i 1 c 1 2 t 1 1 e q n 1 2 i s t h e u p d a t e s c h e m e f o r c 1 q c 0 p o i n t q c a n b e s i m p l y i n i t i a l i z e d b y t h e u n i f o r m p c l e a r n i n g a f o r e s t a f o r e s t i s a n e n s e m b l e o f d e c i s i o n t r e e s f t 1 t k i n t h e t r a i n i n g s t a g e a l l t r e e s i n t h e f o r e s t f u s e t h e s a m e p a r a m e t e r s xce\\\\x98 f o r f e a t u r e l e a r n i n g f u n c t i o n f xc2\\\\xb7 xce\\\\x98 b u t c o r r e s p o n d t o d i f f e r e n t o u t p u t u n i t s o f f a s s i g n e d b y xcf\\\\x95 s e e f i g 2 b u t e a c h t r e e h a s i n d e p e n d e n t l e a f n o d e p r e d i c t i o n s q t h e l o s s f u n c t i o n f o r a f o r e s t i s g i v e n b y a v e r a g i n g t h e l o s s f u n c t i o n s f o r a l l i n d i v i d u a l t r e e s p k 1 r f k k 1 r t k w h e r e r t k i s t h e l o s s f u n c t i o n f o r t r e e t k d e f i n e d b y e q n 3 t o l e a r n xce\\\\x98 b y f i x i n g t h e l e a f n o d e p r e d i c t i o n s q o f a l l t h e t r e e s i n t h e f o r e s t f b a s e d o n t h e d e r i v a t i o n i n s e c 3 2 a n d r e f e r r i n g t o f i g 2 w e h a v e n k xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x i xce\\\\x98 1 x x x xe2\\\\x88\\\\x82 r f xe2\\\\x88\\\\x82 r t k xe2\\\\x88\\\\x82 xce\\\\x98 k i 1 xe2\\\\x88\\\\x82 f xcf\\\\x95 k n x i xce\\\\x98 xe2\\\\x88\\\\x82 xce\\\\x98 1 3 k 1 n xe2\\\\x88\\\\x88 n k w h e r e n k a n d xcf\\\\x95 k xc2\\\\xb7 a r e t h e s p l i t n o d e s e t a n d t h e i n d e x f u n c t i o n o f t k r e s p e c t i v e l y n o t e t h a t t h e i n d e x f u n c t i o n xcf\\\\x95 k xc2\\\\xb7 f o r e a c h t r e e i s r a n d o m l y a s s i g n e d b e f o r e t r e e l e a r n i n g a n d t h u s s p l i t n o d e s c o r r e s p o n d t o a s u b s e t o f o u t p u t u n i t s o f f t h i s s t r a t e g y i s s i m i l a r t o t h e r a n d o m s u b s p a c e m e t h o d 1 7 w h i c h i n c r e a s e s t h e r a n d o m n e s s i n t r a i n i n g t o r e d u c e t h e r i s k o f o v e r f i t t i n g a s f o r q s i n c e e a c h t r e e i n t h e f o r e s t f h a s i t s o w n l e a f n o d e p r e d i c t i o n s q w e c a n u p d a t e t h e m i n d e p e n d e n t l y b y e q n 1 2 g i v e n b y xce\\\\x98 f o r i m p l e m e n t a t i o n a l c o n v e n i e n c e w e d o n o t c o n d u c t t h i s u p d a t e s c h e m e o n t h e w h o l e d a t a s e t s b u t o n a s e t o f m i n i b a t c h e s b t h e t r a i n i n g p r o c e d u r e o f a l d l f i s s h o w n i n a l g o r i t h m 1 a l g o r i t h m 1 t h e t r a i n i n g p r o c e d u r e o f a l d l f r e q u i r e s t r a i n i n g s e t n b t h e n u m b e r o f m i n i b a t c h e s t o u p d a t e q i n i t i a l i z e xce\\\\x98 r a n d o m l y a n d q u n i f o r m l y s e t b xe2\\\\x88\\\\x85 w h i l e n o t c o n v e r g e d o w h i l e b n b d o r a n d o m l y s e l e c t a m i n i b a t c h b f r o m s u p d a t e s xce\\\\x98 b y c o m p u t i n g g r a d i e n t e q n 1 3 o n b b b b e n d w h i l e u p d a t e q b y i t e r a t i n g e q n 1 2 o n b b xe2\\\\x88\\\\x85 e n d w h i l e i n t h e t e s t i n g s t a g e t h e o u t p u t o f t h e f o r e s t f i s g i v e n b y a v e r a g i n g t h e p r e d i c t i o n s f r o m a l l t h e p k 1 i n d i v i d u a l t r e e s g x xce\\\\x98 f k k 1 g x xce\\\\x98 t k 6 x0c 4 e x p e r i m e n t a l r e s u l t s o u r r e a l i z a t i o n o f l d l f s i s b a s e d o n xe2\\\\x80\\\\x9c c a f f e xe2\\\\x80\\\\x9d 1 8 i t i s m o d u l a r a n d i m p l e m e n t e d a s a s t a n d a r d n e u r a l n e t w o r k l a y e r w e c a n e i t h e r u s e i t a s a s h a l l o w s t a n d a l o n e m o d e l s l d l f s o r i n t e g r a t e i t w i t h a n y d e e p n e t w o r k s d l d l f s w e e v a l u a t e s l d l f s o n d i f f e r e n t l d l t a s k s a n d c o m p a r e i t w i t h o t h e r s t a n d a l o n e l d l m e t h o d s a s d l d l f s c a n b e l e a r n e d f r o m r a w i m a g e d a t a i n a n e n d t o e n d m a n n e r w e v e r i f y d l d l f s o n a c o m p u t e r v i s i o n a p p l i c a t i o n i e f a c i a l a g e e s t i m a t i o n t h e d e f a u l t s e t t i n g s f o r t h e p a r a m e t e r s o f o u r f o r e s t s a r e t r e e n u m b e r 5 t r e e d e p t h 7 o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n 6 4 i t e r a t i o n t i m e s t o u p d a t e l e a f n o d e p r e d i c t i o n s 2 0 t h e n u m b e r o f m i n i b a t c h e s t o u p d a t e l e a f n o d e p r e d i c t i o n s 1 0 0 m a x i m u m i t e r a t i o n 2 5 0 0 0 4 1 c o m p a r i s o n o f s l d l f s t o s t a n d a l o n e l d l m e t h o d s w e c o m p a r e o u r s h a l l o w m o d e l s l d l f s w i t h o t h e r s t a t e o f t h e a r t s t a n d a l o n e l d l m e t h o d s f o r s l d l f s t h e f e a t u r e l e a r n i n g f u n c t i o n f x xce\\\\x98 i s a l i n e a r t r a n s f o r m a t i o n o f x i e t h e i t h o u t p u t u n i t f i x xce\\\\xb8 i xce\\\\xb8 i x w h e r e xce\\\\xb8 i i s t h e i t h c o l u m n o f t h e t r a n s f o r m a t i o n m a t r i x xce\\\\x98 w e u s e d 3 p o p u l a r l d l d a t a s e t s i n 6 m o v i e h u m a n g e n e a n d n a t u r a l s c e n e 1 t h e s a m p l e s i n t h e s e 3 d a t a s e t s a r e r e p r e s e n t e d b y n u m e r i c a l d e s c r i p t o r s a n d t h e g r o u n d t r u t h s f o r t h e m a r e t h e r a t i n g d i s t r i b u t i o n s o f c r o w d o p i n i o n o n m o v i e s t h e d i s e a s e s d i s t r i b u t i o n s r e l a t e d t o h u m a n g e n e s a n d l a b e l d i s t r i b u t i o n s o n s c e n e s s u c h a s p l a n t s k y a n d c l o u d r e s p e c t i v e l y t h e l a b e l d i s t r i b u t i o n s o f t h e s e 3 d a t a s e t s a r e m i x t u r e d i s t r i b u t i o n s s u c h a s t h e r a t i n g d i s t r i b u t i o n s h o w n i n f i g 1 b f o l l o w i n g 7 2 7 w e u s e 6 m e a s u r e s t o e v a l u a t e t h e p e r f o r m a n c e s o f l d l m e t h o d s w h i c h c o m p u t e t h e a v e r a g e s i m i l a r i t y d i s t a n c e b e t w e e n t h e p r e d i c t e d r a t i n g d i s t r i b u t i o n s a n d t h e r e a l r a t i n g d i s t r i b u t i o n s i n c l u d i n g 4 d i s t a n c e m e a s u r e s k l e u c l i d e a n s xcf\\\\x86 r e n s e n s q u a r e d xcf\\\\x87 2 a n d t w o s i m i l a r i t y m e a s u r e s f i d e l i t y i n t e r s e c t i o n w e e v a l u a t e o u r s h a l l o w m o d e l s l d l f s o n t h e s e 3 d a t a s e t s a n d c o m p a r e i t w i t h o t h e r s t a t e o f t h e a r t s t a n d a l o n e l d l m e t h o d s t h e r e s u l t s o f s l d l f s a n d t h e c o m p e t i t o r s a r e s u m m a r i z e d i n t a b l e 1 f o r m o v i e w e q u o t e t h e r e s u l t s r e p o r t e d i n 2 7 a s t h e c o d e o f 2 7 i s n o t p u b l i c l y a v a i l a b l e f o r t h e r e s u l t s o f t h e o t h e r s t w o w e r u n c o d e t h a t t h e a u t h o r s h a d m a d e a v a i l a b l e i n a l l c a s e f o l l o w i n g 2 7 6 w e s p l i t e a c h d a t a s e t i n t o 1 0 f i x e d f o l d s a n d d o s t a n d a r d t e n f o l d c r o s s v a l i d a t i o n w h i c h r e p r e s e n t s t h e r e s u l t b y xe2\\\\x80\\\\x9c m e a n xc2\\\\xb1 s t a n d a r d d e v i a t i o n xe2\\\\x80\\\\x9d a n d m a t t e r s l e s s h o w t r a i n i n g a n d t e s t i n g d a t a g e t d i v i d e d a s c a n b e s e e n f r o m t a b l e 1 s l d l f s p e r f o r m b e s t o n a l l o f t h e s i x m e a s u r e s t a b l e 1 c o m p a r i s o n r e s u l t s o n t h r e e l d l d a t a s e t s 6 xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x91 xe2\\\\x80\\\\x9d a n d xe2\\\\x80\\\\x9c xe2\\\\x86\\\\x93 xe2\\\\x80\\\\x9d i n d i c a t e t h e l a r g e r a n d t h e s m a l l e r t h e b e t t e r r e s p e c t i v e l y d a t a s e t m e t h o d k l xe2\\\\x86\\\\x93 e u c l i d e a n xe2\\\\x86\\\\x93 s xcf\\\\x86 r e n s e n xe2\\\\x86\\\\x93 s q u a r e d xcf\\\\x87 2 xe2\\\\x86\\\\x93 f i d e l i t y xe2\\\\x86\\\\x91 i n t e r s e c t i o n xe2\\\\x86\\\\x91 m o v i e s l d l f o u r s a o s o l d l o g i t b o o s t 2 7 l d l o g i t b o o s t 2 7 l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 0 7 3 xc2\\\\xb1 0 0 0 5 0 0 8 6 xc2\\\\xb1 0 0 0 4 0 0 9 0 xc2\\\\xb1 0 0 0 4 0 0 9 2 xc2\\\\xb1 0 0 0 5 0 0 9 9 xc2\\\\xb1 0 0 0 4 0 1 2 9 xc2\\\\xb1 0 0 0 7 0 1 3 3 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 9 xc2\\\\xb1 0 0 0 3 0 1 5 8 xc2\\\\xb1 0 0 0 4 0 1 6 7 xc2\\\\xb1 0 0 0 4 0 1 8 7 xc2\\\\xb1 0 0 0 4 0 1 3 0 xc2\\\\xb1 0 0 0 3 0 1 5 2 xc2\\\\xb1 0 0 0 3 0 1 5 5 xc2\\\\xb1 0 0 0 3 0 1 5 6 xc2\\\\xb1 0 0 0 4 0 1 6 4 xc2\\\\xb1 0 0 0 3 0 1 8 3 xc2\\\\xb1 0 0 0 4 0 0 7 0 xc2\\\\xb1 0 0 0 4 0 0 8 4 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 3 0 0 8 8 xc2\\\\xb1 0 0 0 4 0 0 9 6 xc2\\\\xb1 0 0 0 4 0 1 2 0 xc2\\\\xb1 0 0 0 5 0 9 8 1 xc2\\\\xb1 0 0 0 1 0 9 7 8 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 7 xc2\\\\xb1 0 0 0 1 0 9 7 4 xc2\\\\xb1 0 0 0 1 0 9 6 7 xc2\\\\xb1 0 0 0 1 0 8 7 0 xc2\\\\xb1 0 0 0 3 0 8 4 8 xc2\\\\xb1 0 0 0 3 0 8 4 5 xc2\\\\xb1 0 0 0 3 0 8 4 4 xc2\\\\xb1 0 0 0 4 0 8 3 6 xc2\\\\xb1 0 0 0 3 0 8 1 7 xc2\\\\xb1 0 0 0 4 s l d l f o u r s l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 2 2 8 xc2\\\\xb1 0 0 0 6 0 2 4 5 xc2\\\\xb1 0 0 1 9 0 2 3 1 xc2\\\\xb1 0 0 2 1 0 2 3 9 xc2\\\\xb1 0 0 1 8 0 0 8 5 xc2\\\\xb1 0 0 0 2 0 0 9 9 xc2\\\\xb1 0 0 0 5 0 0 7 6 xc2\\\\xb1 0 0 0 6 0 0 8 9 xc2\\\\xb1 0 0 0 6 0 2 1 2 xc2\\\\xb1 0 0 0 2 0 2 2 9 xc2\\\\xb1 0 0 1 5 0 2 3 1 xc2\\\\xb1 0 0 1 2 0 2 5 3 xc2\\\\xb1 0 0 0 9 0 1 7 9 xc2\\\\xb1 0 0 0 4 0 1 8 9 xc2\\\\xb1 0 0 2 1 0 2 1 1 xc2\\\\xb1 0 0 1 8 0 2 0 5 xc2\\\\xb1 0 0 1 2 0 9 4 8 xc2\\\\xb1 0 0 0 1 0 9 4 0 xc2\\\\xb1 0 0 0 6 0 9 3 8 xc2\\\\xb1 0 0 0 8 0 9 4 4 xc2\\\\xb1 0 0 0 3 0 7 8 8 xc2\\\\xb1 0 0 0 2 0 7 7 1 xc2\\\\xb1 0 0 1 5 0 7 6 9 xc2\\\\xb1 0 0 1 2 0 7 4 7 xc2\\\\xb1 0 0 0 9 s l d l f o u r s l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 5 3 4 xc2\\\\xb1 0 0 1 3 0 8 5 2 xc2\\\\xb1 0 0 2 3 0 8 5 6 xc2\\\\xb1 0 0 6 1 0 8 7 9 xc2\\\\xb1 0 0 2 3 0 3 1 7 xc2\\\\xb1 0 0 1 4 0 5 1 1 xc2\\\\xb1 0 0 2 1 0 4 7 5 xc2\\\\xb1 0 0 2 9 0 4 5 8 xc2\\\\xb1 0 0 1 4 0 3 3 6 xc2\\\\xb1 0 0 1 0 0 4 9 2 xc2\\\\xb1 0 0 1 6 0 5 0 8 xc2\\\\xb1 0 0 2 6 0 5 3 9 xc2\\\\xb1 0 0 1 1 0 4 4 8 xc2\\\\xb1 0 0 1 7 0 5 9 5 xc2\\\\xb1 0 0 2 6 0 7 1 6 xc2\\\\xb1 0 0 4 1 0 7 9 2 xc2\\\\xb1 0 0 1 9 0 8 2 4 xc2\\\\xb1 0 0 0 8 0 8 1 3 xc2\\\\xb1 0 0 0 8 0 7 2 2 xc2\\\\xb1 0 0 2 1 0 6 8 6 xc2\\\\xb1 0 0 0 9 0 6 6 4 xc2\\\\xb1 0 0 1 0 0 5 0 9 xc2\\\\xb1 0 0 1 6 0 4 9 2 xc2\\\\xb1 0 0 2 6 0 4 6 1 xc2\\\\xb1 0 0 1 1 h u m a n g e n e n a t u r a l s c e n e 4 2 e v a l u a t i o n o f d l d l f s o n f a c i a l a g e e s t i m a t i o n i n s o m e l i t e r a t u r e 8 1 1 2 8 1 5 5 a g e e s t i m a t i o n i s f o r m u l a t e d a s a l d l p r o b l e m w e c o n d u c t f a c i a l a g e e s t i m a t i o n e x p e r i m e n t s o n m o r p h 2 4 w h i c h c o n t a i n s m o r e t h a n 5 0 0 0 0 f a c i a l i m a g e s f r o m a b o u t 1 3 0 0 0 p e o p l e o f d i f f e r e n t r a c e s e a c h f a c i a l i m a g e i s a n n o t a t e d w i t h a c h r o n o l o g i c a l a g e t o g e n e r a t e a n a g e d i s t r i b u t i o n f o r e a c h f a c e i m a g e w e f o l l o w t h e s a m e s t r a t e g y u s e d i n 8 2 8 5 w h i c h u s e s a g a u s s i a n d i s t r i b u t i o n w h o s e m e a n i s t h e c h r o n o l o g i c a l a g e o f t h e f a c e i m a g e f i g 1 a t h e p r e d i c t e d a g e f o r a f a c e i m a g e i s s i m p l y t h e a g e h a v i n g t h e h i g h e s t p r o b a b i l i t y i n t h e p r e d i c t e d 1 w e d o w n l o a d t h e s e d a t a s e t s f r o m h t t p c s e s e u e d u c n p e o p l e x g e n g l d l i n d e x h t m 7 x0c l a b e l d i s t r i b u t i o n t h e p e r f o r m a n c e o f a g e e s t i m a t i o n i s e v a l u a t e d b y t h e m e a n a b s o l u t e e r r o r m a e b e t w e e n p r e d i c t e d a g e s a n d c h r o n o l o g i c a l a g e s a s t h e c u r r e n t s t a t e o f t h e a r t r e s u l t o n m o r p h i s o b t a i n b y f i n e t u n i n g d l d l 5 o n v g g f a c e 2 3 w e a l s o b u i l d a d l d l f o n v g g f a c e b y r e p l a c i n g t h e s o f t m a x l a y e r i n v g g n e t b y a l d l f f o l l o w i n g 5 w e d o s t a n d a r d 1 0 t e n f o l d c r o s s v a l i d a t i o n a n d t h e r e s u l t s a r e s u m m a r i z e d i n t a b l e 2 w h i c h s h o w s d l d l f a c h i e v e t h e s t a t e o f t h e a r t p e r f o r m a n c e o n m o r p h n o t e t h a t t h e s i g n i f i c a n t p e r f o r m a n c e g a i n b e t w e e n d e e p l d l m o d e l s d l d l a n d d l d l f a n d n o n d e e p l d l m o d e l s i i s l d l c p n n b f g s l d l a n d t h e s u p e r i o r i t y o f d l d l f c o m p a r e d w i t h d l d l v e r i f i e s t h e e f f e c t i v e n e s s o f e n d t o e n d l e a r n i n g a n d o u r t r e e b a s e d m o d e l f o r l d l r e s p e c t i v e l y t a b l e 2 m a e o f a g e e s t i m a t i o n c o m p a r i s o n o n m o r p h 2 4 m e t h o d i i s l d l 1 1 c p n n 1 1 b f g s l d l 6 d l d l v g g f a c e 5 d l d l f v g g f a c e o u r s m a e 5 6 7 xc2\\\\xb1 0 1 5 4 8 7 xc2\\\\xb1 0 3 1 3 9 4 xc2\\\\xb1 0 0 5 2 4 2 xc2\\\\xb1 0 0 1 2 2 4 xc2\\\\xb1 0 0 2 a s t h e d i s t r i b u t i o n o f g e n d e r a n d e t h n i c i t y i s v e r y u n b a l a n c e d i n m o r p h m a n y a g e e s t i m a t i o n m e t h o d s 1 3 1 4 1 5 a r e e v a l u a t e d o n a s u b s e t o f m o r p h c a l l e d m o r p h s u b f o r s h o r t w h i c h c o n s i s t s o f 2 0 1 6 0 s e l e c t e d f a c i a l i m a g e s t o a v o i d t h e i n f l u e n c e o f u n b a l a n c e d d i s t r i b u t i o n t h e b e s t p e r f o r m a n c e r e p o r t e d o n m o r p h s u b i s g i v e n b y d 2 l d l 1 5 a d a t a d e p e n d e n t l d l m e t h o d a s d 2 l d l u s e d t h e o u t p u t o f t h e xe2\\\\x80\\\\x9c f c 7 xe2\\\\x80\\\\x9d l a y e r i n a l e x n e t 2 1 a s t h e f a c e i m a g e f e a t u r e s h e r e w e i n t e g r a t e a l d l f w i t h a l e x n e t f o l l o w i n g t h e e x p e r i m e n t s e t t i n g u s e d i n d 2 l d l w e e v a l u a t e o u r d l d l f a n d t h e c o m p e t i t o r s i n c l u d i n g b o t h s l l a n d l d l b a s e d m e t h o d s u n d e r s i x d i f f e r e n t t r a i n i n g s e t r a t i o s 1 0 t o 6 0 a l l o f t h e c o m p e t i t o r s a r e t r a i n e d o n t h e s a m e d e e p f e a t u r e s u s e d b y d 2 l d l a s c a n b e s e e n f r o m t a b l e 3 o u r d l d l f s s i g n i f i c a n t l y o u t p e r f o r m o t h e r s f o r a l l t r a i n i n g s e t r a t i o s n o t e t h a t t h e g e n e r a t e d a g e d i s t r i f i g u r e 3 m a e o f a g e e s t i m a t i o n c o m p a r i s o n o n b u t i o n s a r e u n i m o d a l d i s t r i b u t i o n s m o r p h s u b a n d t h e l a b e l d i s t r i b u t i o n s u s e d i n t r a i n i n g s e t r a t i o m e t h o d s e c 4 1 a r e m i x t u r e d i s t r i b u t i o n s 1 0 2 0 3 0 4 0 5 0 6 0 t h e p r o p o s e d m e t h o d l d l f s a c h i e v e a a s 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 t h e s t a t e o f t h e a r t r e s u l t s o n b o t h o f l a r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 i i s a l d l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 t h e m w h i c h v e r i f i e s t h a t o u r m o d e l d 2 l d l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h a s t h e a b i l i t y t o m o d e l a n y g e n e r a l d l d l f o u r s 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f o r m o f l a b e l d i s t r i b u t i o n s 4 3 t i m e c o m p l e x i t y l e t h a n d s b b e t h e t r e e d e p t h a n d t h e b a t c h s i z e r e s p e c t i v e l y e a c h t r e e h a s 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 s p l i t n o d e s a n d 2 h xe2\\\\x88\\\\x92 1 l e a f n o d e s l e t d 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 f o r o n e t r e e a n d o n e s a m p l e t h e c o m p l e x i t y o f a f o r w a r d p a s s a n d a b a c k w a r d p a s s a r e o d d 1 xc3\\\\x97 c o d xc3\\\\x97 c a n d o d 1 xc3\\\\x97 c d xc3\\\\x97 c o d xc3\\\\x97 c r e s p e c t i v e l y s o f o r k t r e e s a n d n b b a t c h e s t h e c o m p l e x i t y o f a f o r w a r d a n d b a c k w a r d p a s s i s o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b t h e c o m p l e x i t y o f a n i t e r a t i o n t o u p d a t e l e a f n o d e s a r e o n b xc3\\\\x97 s b xc3\\\\x97 k xc3\\\\x97 c xc3\\\\x97 d 1 o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b t h u s t h e c o m p l e x i t y f o r t h e t r a i n i n g p r o c e d u r e o n e e p o c h n b b a t c h e s a n d t h e t e s t i n g p r o c e d u r e o n e s a m p l e a r e o d xc3\\\\x97 c xc3\\\\x97 k xc3\\\\x97 n b xc3\\\\x97 s b a n d o d xc3\\\\x97 c xc3\\\\x97 k r e s p e c t i v e l y l d l f s a r e e f f i c i e n t o n m o r p h s u b 1 2 6 3 6 t r a i n i n g i m a g e s 8 4 2 4 t e s t i n g i m a g e s o u r m o d e l o n l y t a k e s 5 2 5 0 s f o r t r a i n i n g 2 5 0 0 0 i t e r a t i o n s a n d 8 s f o r t e s t i n g a l l 8 4 2 4 i m a g e s 4 4 p a r a m e t e r d i s c u s s i o n n o w w e d i s c u s s t h e i n f l u e n c e o f p a r a m e t e r s e t t i n g s o n p e r f o r m a n c e w e r e p o r t t h e r e s u l t s o f r a t i n g p r e d i c t i o n o n m o v i e m e a s u r e d b y k l a n d a g e e s t i m a t i o n o n m o r p h s u b w i t h 6 0 t r a i n i n g s e t r a t i o m e a s u r e d b y m a e f o r d i f f e r e n t p a r a m e t e r s e t t i n g s i n t h i s s e c t i o n t r e e n u m b e r a s a f o r e s t i s a n e n s e m b l e m o d e l i t i s n e c e s s a r y t o i n v e s t i g a t e h o w p e r f o r m a n c e s c h a n g e b y v a r y i n g t h e t r e e n u m b e r u s e d i n a f o r e s t n o t e t h a t a s w e d i s c u s s e d i n s e c 2 t h e e n s e m b l e s t r a t e g y t o l e a r n a f o r e s t p r o p o s e d i n d n d f s 2 0 i s d i f f e r e n t f r o m o u r s t h e r e f o r e i t i s n e c e s s a r y t o s e e w h i c h e n s e m b l e s t r a t e g y i s b e t t e r t o l e a r n a f o r e s t t o w a r d s t h i s e n d w e r e p l a c e o u r e n s e m b l e s t r a t e g y i n d l d l f s b y t h e o n e u s e d i n d n d f s a n d n a m e t h i s m e t h o d d n d f s l d l t h e c o r r e s p o n d i n g s h a l l o w m o d e l i s n a m e d b y s n d f s l d l w e f i x o t h e r p a r a m e t e r s i e t r e e d e p t h a n d 8 x0c o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n a s t h e d e f a u l t s e t t i n g a s s h o w n i n f i g 4 a o u r e n s e m b l e s t r a t e g y c a n i m p r o v e t h e p e r f o r m a n c e b y u s i n g m o r e t r e e s w h i l e t h e o n e u s e d i n d n d f s e v e n l e a d s t o a w o r s e p e r f o r m a n c e t h a n o n e f o r a s i n g l e t r e e o b s e r v e d f r o m f i g 4 t h e p e r f o r m a n c e o f l d l f s c a n b e i m p r o v e d b y u s i n g m o r e t r e e s b u t t h e i m p r o v e m e n t b e c o m e s i n c r e a s i n g l y s m a l l e r a n d s m a l l e r t h e r e f o r e u s i n g m u c h l a r g e r e n s e m b l e s d o e s n o t y i e l d a b i g i m p r o v e m e n t o n m o v i e t h e n u m b e r o f t r e e s k 1 0 0 k l 0 0 7 0 v s k 2 0 k l 0 0 7 1 n o t e t h a t n o t a l l r a n d o m f o r e s t s b a s e d m e t h o d s u s e a l a r g e n u m b e r o f t r e e s e g s h o t t o n e t a l 2 5 o b t a i n e d v e r y g o o d p o s e e s t i m a t i o n r e s u l t s f r o m d e p t h i m a g e s b y o n l y 3 d e c i s i o n t r e e s t r e e d e p t h t r e e d e p t h i s a n o t h e r i m p o r t a n t p a r a m e t e r f o r d e c i s i o n t r e e s i n l d l f s t h e r e i s a n i m p l i c i t c o n s t r a i n t b e t w e e n t r e e d e p t h h a n d o u t p u t u n i t n u m b e r o f t h e f e a t u r e l e a r n i n g f u n c t i o n xcf\\\\x84 xcf\\\\x84 xe2\\\\x89\\\\xa5 2 h xe2\\\\x88\\\\x92 1 xe2\\\\x88\\\\x92 1 t o d i s c u s s t h e i n f l u e n c e o f t r e e d e p t h t o t h e p e r f o r m a n c e o f d l d l f s w e s e t xcf\\\\x84 2 h xe2\\\\x88\\\\x92 1 a n d f i x t r e e n u m b e r k 1 a n d t h e p e r f o r m a n c e c h a n g e b y v a r y i n g t r e e d e p t h i s s h o w n i n f i g 4 b w e s e e t h a t t h e p e r f o r m a n c e f i r s t i m p r o v e s t h e n d e c r e a s e s w i t h t h e i n c r e a s e o f t h e t r e e d e p t h t h e r e a s o n i s a s t h e t r e e d e p t h i n c r e a s e s t h e d i m e n s i o n o f l e a r n e d f e a t u r e s i n c r e a s e s e x p o n e n t i a l l y w h i c h g r e a t l y i n c r e a s e s t h e t r a i n i n g d i f f i c u l t y s o u s i n g m u c h l a r g e r d e p t h s m a y l e a d t o b a d p e r f o r m a n c e o n m o v i e t r e e d e p t h h 1 8 k l 0 1 1 6 2 v s h 9 k l 0 0 8 3 1 f i g u r e 4 t h e p e r f o r m a n c e c h a n g e o f a g e e s t i m a t i o n o n m o r p h s u b a n d r a t i n g p r e d i c t i o n o n m o v i e b y v a r y i n g a t r e e n u m b e r a n d b t r e e d e p t h o u r a p p r o a c h d l d l f s s l d l f s c a n i m p r o v e t h e p e r f o r m a n c e b y u s i n g m o r e t r e e s w h i l e u s i n g t h e e n s e m b l e s t r a t e g y p r o p o s e d i n d n d f s d n d f s l d l s n d f s l d l e v e n l e a d s t o a w o r s e p e r f o r m a n c e t h a n o n e f o r a s i n g l e t r e e 5 c o n c l u s i o n w e p r e s e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r e s t s a n o v e l l a b e l d i s t r i b u t i o n l e a r n i n g a l g o r i t h m i n s p i r e d b y d i f f e r e n t i a b l e d e c i s i o n t r e e s w e d e f i n e d a d i s t r i b u t i o n b a s e d l o s s f u n c t i o n f o r t h e f o r e s t s a n d f o u n d t h a t t h e l e a f n o d e p r e d i c t i o n s c a n b e o p t i m i z e d v i a v a r i a t i o n a l b o u n d i n g w h i c h e n a b l e s a l l t h e t r e e s a n d t h e f e a t u r e t h e y u s e t o b e l e a r n e d j o i n t l y i n a n e n d t o e n d m a n n e r e x p e r i m e n t a l r e s u l t s s h o w e d t h e s u p e r i o r i t y o f o u r a l g o r i t h m f o r s e v e r a l l d l t a s k s a n d a r e l a t e d c o m p u t e r v i s i o n a p p l i c a t i o n a n d v e r i f i e d o u r m o d e l h a s t h e a b i l i t y t o m o d e l a n y g e n e r a l f o r m o f l a b e l d i s t r i b u t i o n s a c k n o w l e d g e m e n t t h i s w o r k w a s s u p p o r t e d i n p a r t b y t h e n a t i o n a l n a t u r a l s c i e n c e f o u n d a t i o n o f c h i n a n o 6 1 6 7 2 3 3 6 i n p a r t b y xe2\\\\x80\\\\x9c c h e n g u a n g xe2\\\\x80\\\\x9d p r o j e c t s u p p o r t e d b y s h a n g h a i m u n i c i p a l e d u c a t i o n c o m m i s s i o n a n d s h a n g h a i e d u c a t i o n d e v e l o p m e n t f o u n d a t i o n n o 1 5 c g 4 3 a n d i n p a r t b y o n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e s 1 y a m i t a n d d g e m a n s h a p e q u a n t i z a t i o n a n d r e c o g n i t i o n w i t h r a n d o m i z e d t r e e s n e u r a l c o m p u t a t i o n 9 7 1 5 4 5 xe2\\\\x80\\\\x93 1 5 8 8 1 9 9 7 2 a l b e r g e r s d p i e t r a a n d v j d p i e t r a a m a x i m u m e n t r o p y a p p r o a c h t o n a t u r a l l a n g u a g e p r o c e s s i n g c o m p u t a t i o n a l l i n g u i s t i c s 2 2 1 3 9 xe2\\\\x80\\\\x93 7 1 1 9 9 6 3 l b r e i m a n r a n d o m f o r e s t s m a c h i n e l e a r n i n g 4 5 1 5 xe2\\\\x80\\\\x93 3 2 2 0 0 1 4 a c r i m i n i s i a n d j s h o t t o n d e c i s i o n f o r e s t s f o r c o m p u t e r v i s i o n a n d m e d i c a l i m a g e a n a l y s i s s p r i n g e r 2 0 1 3 5 b b g a o c x i n g c w x i e j w u a n d x g e n g d e e p l a b e l d i s t r i b u t i o n l e a r n i n g w i t h l a b e l a m b i g u i t y a r x i v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l a b e l d i s t r i b u t i o n l e a r n i n g i e e e t r a n s k n o w l d a t a e n g 2 8 7 1 7 3 4 xe2\\\\x80\\\\x93 1 7 4 8 2 0 1 6 9 x0c 7 x g e n g a n d p h o u p r e r e l e a s e p r e d i c t i o n o f c r o w d o p i n i o n o n m o v i e s b y l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o i j c a i p a g e s 3 5 1 1 xe2\\\\x80\\\\x93 3 5 1 7 2 0 1 5 8 x g e n g k s m i t h m i l e s a n d z z h o u f a c i a l a g e e s t i m a t i o n b y l e a r n i n g f r o m l a b e l d i s t r i b u t i o n s i n p r o c a a a i 2 0 1 0 9 x g e n g q w a n g a n d y x i a f a c i a l a g e e s t i m a t i o n b y a d a p t i v e l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o c i c p r p a g e s 4 4 6 5 xe2\\\\x80\\\\x93 4 4 7 0 2 0 1 4 1 0 x g e n g a n d y x i a h e a d p o s e e s t i m a t i o n b a s e d o n m u l t i v a r i a t e l a b e l d i s t r i b u t i o n i n p r o c c v p r p a g e s 1 8 3 7 xe2\\\\x80\\\\x93 1 8 4 2 2 0 1 4 1 1 x g e n g c y i n a n d z z h o u f a c i a l a g e e s t i m a t i o n b y l e a r n i n g f r o m l a b e l d i s t r i b u t i o n s i e e e t r a n s p a t t e r n a n a l m a c h i n t e l l 3 5 1 0 2 4 0 1 xe2\\\\x80\\\\x93 2 4 1 2 2 0 1 3 1 2 g g u o y f u c r d y e r a n d t s h u a n g i m a g e b a s e d h u m a n a g e e s t i m a t i o n b y m a n i f o l d l e a r n i n g a n d l o c a l l y a d j u s t e d r o b u s t r e g r e s s i o n i e e e t r a n s i m a g e p r o c e s s i n g 1 7 7 1 1 7 8 xe2\\\\x80\\\\x93 1 1 8 8 2 0 0 8 1 3 g g u o a n d g m u h u m a n a g e e s t i m a t i o n w h a t i s t h e i n f l u e n c e a c r o s s r a c e a n d g e n d e r i n c v p r w o r k s h o p s p a g e s 7 1 xe2\\\\x80\\\\x93 7 8 2 0 1 0 1 4 g g u o a n d c z h a n g a s t u d y o n c r o s s p o p u l a t i o n a g e e s t i m a t i o n i n p r o c c v p r p a g e s 4 2 5 7 xe2\\\\x80\\\\x93 4 2 6 3 2 0 1 4 1 5 z h e x l i z z h a n g f w u x g e n g y z h a n g m h y a n g a n d y z h u a n g d a t a d e p e n d e n t l a b e l d i s t r i b u t i o n l e a r n i n g f o r a g e e s t i m a t i o n i e e e t r a n s o n i m a g e p r o c e s s i n g 2 0 1 7 1 6 t k h o r a n d o m d e c i s i o n f o r e s t s i n p r o c i c d a r p a g e s 2 7 8 xe2\\\\x80\\\\x93 2 8 2 1 9 9 5 1 7 t k h o t h e r a n d o m s u b s p a c e m e t h o d f o r c o n s t r u c t i n g d e c i s i o n f o r e s t s i e e e t r a n s p a t t e r n a n a l m a c h i n t e l l 2 0 8 8 3 2 xe2\\\\x80\\\\x93 8 4 4 1 9 9 8 1 8 y j i a e s h e l h a m e r j d o n a h u e s k a r a y e v j l o n g r g i r s h i c k s g u a d a r r a m a a n d t d a r r e l l c a f f e c o n v o l u t i o n a l a r c h i t e c t u r e f o r f a s t f e a t u r e e m b e d d i n g a r x i v p r e p r i n t a r x i v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 m i j o r d a n z g h a h r a m a n i t s j a a k k o l a a n d l k s a u l a n i n t r o d u c t i o n t o v a r i a t i o n a l m e t h o d s f o r g r a p h i c a l m o d e l s m a c h i n e l e a r n i n g 3 7 2 1 8 3 xe2\\\\x80\\\\x93 2 3 3 1 9 9 9 2 0 p k o n t s c h i e d e r m f i t e r a u a c r i m i n i s i a n d s r b u l xc3\\\\xb2 d e e p n e u r a l d e c i s i o n f o r e s t s i n p r o c i c c v p a g e s 1 4 6 7 xe2\\\\x80\\\\x93 1 4 7 5 2 0 1 5 2 1 a k r i z h e v s k y i s u t s k e v e r a n d g e h i n t o n i m a g e n e t c l a s s i f i c a t i o n w i t h d e e p c o n v o l u t i o n a l n e u r a l n e t w o r k s i n p r o c n i p s p a g e s 1 1 0 6 xe2\\\\x80\\\\x93 1 1 1 4 2 0 1 2 2 2 a l a n i t i s c d r a g a n o v a a n d c c h r i s t o d o u l o u c o m p a r i n g d i f f e r e n t c l a s s i f i e r s f o r a u t o m a t i c a g e e s t i m a t i o n i e e e t r a n s o n c y b e r n e t i c s 3 4 1 6 2 1 xe2\\\\x80\\\\x93 6 2 8 2 0 0 4 2 3 o m p a r k h i a v e d a l d i a n d a z i s s e r m a n d e e p f a c e r e c o g n i t i o n i n p r o c b m v c p a g e s 4 1 1 xe2\\\\x80\\\\x93 4 1 1 2 2 0 1 5 2 4 k r i c a n e k a n d t t e s a f a y e m o r p h a l o n g i t u d i n a l i m a g e d a t a b a s e o f n o r m a l a d u l t a g e p r o g r e s s i o n i n p r o c f g p a g e s 3 4 1 xe2\\\\x80\\\\x93 3 4 5 2 0 0 6 2 5 j s h o t t o n a w f i t z g i b b o n m c o o k t s h a r p m f i n o c c h i o r m o o r e a k i p m a n a n d a b l a k e r e a l t i m e h u m a n p o s e r e c o g n i t i o n i n p a r t s f r o m s i n g l e d e p t h i m a g e s i n p r o c c v p r p a g e s 1 2 9 7 xe2\\\\x80\\\\x93 1 3 0 4 2 0 1 1 2 6 g t s o u m a k a s a n d i k a t a k i s m u l t i l a b e l c l a s s i f i c a t i o n a n o v e r v i e w i n t e r n a t i o n a l j o u r n a l o f d a t a w a r e h o u s i n g a n d m i n i n g 3 3 1 xe2\\\\x80\\\\x93 1 3 2 0 0 7 2 7 c x i n g x g e n g a n d h x u e l o g i s t i c b o o s t i n g r e g r e s s i o n f o r l a b e l d i s t r i b u t i o n l e a r n i n g i n p r o c c v p r p a g e s 4 4 8 9 xe2\\\\x80\\\\x93 4 4 9 7 2 0 1 6 2 8 x y a n g x g e n g a n d d z h o u s p a r s i t y c o n d i t i o n a l e n e r g y l a b e l d i s t r i b u t i o n l e a r n i n g f o r a g e e s t i m a t i o n i n p r o c i j c a i p a g e s 2 2 5 9 xe2\\\\x80\\\\x93 2 2 6 5 2 0 1 6 2 9 a l y u i l l e a n d a r a n g a r a j a n t h e c o n c a v e c o n v e x p r o c e d u r e n e u r a l c o m p u t a t i o n 1 5 4 9 1 5 xe2\\\\x80\\\\x93 9 3 6 2 0 0 3 3 0 y z h o u h x u e a n d x g e n g e m o t i o n d i s t r i b u t i o n r e c o g n i t i o n f r o m f a c i a l e x p r e s s i o n s i n p r o c m m p a g e s 1 2 4 7 xe2\\\\x80\\\\x93 1 2 5 0 2 0 1 5 1 0 x0c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.1\n",
      "XGBoost Accuracy on Test set -> 0.26\n",
      "RandomForest Accuracy on Test set -> 0.3\n",
      "DecisionTree Accuracy on Test set -> 0.26\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   l       b       e       l       d       r       b       u       n       l       e       r       n       n       g       f       r       e       w       e       s       h       e       n       1       2       k       z       h       1       y       l       u       g       u       1       a       l       n       y       u       l       l       e       2       k       e       l       b       r       r       f       s       p       e       c       l       f       b       e       r       o       p       c       n       o       p       c       l       a       c       c       e       n       e       w       r       k       s       h       n       g       h       i       n       u       e       f       r       a       v       n       c       e       c       u       n       c       n       n       d       s       c       e       n       c       e       s       c       h       l       f       c       u       n       c       n       n       i       n       f       r       n       e       n       g       n       e       e       r       n       g       s       h       n       g       h       u       n       v       e       r       2       d       e       p       r       e       n       f       c       p       u       e       r       s       c       e       n       c       e       j       h       n       h       p       k       n       u       n       v       e       r       r       x       v       1       7       0       2       0       6       0       8       6       v       4       c       l       g       1       6       o       c       2       0       1       7       1       h       e       n       w       e       1       2       3       1       z       h       k       1       2       0       6       g       l       l       u       n       0       l       n       l       u       l       l       e       g       l       c       a       b       r       c       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       g       e       n       e       r       l       l       e       r       n       n       g       f       r       e       w       r       k       w       h       c       h       g       n       n       n       n       c       e       r       b       u       n       v       e       r       e       f       l       b       e       l       r       h       e       r       h       n       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       c       u       r       r       e       n       l       d       l       e       h       h       v       e       e       h       e       r       r       e       r       c       e       u       p       n       n       h       e       e       x       p       r       e       n       f       r       f       h       e       l       b       e       l       r       b       u       n       r       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       t       h       p       p       e       r       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       d       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       b       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       h       c       h       h       v       e       e       v       e       r       l       v       n       g       e       1       d       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       l       e       f       n       e       p       r       e       c       n       2       t       h       e       l       e       r       n       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       f       r       e       e       n       b       l       n       g       l       l       h       e       r       e       e       b       e       l       e       r       n       e       j       n       l       n       h       w       h       n       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       p       r       e       c       n       w       h       c       h       g       u       r       n       e       e       r       c       e       c       r       e       e       f       h       e       l       f       u       n       c       n       c       n       b       e       e       r       v       e       b       v       r       n       l       b       u       n       n       g       t       h       e       e       f       f       e       c       v       e       n       e       f       h       e       p       r       p       e       l       d       l       f       v       e       r       f       e       n       e       v       e       r       l       l       d       l       k       n       c       p       u       e       r       v       n       p       p       l       c       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       d       l       e       h       1       i       n       r       u       c       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       6       1       1       l       e       r       n       n       g       f       r       e       w       r       k       e       l       w       h       p       r       b       l       e       f       l       b       e       l       b       g       u       u       n       l       k       e       n       g       l       e       l       b       e       l       l       e       r       n       n       g       s       l       l       n       u       l       l       b       e       l       l       e       r       n       n       g       m       l       l       2       6       w       h       c       h       u       e       n       n       n       c       e       g       n       e       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       l       d       l       l       e       r       n       n       g       h       e       r       e       l       v       e       p       r       n       c       e       f       e       c       h       l       b       e       l       n       v       l       v       e       n       h       e       e       c       r       p       n       f       n       n       n       c       e       e       r       b       u       n       v       e       r       h       e       e       f       l       b       e       l       s       u       c       h       l       e       r       n       n       g       r       e       g       u       b       l       e       f       r       n       r       e       l       w       r       l       p       r       b       l       e       w       h       c       h       h       v       e       l       b       e       l       b       g       u       a       n       e       x       p       l       e       f       c       l       g       e       e       n       8       e       v       e       n       h       u       n       c       n       n       p       r       e       c       h       e       p       r       e       c       e       g       e       f       r       n       g       l       e       f       c       l       g       e       t       h       e       h       h       e       p       e       r       n       p       r       b       b       l       n       n       e       g       e       g       r       u       p       n       l       e       l       k       e       l       b       e       n       n       h       e       r       h       e       n       c       e       r       e       n       u       r       l       g       n       r       b       u       n       f       g       e       l       b       e       l       e       c       h       f       c       l       g       e       f       g       1       n       e       f       u       n       g       n       g       l       e       g       e       l       b       e       l       a       n       h       e       r       e       x       p       l       e       v       e       r       n       g       p       r       e       c       n       7       m       n       f       u       v       e       r       e       v       e       w       w       e       b       e       u       c       h       n       e       f       l       x       i       m       d       b       n       d       u       b       n       p       r       v       e       c       r       w       p       n       n       f       r       e       c       h       v       e       p       e       c       f       e       b       h       e       r       b       u       n       f       r       n       g       c       l       l       e       c       e       f       r       h       e       r       u       e       r       f       g       1       b       i       f       e       c       u       l       p       r       e       c       e       l       p       r       e       c       u       c       h       r       n       g       r       b       u       n       f       r       e       v       e       r       v       e       b       e       f       r       e       r       e       l       e       e       v       e       p       r       u       c       e       r       c       n       r       e       u       c       e       h       e       r       n       v       e       e       n       r       k       n       h       e       u       e       n       c       e       c       n       b       e       e       r       c       h       e       w       h       c       h       v       e       w       c       h       m       n       l       d       l       e       h       u       e       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       n       l       e       r       n       b       p       z       n       g       n       e       n       e       r       g       f       u       n       c       n       b       e       n       h       e       e       l       8       1       1       2       8       6       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       e       g       h       f       f       c       u       l       n       r       e       p       r       e       e       n       n       g       x       u       r       e       r       b       u       n       s       e       h       e       r       l       d       l       e       h       e       x       e       n       h       e       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       g       b       b       n       g       n       u       p       p       r       v       e       c       r       r       e       g       r       e       n       e       l       w       h       l       b       e       l       r       b       u       n       7       2       7       w       h       c       h       v       k       n       g       h       u       p       n       b       u       h       v       e       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       h       e       n       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       1       c       n       f       e       r       e       n       c       e       n       n       e       u       r       l       i       n       f       r       n       p       r       c       e       n       g       s       e       n       i       p       s       2       0       1       7       l       n       g       b       e       c       h       c       a       u       s       a       x   0   c       f       g       u       r       e       1       t       h       e       r       e       l       w       r       l       w       h       c       h       r       e       u       b       l       e       b       e       e       l       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       e       e       f       c       l       g       e       u       n       l       r       b       u       n       b       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       u       l       l       r       b       u       n       i       n       h       p       p       e       r       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       d       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       2       0       e       x       e       n       n       g       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       e       l       w       h       h       e       l       d       l       k       h       w       v       n       g       e       o       n       e       h       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       h       e       l       e       f       n       e       p       r       e       c       n       w       h       c       h       v       k       n       g       r       n       g       u       p       n       n       h       e       f       r       f       h       e       l       b       e       l       r       b       u       n       t       h       e       e       c       n       h       h       e       p       l       n       e       p       r       e       e       r       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       w       h       c       h       e       n       b       l       e       c       b       n       n       f       r       e       e       l       e       r       n       n       g       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       r       e       e       b       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       v       e       r       g       e       n       c       e       k       l       b       e       w       e       e       n       h       e       g       r       u       n       r       u       h       l       b       e       l       r       b       u       n       n       h       e       r       b       u       n       p       r       e       c       e       b       h       e       r       e       e       b       f       x       n       g       p       l       n       e       w       e       h       w       h       h       e       p       z       n       f       l       e       f       n       e       p       r       e       c       n       n       z       e       h       e       l       f       u       n       c       n       f       h       e       r       e       e       c       n       b       e       r       e       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       n       w       h       c       h       h       e       r       g       n       l       l       f       u       n       c       n       b       e       n       z       e       g       e       e       r       v       e       l       r       e       p       l       c       e       b       e       c       r       e       n       g       e       q       u       e       n       c       e       f       u       p       p       e       r       b       u       n       f       l       l       w       n       g       h       p       z       n       r       e       g       w       e       e       r       v       e       c       r       e       e       e       r       v       e       f       u       n       c       n       u       p       e       h       e       l       e       f       n       e       p       r       e       c       n       t       l       e       r       n       f       r       e       w       e       v       e       r       g       e       h       e       l       e       f       l       l       h       e       n       v       u       l       r       e       e       b       e       h       e       l       f       r       h       e       f       r       e       n       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       i       n       h       w       h       e       p       l       n       e       p       r       e       e       r       f       l       l       h       e       n       v       u       l       r       e       e       c       n       b       e       l       e       r       n       e       j       n       l       o       u       r       l       d       l       f       c       n       b       e       u       e       h       l       l       w       n       l       n       e       e       l       n       c       n       l       b       e       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       e       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       c       n       b       e       l       n       e       r       r       n       f       r       n       n       e       e       p       n       e       w       r       k       r       e       p       e       c       v       e       l       f       g       2       l       l       u       r       e       k       e       c       h       c       h       r       f       u       r       l       d       l       f       w       h       e       r       e       f       r       e       c       n       f       w       r       e       e       h       w       n       w       e       v       e       r       f       h       e       e       f       f       e       c       v       e       n       e       f       u       r       e       l       n       e       v       e       r       l       l       d       l       k       u       c       h       c       r       w       p       n       n       p       r       e       c       n       n       v       e       n       e       e       p       r       e       c       n       b       e       n       h       u       n       g       e       n       e       w       e       l       l       n       e       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       d       l       e       h       t       h       e       l       b       e       l       r       b       u       n       f       r       h       e       e       k       n       c       l       u       e       b       h       u       n       l       r       b       u       n       e       g       h       e       g       e       r       b       u       n       n       f       g       1       n       x       u       r       e       r       b       u       n       h       e       r       n       g       r       b       u       n       n       v       e       n       f       g       1       b       t       h       e       u       p       e       r       r       f       u       r       e       l       n       b       h       f       h       e       v       e       r       f       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       f       g       u       r       e       2       i       l       l       u       r       n       f       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       t       h       e       p       c       r       c       l       e       e       n       e       h       e       u       p       u       u       n       f       h       e       f       u       n       c       n       f       p       r       e       e       r       z       e       b       x   c   e   \\\\   x   9   8       w       h       c       h       c       n       b       e       f       e       u       r       e       v       e       c       r       r       f       u       l       l       c       n       n       e       c       e       l       e       r       f       e       e       p       n       e       w       r       k       t       h       e       b       l       u       e       n       g       r       e       e       n       c       r       c       l       e       r       e       p       l       n       e       n       l       e       f       n       e       r       e       p       e       c       v       e       l       t       w       n       e       x       f       u       n       c       n       x   c   f   \\\\   x   9   5       1       n       x   c   f   \\\\   x   9   5       2       r       e       g       n       e       h       e       e       w       r       e       e       r       e       p       e       c       v       e       l       t       h       e       b       l       c       k       h       r       r       w       n       c       e       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       f       h       e       e       w       r       e       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       e       h       n       e       u       p       u       u       n       c       r       r       e       p       n       h       e       p       l       n       e       b       e       l       n       g       n       g       f       f       e       r       e       n       r       e       e       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       e       n       e       b       h       g       r       n       l       e       f       n       e       t       h       e       u       p       u       f       h       e       f       r       e       x       u       r       e       f       h       e       r       e       e       p       r       e       c       n       f       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       n       q       r       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       2       x   0   c       2       r       e       l       e       w       r       k       s       n       c       e       u       r       l       d       l       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       n       e       c       e       r       f       r       r       e       v       e       w       e       p       c       l       e       c       h       n       q       u       e       f       e       c       n       r       e       e       t       h       e       n       w       e       c       u       c       u       r       r       e       n       l       d       l       e       h       d       e       c       n       r       e       e       r       n       f       r       e       r       r       n       z       e       e       c       n       r       e       e       1       6       1       3       4       r       e       p       p       u       l       r       e       n       e       b       l       e       p       r       e       c       v       e       e       l       u       b       l       e       f       r       n       c       h       n       e       l       e       r       n       n       g       k       i       n       h       e       p       l       e       r       n       n       g       f       e       c       n       r       e       e       w       b       e       n       h       e       u       r       c       u       c       h       g       r       e       e       l       g       r       h       w       h       e       r       e       l       c       l       l       p       l       h       r       e       c       n       r       e       e       e       c       h       p       l       n       e       1       n       h       u       c       n       n       b       e       n       e       g       r       e       n       n       e       e       p       l       e       r       n       n       g       f       r       e       w       r       k       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       t       h       e       n       e       w       l       p       r       p       e       e       e       p       n       e       u       r       l       e       c       n       f       r       e       n       d       f       2       0       v       e       r       c       e       h       p       r       b       l       e       b       n       r       u       c       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       h       e       p       l       n       e       n       g       l       b       l       l       f       u       n       c       n       e       f       n       e       n       r       e       e       t       h       e       n       u       r       e       h       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       n       l       e       f       n       e       p       r       e       c       n       c       n       b       e       u       p       e       b       c       r       e       e       e       r       v       e       f       u       n       c       n       o       u       r       e       h       e       x       e       n       n       d       f       r       e       l       d       l       p       r       b       l       e       b       u       h       e       x       e       n       n       n       n       r       v       l       b       e       c       u       e       l       e       r       n       n       g       l       e       f       n       e       p       r       e       c       n       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       a       l       h       u       g       h       e       p       z       e       f       r       e       e       u       p       e       f       u       n       c       n       w       g       v       e       n       n       n       d       f       u       p       e       l       e       f       n       e       p       r       e       c       n       w       n       l       p       r       v       e       c       n       v       e       r       g       e       f       r       c       l       f       c       n       l       c       n       e       q       u       e       n       l       w       u       n       c       l       e       r       h       w       b       n       u       c       h       n       u       p       e       f       u       n       c       n       f       r       h       e       r       l       e       w       e       b       e       r       v       e       h       w       e       v       e       r       h       h       e       u       p       e       f       u       n       c       n       n       n       d       f       c       n       b       e       e       r       v       e       f       r       v       r       n       l       b       u       n       n       g       w       h       c       h       l       l       w       u       e       x       e       n       u       r       l       d       l       l       i       n       n       h       e       r       e       g       e       u       e       n       l       d       l       f       n       n       d       f       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       f       r       e       r       e       f       f       e       r       e       n       1       w       e       e       x       p       l       c       l       e       f       n       e       l       f       u       n       c       n       f       r       f       r       e       w       h       l       e       n       l       h       e       l       f       u       n       c       n       f       r       n       g       l       e       r       e       e       w       e       f       n       e       n       n       d       f       2       w       e       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       w       h       l       e       n       d       f       n       3       l       l       r       e       e       n       l       d       l       f       c       n       b       e       l       e       r       n       e       j       n       l       w       h       l       e       r       e       e       n       n       d       f       w       e       r       e       l       e       r       n       e       l       e       r       n       v       e       l       t       h       e       e       c       h       n       g       e       n       h       e       e       n       e       b       l       e       l       e       r       n       n       g       r       e       p       r       n       b       e       c       u       e       h       w       n       n       u       r       e       x       p       e       r       e       n       s       e       c       4       4       l       d       l       f       c       n       g       e       b       e       e       r       r       e       u       l       b       u       n       g       r       e       r       e       e       b       u       b       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       d       f       h       e       r       e       u       l       f       f       r       e       r       e       e       v       e       n       w       r       e       h       n       h       e       f       r       n       g       l       e       r       e       e       t       u       u       p       w       r       n       d       f       2       0       h       e       c       n       r       b       u       n       f       l       d       l       f       r       e       f       r       w       e       e       x       e       n       f       r       c       l       f       c       n       2       0       r       b       u       n       l       e       r       n       n       g       b       p       r       p       n       g       r       b       u       n       b       e       l       f       r       h       e       f       r       e       n       e       r       v       e       h       e       g       r       e       n       l       e       r       n       p       l       n       e       w       r       h       l       e       c       n       w       e       e       r       v       e       h       e       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       b       v       r       n       l       b       u       n       n       g       h       v       n       g       b       e       r       v       e       h       h       e       u       p       e       f       u       n       c       n       n       2       0       w       p       e       c       l       c       e       f       v       r       n       l       b       u       n       n       g       l       b       u       n       h       e       l       e       w       e       p       r       p       e       b       v       e       h       r       e       e       r       e       g       e       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       w       h       c       h       r       e       f       f       e       r       e       n       f       r       2       0       b       u       w       e       h       w       r       e       e       f       f       e       c       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       a       n       u       b       e       r       f       p       e       c       l       z       e       l       g       r       h       h       v       e       b       e       e       n       p       r       p       e       r       e       h       e       l       d       l       k       n       h       v       e       h       w       n       h       e       r       e       f       f       e       c       v       e       n       e       n       n       c       p       u       e       r       v       n       p       p       l       c       n       u       c       h       f       c       l       g       e       e       n       8       1       1       2       8       e       x       p       r       e       n       r       e       c       g       n       n       3       0       n       h       n       r       e       n       n       e       n       1       0       g       e       n       g       e       l       8       e       f       n       e       h       e       l       b       e       l       r       b       u       n       f       r       n       n       n       c       e       v       e       c       r       c       n       n       n       g       h       e       p       r       b       b       l       e       f       h       e       n       n       c       e       h       v       n       g       e       c       h       l       b       e       l       t       h       e       l       g       v       e       r       e       g       g       n       p       r       p       e       r       l       b       e       l       r       b       u       n       n       n       n       c       e       w       h       n       g       l       e       l       b       e       l       e       g       n       n       g       g       u       n       r       t       r       n       g       l       e       r       b       u       n       w       h       e       p       e       k       h       e       n       g       l       e       l       b       e       l       n       p       r       p       e       n       l       g       r       h       c       l       l       e       i       i       s       l       l       d       w       h       c       h       n       e       r       v       e       p       z       n       p       r       c       e       b       e       n       w       l       e       r       e       n       e       r       g       b       e       e       l       y       n       g       e       l       2       8       h       e       n       e       f       n       e       h       r       e       e       l       e       r       e       n       e       r       g       b       e       e       l       c       l       l       e       s       c       e       l       d       l       n       w       h       c       h       h       e       b       l       p       e       r       f       r       f       e       u       r       e       l       e       r       n       n       g       p       r       v       e       b       n       g       h       e       e       x       r       h       e       n       l       e       r       n       p       r       c       n       r       n       r       e       l       n       c       r       p       r       e       e       l       r       e       h       e       e       l       g       e       n       g       6       e       v       e       l       p       e       n       c       c       e       l       e       r       e       v       e       r       n       f       i       i       s       l       l       d       c       l       l       e       b       f       g       s       l       d       l       b       u       n       g       q       u       n       e       w       n       p       z       n       a       l       l       h       e       b       v       e       l       d       l       e       h       u       e       h       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       a       n       h       e       r       w       r       e       h       e       l       d       l       k       e       x       e       n       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       l       w       h       l       b       e       l       r       b       u       n       g       e       n       g       n       h       u       7       p       r       p       e       l       d       s       v       r       l       d       l       e       h       b       e       x       e       n       n       g       u       p       p       r       v       e       c       r       r       e       g       r       e       r       w       h       c       h       f       g       f       u       n       c       n       e       c       h       c       p       n       e       n       f       h       e       r       b       u       n       u       l       n       e       u       l       b       u       p       p       r       v       e       c       r       c       h       n       e       x       n       g       e       l       2       7       h       e       n       e       x       e       n       e       b       n       g       r       e       h       e       l       d       l       k       b       v       e       w       e       g       h       e       r       e       g       r       e       r       t       h       e       h       w       e       h       u       n       g       h       e       v       e       c       r       r       e       e       e       l       h       e       w       e       k       r       e       g       r       e       r       c       n       l       e       b       e       e       r       p       e       r       f       r       n       c       e       n       n       e       h       e       h       a       o       s       o       l       d       l       l       g       b       a       h       e       l       e       r       n       n       g       f       h       r       e       e       e       l       b       e       n       l       c       l       l       p       l       h       r       p       r       n       f       u       n       c       n       e       c       h       p       l       n       e       a       o       s       o       l       d       l       l       g       b       u       n       b       l       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       x       e       n       n       g       c       u       r       r       e       n       e       e       p       l       e       r       n       n       g       l       g       r       h       3       x   0   c       r       e       h       e       l       d       l       k       n       n       e       r       e       n       g       p       c       b       u       h       e       e       x       n       g       u       c       h       e       h       c       l       l       e       d       l       d       l       5       l       l       f       c       u       e       n       x       u       e       n       r       p       e       l       b       e       l       d       l       o       u       r       e       h       l       d       l       f       e       x       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       r       e       l       d       l       k       n       w       h       c       h       h       e       p       r       e       c       e       l       b       e       l       r       b       u       n       f       r       p       l       e       c       n       b       e       e       x       p       r       e       e       b       l       n       e       r       c       b       n       n       f       h       e       l       b       e       l       r       b       u       n       f       h       e       r       n       n       g       n       h       u       h       v       e       n       r       e       r       c       n       n       h       e       r       b       u       n       e       g       n       r       e       q       u       r       e       e       n       f       h       e       x       u       e       n       r       p       e       l       i       n       n       h       n       k       h       e       n       r       u       c       n       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       l       d       l       f       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       l       b       e       l       d       r       b       u       n       l       e       r       n       n       g       f       r       e       a       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       w       e       f       r       n       r       u       c       e       h       w       l       e       r       n       n       g       l       e       e       c       n       r       e       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       h       e       n       e       c       r       b       e       h       e       l       e       r       n       n       g       f       f       r       e       3       1       p       r       b       l       e       f       r       u       l       n       l       e       x       r       e       n       e       h       e       n       p       u       p       c       e       n       y       1       2       c       e       n       e       h       e       c       p       l       e       e       e       f       l       b       e       l       w       h       e       r       e       c       h       e       n       u       b       e       r       f       p       b       l       e       l       b       e       l       v       l       u       e       w       e       c       n       e       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       p       r       b       l       e       w       h       e       r       e       f       r       e       c       h       n       p       u       p       l       e       x       x   e   2   \\\\   x   8   8   \\\\   x   8   8       x       h       e       r       e       l       b       e       l       r       b       u       n       x       1       x       2       x       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       r       c       h       e       r       e       x       c       e       x       p       r       e       e       h       e       p       r       b       b       l       f       h       e       p       l       e       x       h       v       n       g       h       e       c       h       l       b       e       l       c       n       h       u       h       h       e       p       c       c       n       r       n       h       x       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       n       c       1       x       c       1       t       h       e       g       l       f       h       e       l       d       l       p       r       b       l       e       l       e       r       n       p       p       n       g       f       u       n       c       n       g       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       b       e       w       e       e       n       n       n       p       u       p       l       e       x       n       c       r       r       e       p       n       n       g       l       b       e       l       r       b       u       n       h       e       r       e       w       e       w       n       l       e       r       n       h       e       p       p       n       g       f       u       n       c       n       g       x       b       e       c       n       r       e       e       b       e       e       l       t       a       e       c       n       r       e       e       c       n       f       e       f       p       l       n       e       n       n       e       f       l       e       f       n       e       l       e       c       h       p       l       n       e       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       e       f       n       e       p       l       f       u       n       c       n       n       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       0       1       p       r       e       e       r       z       e       b       x   c   e   \\\\   x   9   8       e       e       r       n       e       w       h       e       h       e       r       p       l       e       e       n       h       e       l       e       f       r       r       g       h       u       b       r       e       e       e       c       h       l       e       f       n       e       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       h       l       r       b       u       n       q       q       1       q       2       q       c       p       c       v       e       r       y       e       q       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       n       c       1       q       c       1       t       b       u       l       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       f       l       l       w       n       g       2       0       w       e       u       e       p       r       b       b       l       c       p       l       f       u       n       c       n       n       x       x   c   e   \\\\   x   9   8       x   c   f   \\\\   x   8   3       f       x   c   f   \\\\   x   9   5       n       x       x   c   e   \\\\   x   9   8       w       h       e       r       e       x   c   f   \\\\   x   8   3       x   c   2   \\\\   x   b   7       g       f       u       n       c       n       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       n       n       e       x       f       u       n       c       n       b       r       n       g       h       e       x   c   f   \\\\   x   9   5       n       h       u       p       u       f       f       u       n       c       n       f       x       x   c   e   \\\\   x   9   8       n       c       r       r       e       p       n       e       n       c       e       w       h       p       l       n       e       n       n       f       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       r       m       r       e       l       v       l       u       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       e       p       e       n       n       g       n       h       e       p       l       e       x       n       h       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       n       c       n       k       e       n       f       r       f       r       p       l       e       f       r       c       n       b       e       l       n       e       r       r       n       f       r       n       f       x       w       h       e       r       e       x   c   e   \\\\   x   9   8       h       e       r       n       f       r       n       r       x       f       r       c       p       l       e       x       f       r       c       n       b       e       e       e       p       n       e       w       r       k       p       e       r       f       r       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       h       e       n       x   c   e   \\\\   x   9   8       h       e       n       e       w       r       k       p       r       e       e       r       t       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       c       e       b       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       h       r       n       l       g       e       n       e       r       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       e       w       h       c       h       u       p       u       u       n       f       r       x   e   2   \\\\   x   8   0   \\\\   x   9   c       f       x   e   2   \\\\   x   8   0   \\\\   x   9   d       r       e       u       e       f       r       c       n       r       u       c       n       g       r       e       e       e       e       r       n       e       r       n       l       a       n       e       x       p       l       e       e       n       r       e       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       h       w       n       n       f       g       2       t       h       e       n       h       e       p       r       b       b       l       f       h       e       p       l       e       x       f       l       l       n       g       n       l       e       f       n       e       g       v       e       n       b       y       l       r       p       x       x   c   e   \\\\   x   9   8       n       x       x   c   e   \\\\   x   9   8       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       n       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       x       x   c   e   \\\\   x   9   8       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       n       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       w       h       e       r       e       1       x   c   2   \\\\   x   b   7       n       n       c       r       f       u       n       c       n       n       l       l       n       n       l       r       n       e       n       e       h       e       e       f       l       e       f       n       e       h       e       l       b       h       e       l       e       f       n       r       g       h       u       b       r       e       e       f       n       e       n       t       n       l       n       t       n       r       r       e       p       e       c       v       e       l       t       h       e       u       p       u       f       h       e       r       e       e       t       w       r       x       e       h       e       p       p       n       g       f       u       n       c       n       g       e       f       n       e       b       x       g       x       x   c   e   \\\\   x   9   8       t       p       x       x   c   e   \\\\   x   9   8       q       2       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       3       2       t       r       e       e       o       p       z       n       g       v       e       n       r       n       n       g       e       s       x       n       1       u       r       g       l       l       e       r       n       e       c       n       r       e       e       t       e       c       r       b       e       n       s       e       c       3       1       w       h       c       h       c       n       u       p       u       r       b       u       n       g       x       x   c   e   \\\\   x   9   8       t       l       r       f       r       e       c       h       p       l       e       x       t       h       e       n       r       g       h       f       r       w       r       w       n       z       e       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       k       l       v       e       r       g       e       n       c       e       b       e       w       e       e       n       e       c       h       g       x       x   c   e   \\\\   x   9   8       t       n       r       e       q       u       v       l       e       n       l       n       z       e       h       e       f       l       l       w       n       g       c       r       e       n       r       p       l       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       c       n       c       x   1   0       x       x   1   1       1       x       x       c       1       x       x       c       x       l       g       g       c       x       x   c   e   \\\\   x   9   8       t       x   e   2   \\\\   x   8   8   \\\\   x   9   2       x       l       g       p       x       x   c   e   \\\\   x   9   8       q       c       3       n       1       c       1       n       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       4       x   0   c       w       h       e       r       e       q       e       n       e       h       e       r       b       u       n       h       e       l       b       l       l       h       e       l       e       f       n       e       l       n       g       c       x       x   c   e   \\\\   x   9   8       t       h       e       c       h       u       p       u       u       n       f       g       x       x   c   e   \\\\   x   9   8       t       l       e       r       n       n       g       h       e       r       e       e       t       r       e       q       u       r       e       h       e       e       n       f       w       p       r       e       e       r       1       h       e       p       l       n       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       n       2       h       e       r       b       u       n       q       h       e       l       b       h       e       l       e       f       n       e       t       h       e       b       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   7       q       x   e   2   \\\\   x   8   8   \\\\   x   9   7       r       e       e       e       r       n       e       b       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   7       q       x   e   2   \\\\   x   8   8   \\\\   x   9   7       r       g       n       r       q       x   c   e   \\\\   x   9   8       s       4       x   c   e   \\\\   x   9   8       q       t       l       v       e       e       q       n       4       w       e       c       n       e       r       n       l       e       r       n       n       g       p       z       n       r       e       g       f       r       w       e       f       x       q       n       p       z       e       x   c   e   \\\\   x   9   8       t       h       e       n       w       e       f       x       x   c   e   \\\\   x   9   8       n       p       z       e       q       t       h       e       e       w       l       e       r       n       n       g       e       p       r       e       l       e       r       n       v       e       l       p       e       r       f       r       e       u       n       l       c       n       v       e       r       g       e       n       c       e       r       x       u       n       u       b       e       r       f       e       r       n       r       e       c       h       e       e       f       n       e       n       h       e       e       x       p       e       r       e       n       3       2       1       l       e       r       n       n       g       s       p       l       n       e       i       n       h       e       c       n       w       e       e       c       r       b       e       h       w       l       e       r       n       h       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       f       r       p       l       n       e       w       h       e       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       r       e       f       x       e       w       e       c       p       u       e       h       e       g       r       e       n       f       h       e       l       r       q       x   c   e   \\\\   x   9   8       s       w       r       x   c   e   \\\\   x   9   8       b       h       e       c       h       n       r       u       l       e       n       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       x       x       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       x   c   e   \\\\   x   9   8       5       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       w       h       e       r       e       n       l       h       e       f       r       e       r       e       p       e       n       n       h       e       r       e       e       n       h       e       e       c       n       e       r       e       p       e       n       n       h       e       p       e       c       f       c       p       e       f       h       e       f       u       n       c       n       f       x   c   f   \\\\   x   9   5       n       t       h       e       f       r       e       r       g       v       e       n       b       c       x   0   1       g       c       x       x   c   e   \\\\   x   9   8       t       n       l       x   1   1       g       c       x       x   c   e   \\\\   x   9   8       t       n       r       1       x       c       x   1   0       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       x       n       x       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       x       x   c   e   \\\\   x   9   8       6       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       x   c   e   \\\\   x   9   8       n       c       1       g       c       x       x   c   e   \\\\   x   9   8       t       g       c       x       x   c   e   \\\\   x   9   8       t       p       p       w       h       e       r       e       g       c       x       x   c   e   \\\\   x   9   8       t       n       l       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       l       n       p       x       x   c   e   \\\\   x   9   8       q       c       n       g       c       x       x   c   e   \\\\   x   9   8       t       n       r       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       r       n       p       x       x   c   e   \\\\   x   9   8       q       c       n       e       h       l       e       t       n       b       e       h       e       r       e       e       r       e       h       e       n       e       n       h       e       n       w       e       h       v       e       g       c       x       x   c   e   \\\\   x   9   8       t       n       g       c       x       x   c   e   \\\\   x   9   8       t       n       l       g       c       x       x   c   e   \\\\   x   9   8       t       n       r       t       h       e       n       h       e       g       r       e       n       c       p       u       n       n       e       q       n       6       c       n       b       e       r       e       h       e       l       e       f       n       e       n       c       r       r       e       u       n       b       u       p       n       n       e       r       t       h       u       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       n       r       b       c       k       p       r       p       g       n       3       2       2       l       e       r       n       n       g       l       e       f       n       e       n       w       f       x       n       g       h       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       w       e       h       w       h       w       l       e       r       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       w       h       c       h       c       n       r       n       e       p       z       n       p       r       b       l       e       n       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   8   0       q       c       x       q       c       1       7       c       1       h       e       r       e       w       e       p       r       p       e       r       e       h       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       w       h       c       h       l       e       e       p       z       e       f       r       e       e       n       f       c       n       v       e       r       g       e       u       p       e       r       u       l       e       f       r       q       i       n       v       r       n       l       b       u       n       n       g       n       r       g       n       l       b       j       e       c       v       e       f       u       n       c       n       b       e       n       z       e       g       e       r       e       p       l       c       e       b       b       u       n       n       n       e       r       v       e       n       n       e       r       a       u       p       p       e       r       b       u       n       f       r       h       e       l       f       u       n       c       n       r       q       x   c   e   \\\\   x   9   8       s       c       n       b       e       b       n       e       b       j       e       n       e       n       x   e   2   \\\\   x   8   0   \\\\   x   9   9       n       e       q       u       l       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       c       x   1   0       x       x   1   1       1       x       x       c       x       l       g       p       x       x   c   e   \\\\   x   9   8       q       c       n       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       x   e   2   \\\\   x   8   9   \\\\   x   a   4       x   e   2   \\\\   x   8   8   \\\\   x   9   2       w       h       e       r       e       x   c   e   \\\\   x   b   e       q       c       x       1       n       n       x       c       x       1       c       1       p       x       x   c   e   \\\\   x   9   8       q       c       g       c       x       x   c   e   \\\\   x   9   8       t       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       x   e   2   \\\\   x   8   8   \\\\   x   9   2       x       c       x       x   c   e   \\\\   x   b   e       q       x   c   c   \\\\   x   8   4       c       x       l       g       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       x   1   0       p       x       x   c   e   \\\\   x   9   8       q       x   1   1       c       x   c   e   \\\\   x   b   e       q       x   c   c   \\\\   x   8   4       c       x       8       w       e       e       f       n       e       n       c       x   1   0       p       x       x   c   e   \\\\   x   9   8       q       x   1   1       1       x       x       c       x       c       x       x   c   e   \\\\   x   b   e       q       x   c   c   \\\\   x   8   4       c       x       l       g       n       1       c       1       x   c   e   \\\\   x   b   e       q       x   c   c   \\\\   x   8   4       c       x       9       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       t       h       e       n       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       n       u       p       p       e       r       b       u       n       f       r       r       q       x   c   e   \\\\   x   9   8       s       w       h       c       h       h       h       e       p       r       p       e       r       h       f       r       n       q       n       q       x   c   c   \\\\   x   8   4       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       x   e   2   \\\\   x   8   9   \\\\   x   a   5       r       q       x   c   e   \\\\   x   9   8       s       n       x   c   f   \\\\   x   8   6       q       q       r       q       x   c   e   \\\\   x   9   8       s       a       u       e       h       w       e       r       e       p       n       q       c       r       r       e       p       n       n       g       h       e       h       e       r       n       h       e       n       x   c   f   \\\\   x   8   6       q       q       n       u       p       p       e       r       b       u       n       f       r       r       q       x   c   e   \\\\   x   9   8       s       i       n       h       e       n       e       x       e       r       n       q       1       c       h       e       n       u       c       h       h       x   c   f   \\\\   x   8   6       q       1       q       x   e   2   \\\\   x   8   9   \\\\   x   a   4       r       q       x   c   e   \\\\   x   9   8       s       w       h       c       h       p       l       e       r       q       1       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   9   \\\\   x   a   4       r       q       x   c   e   \\\\   x   9   8       s       5       x   0   c       c       n       e       q       u       e       n       l       w       e       c       n       n       z       e       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       n       e       f       r       q       x   c   e   \\\\   x   9   8       s       f       e       r       e       n       u       r       n       g       h       r       q       x   c   e   \\\\   x   9   8       s       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       e       q       x   c   c   \\\\   x   8   4       q       s       w       e       h       v       e       q       1       r       g       n       x   c   f   \\\\   x   8   6       q       q       x   e   2   \\\\   x   8   8   \\\\   x   8   0       q       c       x       q       c       1       1       0       c       1       w       h       c       h       l       e       n       z       n       g       h       e       l       g       r       n       g       n       e       f       n       e       b       x   c   f   \\\\   x   9   5       q       q       x   c   f   \\\\   x   8   6       q       q       x       x   c   e   \\\\   x   b   b       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       w       h       e       r       e       x   c   e   \\\\   x   b   b       h       e       l       g       r       n       g       e       u       l       p       l       e       r       b       e       n       g       x   c   e   \\\\   x   b   b       1       n       e       h       q       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       n       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       t       h       e       r       n       g       0       r       b       u       n       q       c       c       1       3       3       q       c       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       1       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   f   \\\\   x   9   5       q       q       x   e   2   \\\\   x   8   8   \\\\   x   8   2       q       c       n       c       1       x       x       c       1       x   c   e   \\\\   x   b   e       q       c       x       n       q       c       n       1       c       1       x       f       e       h       q       c       c       x       0       w       e       h       v       e       p       n       c       x       x   c   e   \\\\   x   b   e       q       c       x       p       c       1       p       n       c       x   c   e   \\\\   x   b   e       q       x       x       c       1       1       c       1       2       1       1       e       q       n       1       2       h       e       u       p       e       c       h       e       e       f       r       c       1       q       c       0       p       n       q       c       n       b       e       p       l       n       l       z       e       b       h       e       u       n       f       r       p       c       l       e       r       n       n       g       f       r       e       a       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       f       t       1       t       k       i       n       h       e       r       n       n       g       g       e       l       l       r       e       e       n       h       e       f       r       e       f       u       e       h       e       e       p       r       e       e       r       x   c   e   \\\\   x   9   8       f       r       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       b       u       c       r       r       e       p       n       f       f       e       r       e       n       u       p       u       u       n       f       f       g       n       e       b       x   c   f   \\\\   x   9   5       e       e       f       g       2       b       u       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       t       h       e       l       f       u       n       c       n       f       r       f       r       e       g       v       e       n       b       v       e       r       g       n       g       h       e       l       f       u       n       c       n       f       r       l       l       n       v       u       l       r       e       e       p       k       1       r       f       k       k       1       r       t       k       w       h       e       r       e       r       t       k       h       e       l       f       u       n       c       n       f       r       r       e       e       t       k       e       f       n       e       b       e       q       n       3       t       l       e       r       n       x   c   e   \\\\   x   9   8       b       f       x       n       g       h       e       l       e       f       n       e       p       r       e       c       n       q       f       l       l       h       e       r       e       e       n       h       e       f       r       e       f       b       e       n       h       e       e       r       v       n       n       s       e       c       3       2       n       r       e       f       e       r       r       n       g       f       g       2       w       e       h       v       e       n       k       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       k       n       x       x   c   e   \\\\   x   9   8       1       x       x       x       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       f       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       t       k       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       k       1       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       k       n       x       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       1       3       k       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       k       w       h       e       r       e       n       k       n       x   c   f   \\\\   x   9   5       k       x   c   2   \\\\   x   b   7       r       e       h       e       p       l       n       e       e       n       h       e       n       e       x       f       u       n       c       n       f       t       k       r       e       p       e       c       v       e       l       n       e       h       h       e       n       e       x       f       u       n       c       n       x   c   f   \\\\   x   9   5       k       x   c   2   \\\\   x   b   7       f       r       e       c       h       r       e       e       r       n       l       g       n       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       n       h       u       p       l       n       e       c       r       r       e       p       n       u       b       e       f       u       p       u       u       n       f       f       t       h       r       e       g       l       r       h       e       r       n       u       b       p       c       e       e       h       1       7       w       h       c       h       n       c       r       e       e       h       e       r       n       n       e       n       r       n       n       g       r       e       u       c       e       h       e       r       k       f       v       e       r       f       n       g       a       f       r       q       n       c       e       e       c       h       r       e       e       n       h       e       f       r       e       f       h       w       n       l       e       f       n       e       p       r       e       c       n       q       w       e       c       n       u       p       e       h       e       n       e       p       e       n       e       n       l       b       e       q       n       1       2       g       v       e       n       b       x   c   e   \\\\   x   9   8       f       r       p       l       e       e       n       n       l       c       n       v       e       n       e       n       c       e       w       e       n       c       n       u       c       h       u       p       e       c       h       e       e       n       h       e       w       h       l       e       e       s       b       u       n       e       f       n       b       c       h       e       b       t       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       d       l       f       h       w       n       n       a       l       g       r       h       1       a       l       g       r       h       1       t       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       d       l       f       r       e       q       u       r       e       s       r       n       n       g       e       n       b       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       q       i       n       l       z       e       x   c   e   \\\\   x   9   8       r       n       l       n       q       u       n       f       r       l       e       b       x   e   2   \\\\   x   8   8   \\\\   x   8   5       w       h       l       e       n       c       n       v       e       r       g       e       w       h       l       e       b       n       b       r       n       l       e       l       e       c       n       b       c       h       b       f       r       s       u       p       e       s       x   c   e   \\\\   x   9   8       b       c       p       u       n       g       g       r       e       n       e       q       n       1       3       n       b       b       b       b       e       n       w       h       l       e       u       p       e       q       b       e       r       n       g       e       q       n       1       2       n       b       b       x   e   2   \\\\   x   8   8   \\\\   x   8   5       e       n       w       h       l       e       i       n       h       e       e       n       g       g       e       h       e       u       p       u       f       h       e       f       r       e       f       g       v       e       n       b       v       e       r       g       n       g       h       e       p       r       e       c       n       f       r       l       l       h       e       p       k       1       n       v       u       l       r       e       e       g       x       x   c   e   \\\\   x   9   8       f       k       k       1       g       x       x   c   e   \\\\   x   9   8       t       k       6       x   0   c       4       e       x       p       e       r       e       n       l       r       e       u       l       o       u       r       r       e       l       z       n       f       l       d       l       f       b       e       n       x   e   2   \\\\   x   8   0   \\\\   x   9   c       c       f       f       e       x   e   2   \\\\   x   8   0   \\\\   x   9   d       1       8       i       u       l       r       n       p       l       e       e       n       e       n       r       n       e       u       r       l       n       e       w       r       k       l       e       r       w       e       c       n       e       h       e       r       u       e       h       l       l       w       n       l       n       e       e       l       l       d       l       f       r       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       l       d       l       f       w       e       e       v       l       u       e       l       d       l       f       n       f       f       e       r       e       n       l       d       l       k       n       c       p       r       e       w       h       h       e       r       n       l       n       e       l       d       l       e       h       a       l       d       l       f       c       n       b       e       l       e       r       n       e       f       r       r       w       g       e       n       n       e       n       e       n       n       n       e       r       w       e       v       e       r       f       l       d       l       f       n       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       t       h       e       e       f       u       l       e       n       g       f       r       h       e       p       r       e       e       r       f       u       r       f       r       e       r       e       r       e       e       n       u       b       e       r       5       r       e       e       e       p       h       7       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       6       4       e       r       n       e       u       p       e       l       e       f       n       e       p       r       e       c       n       2       0       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       l       e       f       n       e       p       r       e       c       n       1       0       0       x       u       e       r       n       2       5       0       0       0       4       1       c       p       r       n       f       l       d       l       f       s       n       l       n       e       l       d       l       m       e       h       w       e       c       p       r       e       u       r       h       l       l       w       e       l       l       d       l       f       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       d       l       e       h       f       r       l       d       l       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x       x   c   e   \\\\   x   9   8       l       n       e       r       r       n       f       r       n       f       x       e       h       e       h       u       p       u       u       n       f       x       x   c   e   \\\\   x   b   8       x   c   e   \\\\   x   b   8       x       w       h       e       r       e       x   c   e   \\\\   x   b   8       h       e       h       c       l       u       n       f       h       e       r       n       f       r       n       r       x       x   c   e   \\\\   x   9   8       w       e       u       e       3       p       p       u       l       r       l       d       l       e       n       6       m       v       e       h       u       n       g       e       n       e       n       n       u       r       l       s       c       e       n       e       1       t       h       e       p       l       e       n       h       e       e       3       e       r       e       r       e       p       r       e       e       n       e       b       n       u       e       r       c       l       e       c       r       p       r       n       h       e       g       r       u       n       r       u       h       f       r       h       e       r       e       h       e       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       h       e       e       e       r       b       u       n       r       e       l       e       h       u       n       g       e       n       e       n       l       b       e       l       r       b       u       n       n       c       e       n       e       u       c       h       p       l       n       k       n       c       l       u       r       e       p       e       c       v       e       l       t       h       e       l       b       e       l       r       b       u       n       f       h       e       e       3       e       r       e       x       u       r       e       r       b       u       n       u       c       h       h       e       r       n       g       r       b       u       n       h       w       n       n       f       g       1       b       f       l       l       w       n       g       7       2       7       w       e       u       e       6       e       u       r       e       e       v       l       u       e       h       e       p       e       r       f       r       n       c       e       f       l       d       l       e       h       w       h       c       h       c       p       u       e       h       e       v       e       r       g       e       l       r       n       c       e       b       e       w       e       e       n       h       e       p       r       e       c       e       r       n       g       r       b       u       n       n       h       e       r       e       l       r       n       g       r       b       u       n       n       c       l       u       n       g       4       n       c       e       e       u       r       e       k       l       e       u       c       l       e       n       s       x   c   f   \\\\   x   8   6       r       e       n       e       n       s       q       u       r       e       x   c   f   \\\\   x   8   7       2       n       w       l       r       e       u       r       e       f       e       l       i       n       e       r       e       c       n       w       e       e       v       l       u       e       u       r       h       l       l       w       e       l       l       d       l       f       n       h       e       e       3       e       n       c       p       r       e       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       d       l       e       h       t       h       e       r       e       u       l       f       l       d       l       f       n       h       e       c       p       e       r       r       e       u       r       z       e       n       t       b       l       e       1       f       r       m       v       e       w       e       q       u       e       h       e       r       e       u       l       r       e       p       r       e       n       2       7       h       e       c       e       f       2       7       n       p       u       b       l       c       l       v       l       b       l       e       f       r       h       e       r       e       u       l       f       h       e       h       e       r       w       w       e       r       u       n       c       e       h       h       e       u       h       r       h       e       v       l       b       l       e       i       n       l       l       c       e       f       l       l       w       n       g       2       7       6       w       e       p       l       e       c       h       e       n       1       0       f       x       e       f       l       n       n       r       e       n       f       l       c       r       v       l       n       w       h       c       h       r       e       p       r       e       e       n       h       e       r       e       u       l       b       x   e   2   \\\\   x   8   0   \\\\   x   9   c       e       n       x   c   2   \\\\   x   b   1       n       r       e       v       n       x   e   2   \\\\   x   8   0   \\\\   x   9   d       n       e       r       l       e       h       w       r       n       n       g       n       e       n       g       g       e       v       e       a       c       n       b       e       e       e       n       f       r       t       b       l       e       1       l       d       l       f       p       e       r       f       r       b       e       n       l       l       f       h       e       x       e       u       r       e       t       b       l       e       1       c       p       r       n       r       e       u       l       n       h       r       e       e       l       d       l       e       6       x   e   2   \\\\   x   8   0   \\\\   x   9   c       x   e   2   \\\\   x   8   6   \\\\   x   9   1       x   e   2   \\\\   x   8   0   \\\\   x   9   d       n       x   e   2   \\\\   x   8   0   \\\\   x   9   c       x   e   2   \\\\   x   8   6   \\\\   x   9   3       x   e   2   \\\\   x   8   0   \\\\   x   9   d       n       c       e       h       e       l       r       g       e       r       n       h       e       l       l       e       r       h       e       b       e       e       r       r       e       p       e       c       v       e       l       d       e       m       e       h       k       l       x   e   2   \\\\   x   8   6   \\\\   x   9   3       e       u       c       l       e       n       x   e   2   \\\\   x   8   6   \\\\   x   9   3       s       x   c   f   \\\\   x   8   6       r       e       n       e       n       x   e   2   \\\\   x   8   6   \\\\   x   9   3       s       q       u       r       e       x   c   f   \\\\   x   8   7       2       x   e   2   \\\\   x   8   6   \\\\   x   9   3       f       e       l       x   e   2   \\\\   x   8   6   \\\\   x   9   1       i       n       e       r       e       c       n       x   e   2   \\\\   x   8   6   \\\\   x   9   1       m       v       e       l       d       l       f       u       r       a       o       s       o       l       d       l       g       b       2       7       l       d       l       g       b       2       7       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       0       7       3       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       8       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       0       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       2       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       9       9       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       2       9       x   c   2   \\\\   x   b   1       0       0       0       7       0       1       3       3       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       9       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       8       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       6       7       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       8       7       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       3       0       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       2       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       6       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       8       3       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       7       0       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       8       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       0       8       8       x   c   2   \\\\   x   b   1       0       0       0       3       0       0       8       8       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       2       0       x   c   2   \\\\   x   b   1       0       0       0       5       0       9       8       1       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       8       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       4       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       6       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       8       7       0       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       8       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       4       x   c   2   \\\\   x   b   1       0       0       0       4       0       8       3       6       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       1       7       x   c   2   \\\\   x   b   1       0       0       0       4       l       d       l       f       u       r       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       2       2       8       x   c   2   \\\\   x   b   1       0       0       0       6       0       2       4       5       x   c   2   \\\\   x   b   1       0       0       1       9       0       2       3       1       x   c   2   \\\\   x   b   1       0       0       2       1       0       2       3       9       x   c   2   \\\\   x   b   1       0       0       1       8       0       0       8       5       x   c   2   \\\\   x   b   1       0       0       0       2       0       0       9       9       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       7       6       x   c   2   \\\\   x   b   1       0       0       0       6       0       0       8       9       x   c   2   \\\\   x   b   1       0       0       0       6       0       2       1       2       x   c   2   \\\\   x   b   1       0       0       0       2       0       2       2       9       x   c   2   \\\\   x   b   1       0       0       1       5       0       2       3       1       x   c   2   \\\\   x   b   1       0       0       1       2       0       2       5       3       x   c   2   \\\\   x   b   1       0       0       0       9       0       1       7       9       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       8       9       x   c   2   \\\\   x   b   1       0       0       2       1       0       2       1       1       x   c   2   \\\\   x   b   1       0       0       1       8       0       2       0       5       x   c   2   \\\\   x   b   1       0       0       1       2       0       9       4       8       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       4       0       x   c   2   \\\\   x   b   1       0       0       0       6       0       9       3       8       x   c   2   \\\\   x   b   1       0       0       0       8       0       9       4       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       7       8       8       x   c   2   \\\\   x   b   1       0       0       0       2       0       7       7       1       x   c   2   \\\\   x   b   1       0       0       1       5       0       7       6       9       x   c   2   \\\\   x   b   1       0       0       1       2       0       7       4       7       x   c   2   \\\\   x   b   1       0       0       0       9       l       d       l       f       u       r       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       5       3       4       x   c   2   \\\\   x   b   1       0       0       1       3       0       8       5       2       x   c   2   \\\\   x   b   1       0       0       2       3       0       8       5       6       x   c   2   \\\\   x   b   1       0       0       6       1       0       8       7       9       x   c   2   \\\\   x   b   1       0       0       2       3       0       3       1       7       x   c   2   \\\\   x   b   1       0       0       1       4       0       5       1       1       x   c   2   \\\\   x   b   1       0       0       2       1       0       4       7       5       x   c   2   \\\\   x   b   1       0       0       2       9       0       4       5       8       x   c   2   \\\\   x   b   1       0       0       1       4       0       3       3       6       x   c   2   \\\\   x   b   1       0       0       1       0       0       4       9       2       x   c   2   \\\\   x   b   1       0       0       1       6       0       5       0       8       x   c   2   \\\\   x   b   1       0       0       2       6       0       5       3       9       x   c   2   \\\\   x   b   1       0       0       1       1       0       4       4       8       x   c   2   \\\\   x   b   1       0       0       1       7       0       5       9       5       x   c   2   \\\\   x   b   1       0       0       2       6       0       7       1       6       x   c   2   \\\\   x   b   1       0       0       4       1       0       7       9       2       x   c   2   \\\\   x   b   1       0       0       1       9       0       8       2       4       x   c   2   \\\\   x   b   1       0       0       0       8       0       8       1       3       x   c   2   \\\\   x   b   1       0       0       0       8       0       7       2       2       x   c   2   \\\\   x   b   1       0       0       2       1       0       6       8       6       x   c   2   \\\\   x   b   1       0       0       0       9       0       6       6       4       x   c   2   \\\\   x   b   1       0       0       1       0       0       5       0       9       x   c   2   \\\\   x   b   1       0       0       1       6       0       4       9       2       x   c   2   \\\\   x   b   1       0       0       2       6       0       4       6       1       x   c   2   \\\\   x   b   1       0       0       1       1       h       u       n       g       e       n       e       n       u       r       l       s       c       e       n       e       4       2       e       v       l       u       n       f       l       d       l       f       n       f       c       l       a       g       e       e       n       i       n       e       l       e       r       u       r       e       8       1       1       2       8       1       5       5       g       e       e       n       f       r       u       l       e       l       d       l       p       r       b       l       e       w       e       c       n       u       c       f       c       l       g       e       e       n       e       x       p       e       r       e       n       n       m       r       p       h       2       4       w       h       c       h       c       n       n       r       e       h       n       5       0       0       0       0       f       c       l       g       e       f       r       b       u       1       3       0       0       0       p       e       p       l       e       f       f       f       e       r       e       n       r       c       e       e       c       h       f       c       l       g       e       n       n       e       w       h       c       h       r       n       l       g       c       l       g       e       t       g       e       n       e       r       e       n       g       e       r       b       u       n       f       r       e       c       h       f       c       e       g       e       w       e       f       l       l       w       h       e       e       r       e       g       u       e       n       8       2       8       5       w       h       c       h       u       e       g       u       n       r       b       u       n       w       h       e       e       n       h       e       c       h       r       n       l       g       c       l       g       e       f       h       e       f       c       e       g       e       f       g       1       t       h       e       p       r       e       c       e       g       e       f       r       f       c       e       g       e       p       l       h       e       g       e       h       v       n       g       h       e       h       g       h       e       p       r       b       b       l       n       h       e       p       r       e       c       e       1       w       e       w       n       l       h       e       e       e       f       r       h       p       c       e       e       u       e       u       c       n       p       e       p       l       e       x       g       e       n       g       l       d       l       n       e       x       h       7       x   0   c       l       b       e       l       r       b       u       n       t       h       e       p       e       r       f       r       n       c       e       f       g       e       e       n       e       v       l       u       e       b       h       e       e       n       b       l       u       e       e       r       r       r       m       a       e       b       e       w       e       e       n       p       r       e       c       e       g       e       n       c       h       r       n       l       g       c       l       g       e       a       h       e       c       u       r       r       e       n       e       f       h       e       r       r       e       u       l       n       m       r       p       h       b       n       b       f       n       e       u       n       n       g       d       l       d       l       5       n       v       g       g       f       c       e       2       3       w       e       l       b       u       l       l       d       l       f       n       v       g       g       f       c       e       b       r       e       p       l       c       n       g       h       e       f       x       l       e       r       n       v       g       g       n       e       b       l       d       l       f       f       l       l       w       n       g       5       w       e       n       r       1       0       e       n       f       l       c       r       v       l       n       n       h       e       r       e       u       l       r       e       u       r       z       e       n       t       b       l       e       2       w       h       c       h       h       w       l       d       l       f       c       h       e       v       e       h       e       e       f       h       e       r       p       e       r       f       r       n       c       e       n       m       r       p       h       n       e       h       h       e       g       n       f       c       n       p       e       r       f       r       n       c       e       g       n       b       e       w       e       e       n       e       e       p       l       d       l       e       l       d       l       d       l       n       l       d       l       f       n       n       n       e       e       p       l       d       l       e       l       i       i       s       l       d       l       c       p       n       n       b       f       g       s       l       d       l       n       h       e       u       p       e       r       r       f       l       d       l       f       c       p       r       e       w       h       d       l       d       l       v       e       r       f       e       h       e       e       f       f       e       c       v       e       n       e       f       e       n       e       n       l       e       r       n       n       g       n       u       r       r       e       e       b       e       e       l       f       r       l       d       l       r       e       p       e       c       v       e       l       t       b       l       e       2       m       a       e       f       g       e       e       n       c       p       r       n       n       m       r       p       h       2       4       m       e       h       i       i       s       l       d       l       1       1       c       p       n       n       1       1       b       f       g       s       l       d       l       6       d       l       d       l       v       g       g       f       c       e       5       l       d       l       f       v       g       g       f       c       e       u       r       m       a       e       5       6       7       x   c   2   \\\\   x   b   1       0       1       5       4       8       7       x   c   2   \\\\   x   b   1       0       3       1       3       9       4       x   c   2   \\\\   x   b   1       0       0       5       2       4       2       x   c   2   \\\\   x   b   1       0       0       1       2       2       4       x   c   2   \\\\   x   b   1       0       0       2       a       h       e       r       b       u       n       f       g       e       n       e       r       n       e       h       n       c       v       e       r       u       n       b       l       n       c       e       n       m       r       p       h       n       g       e       e       n       e       h       1       3       1       4       1       5       r       e       e       v       l       u       e       n       u       b       e       f       m       r       p       h       c       l       l       e       m       r       p       h       s       u       b       f       r       h       r       w       h       c       h       c       n       f       2       0       1       6       0       e       l       e       c       e       f       c       l       g       e       v       h       e       n       f       l       u       e       n       c       e       f       u       n       b       l       n       c       e       r       b       u       n       t       h       e       b       e       p       e       r       f       r       n       c       e       r       e       p       r       e       n       m       r       p       h       s       u       b       g       v       e       n       b       d       2       l       d       l       1       5       e       p       e       n       e       n       l       d       l       e       h       a       d       2       l       d       l       u       e       h       e       u       p       u       f       h       e       x   e   2   \\\\   x   8   0   \\\\   x   9   c       f       c       7       x   e   2   \\\\   x   8   0   \\\\   x   9   d       l       e       r       n       a       l       e       x       n       e       2       1       h       e       f       c       e       g       e       f       e       u       r       e       h       e       r       e       w       e       n       e       g       r       e       l       d       l       f       w       h       a       l       e       x       n       e       f       l       l       w       n       g       h       e       e       x       p       e       r       e       n       e       n       g       u       e       n       d       2       l       d       l       w       e       e       v       l       u       e       u       r       l       d       l       f       n       h       e       c       p       e       r       n       c       l       u       n       g       b       h       s       l       l       n       l       d       l       b       e       e       h       u       n       e       r       x       f       f       e       r       e       n       r       n       n       g       e       r       1       0       6       0       a       l       l       f       h       e       c       p       e       r       r       e       r       n       e       n       h       e       e       e       e       p       f       e       u       r       e       u       e       b       d       2       l       d       l       a       c       n       b       e       e       e       n       f       r       t       b       l       e       3       u       r       l       d       l       f       g       n       f       c       n       l       u       p       e       r       f       r       h       e       r       f       r       l       l       r       n       n       g       e       r       n       e       h       h       e       g       e       n       e       r       e       g       e       r       f       g       u       r       e       3       m       a       e       f       g       e       e       n       c       p       r       n       n       b       u       n       r       e       u       n       l       r       b       u       n       m       r       p       h       s       u       b       n       h       e       l       b       e       l       r       b       u       n       u       e       n       t       r       n       n       g       e       r       m       e       h       s       e       c       4       1       r       e       x       u       r       e       r       b       u       n       1       0       2       0       3       0       4       0       5       0       6       0       t       h       e       p       r       p       e       e       h       l       d       l       f       c       h       e       v       e       a       a       s       2       2       4       9       0       8       1       4       7       6       1       6       4       6       5       0       7       4       5       5       5       3       4       4       6       9       0       4       4       0       6       1       h       e       e       f       h       e       r       r       e       u       l       n       b       h       f       l       a       r       r       1       2       4       7       5       0       1       4       6       1       1       2       4       5       1       3       1       4       4       2       7       3       4       3       5       0       0       4       2       9       4       9       i       i       s       a       l       d       l       9       4       1       7       9       1       4       1       6       8       3       4       1       2       2       8       4       1       1       0       7       4       1       0       2       4       4       0       9       0       2       h       e       w       h       c       h       v       e       r       f       e       h       u       r       e       l       d       2       l       d       l       1       5       4       1       0       8       0       3       9       8       5       7       3       9       2       0       4       3       8       7       1       2       3       8       5       6       0       3       8       3       8       5       h       h       e       b       l       e       l       n       g       e       n       e       r       l       l       d       l       f       u       r       3       8       4       9       5       3       6       2       2       0       3       3       9       9       1       3       2       4       0       1       3       1       9       1       7       3       1       2       2       4       f       r       f       l       b       e       l       r       b       u       n       4       3       t       e       c       p       l       e       x       l       e       h       n       b       b       e       h       e       r       e       e       e       p       h       n       h       e       b       c       h       z       e       r       e       p       e       c       v       e       l       e       c       h       r       e       e       h       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       p       l       n       e       n       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       l       e       f       n       e       l       e       d       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       f       r       n       e       r       e       e       n       n       e       p       l       e       h       e       c       p       l       e       x       f       f       r       w       r       p       n       b       c       k       w       r       p       r       e       o       d       d       1       x   c   3   \\\\   x   9   7       c       o       d       x   c   3   \\\\   x   9   7       c       n       o       d       1       x   c   3   \\\\   x   9   7       c       d       x   c   3   \\\\   x   9   7       c       o       d       x   c   3   \\\\   x   9   7       c       r       e       p       e       c       v       e       l       s       f       r       k       r       e       e       n       n       b       b       c       h       e       h       e       c       p       l       e       x       f       f       r       w       r       n       b       c       k       w       r       p       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       b       t       h       e       c       p       l       e       x       f       n       e       r       n       u       p       e       l       e       f       n       e       r       e       o       n       b       x   c   3   \\\\   x   9   7       b       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       d       1       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       b       t       h       u       h       e       c       p       l       e       x       f       r       h       e       r       n       n       g       p       r       c       e       u       r       e       n       e       e       p       c       h       n       b       b       c       h       e       n       h       e       e       n       g       p       r       c       e       u       r       e       n       e       p       l       e       r       e       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       b       n       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       r       e       p       e       c       v       e       l       l       d       l       f       r       e       e       f       f       c       e       n       o       n       m       r       p       h       s       u       b       1       2       6       3       6       r       n       n       g       g       e       8       4       2       4       e       n       g       g       e       u       r       e       l       n       l       k       e       5       2       5       0       f       r       r       n       n       g       2       5       0       0       0       e       r       n       n       8       f       r       e       n       g       l       l       8       4       2       4       g       e       4       4       p       r       e       e       r       d       c       u       n       n       w       w       e       c       u       h       e       n       f       l       u       e       n       c       e       f       p       r       e       e       r       e       n       g       n       p       e       r       f       r       n       c       e       w       e       r       e       p       r       h       e       r       e       u       l       f       r       n       g       p       r       e       c       n       n       m       v       e       e       u       r       e       b       k       l       n       g       e       e       n       n       m       r       p       h       s       u       b       w       h       6       0       r       n       n       g       e       r       e       u       r       e       b       m       a       e       f       r       f       f       e       r       e       n       p       r       e       e       r       e       n       g       n       h       e       c       n       t       r       e       e       n       u       b       e       r       a       f       r       e       n       e       n       e       b       l       e       e       l       n       e       c       e       r       n       v       e       g       e       h       w       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       h       e       r       e       e       n       u       b       e       r       u       e       n       f       r       e       n       e       h       w       e       c       u       e       n       s       e       c       2       h       e       e       n       e       b       l       e       r       e       g       l       e       r       n       f       r       e       p       r       p       e       n       n       d       f       2       0       f       f       e       r       e       n       f       r       u       r       t       h       e       r       e       f       r       e       n       e       c       e       r       e       e       w       h       c       h       e       n       e       b       l       e       r       e       g       b       e       e       r       l       e       r       n       f       r       e       t       w       r       h       e       n       w       e       r       e       p       l       c       e       u       r       e       n       e       b       l       e       r       e       g       n       l       d       l       f       b       h       e       n       e       u       e       n       n       d       f       n       n       e       h       e       h       n       d       f       l       d       l       t       h       e       c       r       r       e       p       n       n       g       h       l       l       w       e       l       n       e       b       n       d       f       l       d       l       w       e       f       x       h       e       r       p       r       e       e       r       e       r       e       e       e       p       h       n       8       x   0   c       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       h       e       e       f       u       l       e       n       g       a       h       w       n       n       f       g       4       u       r       e       n       e       b       l       e       r       e       g       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       h       e       n       e       u       e       n       n       d       f       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       o       b       e       r       v       e       f       r       f       g       4       h       e       p       e       r       f       r       n       c       e       f       l       d       l       f       c       n       b       e       p       r       v       e       b       u       n       g       r       e       r       e       e       b       u       h       e       p       r       v       e       e       n       b       e       c       e       n       c       r       e       n       g       l       l       l       e       r       n       l       l       e       r       t       h       e       r       e       f       r       e       u       n       g       u       c       h       l       r       g       e       r       e       n       e       b       l       e       e       n       e       l       b       g       p       r       v       e       e       n       o       n       m       v       e       h       e       n       u       b       e       r       f       r       e       e       k       1       0       0       k       l       0       0       7       0       v       k       2       0       k       l       0       0       7       1       n       e       h       n       l       l       r       n       f       r       e       b       e       e       h       u       e       l       r       g       e       n       u       b       e       r       f       r       e       e       e       g       s       h       n       e       l       2       5       b       n       e       v       e       r       g       p       e       e       n       r       e       u       l       f       r       e       p       h       g       e       b       n       l       3       e       c       n       r       e       e       t       r       e       e       e       p       h       t       r       e       e       e       p       h       n       h       e       r       p       r       n       p       r       e       e       r       f       r       e       c       n       r       e       e       i       n       l       d       l       f       h       e       r       e       n       p       l       c       c       n       r       n       b       e       w       e       e       n       r       e       e       e       p       h       h       n       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       x   c   f   \\\\   x   8   4       x   c   f   \\\\   x   8   4       x   e   2   \\\\   x   8   9   \\\\   x   a   5       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       t       c       u       h       e       n       f       l       u       e       n       c       e       f       r       e       e       e       p       h       h       e       p       e       r       f       r       n       c       e       f       l       d       l       f       w       e       e       x   c   f   \\\\   x   8   4       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       n       f       x       r       e       e       n       u       b       e       r       k       1       n       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       r       e       e       e       p       h       h       w       n       n       f       g       4       b       w       e       e       e       h       h       e       p       e       r       f       r       n       c       e       f       r       p       r       v       e       h       e       n       e       c       r       e       e       w       h       h       e       n       c       r       e       e       f       h       e       r       e       e       e       p       h       t       h       e       r       e       n       h       e       r       e       e       e       p       h       n       c       r       e       e       h       e       e       n       n       f       l       e       r       n       e       f       e       u       r       e       n       c       r       e       e       e       x       p       n       e       n       l       l       w       h       c       h       g       r       e       l       n       c       r       e       e       h       e       r       n       n       g       f       f       c       u       l       s       u       n       g       u       c       h       l       r       g       e       r       e       p       h       l       e       b       p       e       r       f       r       n       c       e       o       n       m       v       e       r       e       e       e       p       h       h       1       8       k       l       0       1       1       6       2       v       h       9       k       l       0       0       8       3       1       f       g       u       r       e       4       t       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       f       g       e       e       n       n       m       r       p       h       s       u       b       n       r       n       g       p       r       e       c       n       n       m       v       e       b       v       r       n       g       r       e       e       n       u       b       e       r       n       b       r       e       e       e       p       h       o       u       r       p       p       r       c       h       l       d       l       f       l       d       l       f       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       d       f       n       d       f       l       d       l       n       d       f       l       d       l       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       5       c       n       c       l       u       n       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       h       e       f       r       e       n       f       u       n       h       h       e       l       e       f       n       e       p       r       e       c       n       c       n       b       e       p       z       e       v       v       r       n       l       b       u       n       n       g       w       h       c       h       e       n       b       l       e       l       l       h       e       r       e       e       n       h       e       f       e       u       r       e       h       e       u       e       b       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       e       x       p       e       r       e       n       l       r       e       u       l       h       w       e       h       e       u       p       e       r       r       f       u       r       l       g       r       h       f       r       e       v       e       r       l       l       d       l       k       n       r       e       l       e       c       p       u       e       r       v       n       p       p       l       c       n       n       v       e       r       f       e       u       r       e       l       h       h       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       a       c       k       n       w       l       e       g       e       e       n       t       h       w       r       k       w       u       p       p       r       e       n       p       r       b       h       e       n       n       l       n       u       r       l       s       c       e       n       c       e       f       u       n       n       f       c       h       n       n       6       1       6       7       2       3       3       6       n       p       r       b       x   e   2   \\\\   x   8   0   \\\\   x   9   c       c       h       e       n       g       u       n       g       x   e   2   \\\\   x   8   0   \\\\   x   9   d       p       r       j       e       c       u       p       p       r       e       b       s       h       n       g       h       m       u       n       c       p       l       e       u       c       n       c       n       n       s       h       n       g       h       e       u       c       n       d       e       v       e       l       p       e       n       f       u       n       n       n       1       5       c       g       4       3       n       n       p       r       b       o       n       r       n       0       0       0       1       4       1       5       1       2       3       5       6       r       e       f       e       r       e       n       c       e       1       y       a       n       d       g       e       n       s       h       p       e       q       u       n       z       n       n       r       e       c       g       n       n       w       h       r       n       z       e       r       e       e       n       e       u       r       l       c       p       u       n       9       7       1       5       4       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       5       8       8       1       9       9       7       2       a       l       b       e       r       g       e       r       s       d       p       e       r       n       v       j       d       p       e       r       a       x       u       e       n       r       p       p       p       r       c       h       n       u       r       l       l       n       g       u       g       e       p       r       c       e       n       g       c       p       u       n       l       l       n       g       u       c       2       2       1       3       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       7       1       1       9       9       6       3       l       b       r       e       n       r       n       f       r       e       m       c       h       n       e       l       e       r       n       n       g       4       5       1       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       2       2       0       0       1       4       a       c       r       n       n       j       s       h       n       d       e       c       n       f       r       e       f       r       c       p       u       e       r       v       n       n       m       e       c       l       i       g       e       a       n       l       s       p       r       n       g       e       r       2       0       1       3       5       b       b       g       c       x       n       g       c       w       x       e       j       w       u       n       x       g       e       n       g       d       e       e       p       l       b       e       l       r       b       u       n       l       e       r       n       n       g       w       h       l       b       e       l       b       g       u       r       x       v       1       6       1       1       0       1       7       3       1       2       0       1       7       6       x       g       e       n       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       e       e       e       t       r       n       k       n       w       l       d       e       n       g       2       8       7       1       7       3       4       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       7       4       8       2       0       1       6       9       x   0   c       7       x       g       e       n       g       n       p       h       u       p       r       e       r       e       l       e       e       p       r       e       c       n       f       c       r       w       p       n       n       n       v       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       i       j       c       a       i       p       g       e       3       5       1       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       5       1       7       2       0       1       5       8       x       g       e       n       g       k       s       h       m       l       e       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       i       n       p       r       c       a       a       a       i       2       0       1       0       9       x       g       e       n       g       q       w       n       g       n       y       x       f       c       l       g       e       e       n       b       p       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       c       i       c       p       r       p       g       e       4       4       6       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       4       7       0       2       0       1       4       1       0       x       g       e       n       g       n       y       x       h       e       p       e       e       n       b       e       n       u       l       v       r       e       l       b       e       l       r       b       u       n       i       n       p       r       c       c       v       p       r       p       g       e       1       8       3       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       8       4       2       2       0       1       4       1       1       x       g       e       n       g       c       y       n       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       i       e       e       e       t       r       n       p       e       r       n       a       n       l       m       c       h       i       n       e       l       l       3       5       1       0       2       4       0       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       4       1       2       2       0       1       3       1       2       g       g       u       y       f       u       c       r       d       e       r       n       t       s       h       u       n       g       i       g       e       b       e       h       u       n       g       e       e       n       b       n       f       l       l       e       r       n       n       g       n       l       c       l       l       j       u       e       r       b       u       r       e       g       r       e       n       i       e       e       e       t       r       n       i       g       e       p       r       c       e       n       g       1       7       7       1       1       7       8       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       1       8       8       2       0       0       8       1       3       g       g       u       n       g       m       u       h       u       n       g       e       e       n       w       h       h       e       n       f       l       u       e       n       c       e       c       r       r       c       e       n       g       e       n       e       r       i       n       c       v       p       r       w       r       k       h       p       p       g       e       7       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       7       8       2       0       1       0       1       4       g       g       u       n       c       z       h       n       g       a       u       n       c       r       p       p       u       l       n       g       e       e       n       i       n       p       r       c       c       v       p       r       p       g       e       4       2       5       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       2       6       3       2       0       1       4       1       5       z       h       e       x       l       z       z       h       n       g       f       w       u       x       g       e       n       g       y       z       h       n       g       m       h       y       n       g       n       y       z       h       u       n       g       d       e       p       e       n       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       i       e       e       e       t       r       n       n       i       g       e       p       r       c       e       n       g       2       0       1       7       1       6       t       k       h       r       n       e       c       n       f       r       e       i       n       p       r       c       i       c       d       a       r       p       g       e       2       7       8       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       8       2       1       9       9       5       1       7       t       k       h       t       h       e       r       n       u       b       p       c       e       e       h       f       r       c       n       r       u       c       n       g       e       c       n       f       r       e       i       e       e       e       t       r       n       p       e       r       n       a       n       l       m       c       h       i       n       e       l       l       2       0       8       8       3       2       x   e   2   \\\\   x   8   0   \\\\   x   9   3       8       4       4       1       9       9       8       1       8       y       j       e       s       h       e       l       h       e       r       j       d       n       h       u       e       s       k       r       e       v       j       l       n       g       r       g       r       h       c       k       s       g       u       r       r       n       t       d       r       r       e       l       l       c       f       f       e       c       n       v       l       u       n       l       r       c       h       e       c       u       r       e       f       r       f       f       e       u       r       e       e       b       e       n       g       r       x       v       p       r       e       p       r       n       r       x       v       1       4       0       8       5       0       9       3       2       0       1       4       1       9       m       i       j       r       n       z       g       h       h       r       n       t       s       j       k       k       l       n       l       k       s       u       l       a       n       n       r       u       c       n       v       r       n       l       e       h       f       r       g       r       p       h       c       l       e       l       m       c       h       n       e       l       e       r       n       n       g       3       7       2       1       8       3       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       3       3       1       9       9       9       2       0       p       k       n       c       h       e       e       r       m       f       e       r       u       a       c       r       n       n       s       r       b       u       l       x   c   3   \\\\   x   b   2       d       e       e       p       n       e       u       r       l       e       c       n       f       r       e       i       n       p       r       c       i       c       c       v       p       g       e       1       4       6       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       4       7       5       2       0       1       5       2       1       a       k       r       z       h       e       v       k       i       s       u       k       e       v       e       r       n       g       e       h       n       n       i       g       e       n       e       c       l       f       c       n       w       h       e       e       p       c       n       v       l       u       n       l       n       e       u       r       l       n       e       w       r       k       i       n       p       r       c       n       i       p       s       p       g       e       1       1       0       6       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       1       1       4       2       0       1       2       2       2       a       l       n       c       d       r       g       n       v       n       c       c       h       r       u       l       u       c       p       r       n       g       f       f       e       r       e       n       c       l       f       e       r       f       r       u       c       g       e       e       n       i       e       e       e       t       r       n       n       c       b       e       r       n       e       c       3       4       1       6       2       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       6       2       8       2       0       0       4       2       3       o       m       p       r       k       h       a       v       e       l       n       a       z       e       r       n       d       e       e       p       f       c       e       r       e       c       g       n       n       i       n       p       r       c       b       m       v       c       p       g       e       4       1       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       1       1       2       2       0       1       5       2       4       k       r       c       n       e       k       n       t       t       e       f       e       m       o       r       p       h       a       l       n       g       u       n       l       g       e       b       e       f       n       r       l       u       l       g       e       p       r       g       r       e       n       i       n       p       r       c       f       g       p       g       e       3       4       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       4       5       2       0       0       6       2       5       j       s       h       n       a       w       f       z       g       b       b       n       m       c       k       t       s       h       r       p       m       f       n       c       c       h       r       m       r       e       a       k       p       n       n       a       b       l       k       e       r       e       l       e       h       u       n       p       e       r       e       c       g       n       n       n       p       r       f       r       n       g       l       e       e       p       h       g       e       i       n       p       r       c       c       v       p       r       p       g       e       1       2       9       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       3       0       4       2       0       1       1       2       6       g       t       u       k       n       i       k       k       m       u       l       l       b       e       l       c       l       f       c       n       a       n       v       e       r       v       e       w       i       n       e       r       n       n       l       j       u       r       n       l       f       d       w       r       e       h       u       n       g       n       m       n       n       g       3       3       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       3       2       0       0       7       2       7       c       x       n       g       x       g       e       n       g       n       h       x       u       e       l       g       c       b       n       g       r       e       g       r       e       n       f       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       c       c       v       p       r       p       g       e       4       4       8       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       4       9       7       2       0       1       6       2       8       x       y       n       g       x       g       e       n       g       n       d       z       h       u       s       p       r       c       n       n       l       e       n       e       r       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       i       n       p       r       c       i       j       c       a       i       p       g       e       2       2       5       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       2       6       5       2       0       1       6       2       9       a       l       y       u       l       l       e       n       a       r       n       g       r       j       n       t       h       e       c       n       c       v       e       c       n       v       e       x       p       r       c       e       u       r       e       n       e       u       r       l       c       p       u       n       1       5       4       9       1       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       9       3       6       2       0       0       3       3       0       y       z       h       u       h       x       u       e       n       x       g       e       n       g       e       n       r       b       u       n       r       e       c       g       n       n       f       r       f       c       l       e       x       p       r       e       n       i       n       p       r       c       m       m       p       g       e       1       2       4       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       2       5       0       2       0       1       5       1       0       x   0   c   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.22\n",
      "XGBoost Accuracy on Test set -> 0.36\n",
      "RandomForest Accuracy on Test set -> 0.34\n",
      "DecisionTree Accuracy on Test set -> 0.24\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l d r b u n l e r n n g f r e w e s h e n 1 2 k z h 1 y l u g u 1 a l n y u l l e 2 k e l b r r f s p e c l f b e r o p c n o p c l a c c e n e w r k s h n g h i n u e f r a v n c e c u n c n n d s c e n c e s c h l f c u n c n n i n f r n e n g n e e r n g s h n g h u n v e r 2 d e p r e n f c p u e r s c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 o c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c a b r c l b e l r b u n l e r n n g l d l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l d l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r t h p p e r p r e e n l b e l r b u n l e r n n g f r e l d l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 d e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 t h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g t h e e f f e c v e n e f h e p r p e l d l f v e r f e n e v e r l l d l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l d l e h 1 i n r u c n l b e l r b u n l e r n n g l d l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g s l l n u l l b e l l e r n n g m l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l d l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l s u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u a n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e t h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l a n h e r e x p l e v e r n g p r e c n 7 m n f u v e r e v e w w e b e u c h n e f l x i m d b n d u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b i f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h m n l d l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n s e h e r l d l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l i n f r n p r c e n g s e n i p s 2 0 1 7 l n g b e c h c a u s a x 0 c f g u r e 1 t h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n i n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l d l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l d l k h w v n g e o n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n t h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n t l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n i n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l o u r l d l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l d l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l d l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l d l e h t h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b t h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 i l l u r n f l b e l r b u n l e r n n g f r e t h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k t h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l t w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l t h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e t h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k s n c e u r l d l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e t h e n w e c u c u r r e n l d l e h d e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k i n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r t h e n e w l p r p e e e p n e u r l e c n f r e n d f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e t h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n o u r e h e x e n n d f r e l d l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e a l h u g h e p z e f r e e u p e f u n c n w g v e n n n d f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n d f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l d l l i n n h e r e g e u e n l d l f n n d f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n d f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n d f n 3 l l r e e n l d l f c n b e l e r n e j n l w h l e r e e n n d f w e r e l e r n e l e r n v e l t h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n s e c 4 4 l d l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n d f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e t u u p w r n d f 2 0 h e c n r b u n f l d l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g a n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l d l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l t h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r t r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e i i s l l d w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l y n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e s c e l d l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f i i s l l d c l l e b f g s l d l b u n g q u n e w n p z n a l l h e b v e l d l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r a n h e r w r e h e l d l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l d s v r l d l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l d l k b v e w e g h e r e g r e r t h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h a o s o l d l l g b a h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e a o s o l d l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l d l k n n e r e n g p c b u h e e x n g u c h e h c l l e d l d l 5 l l f c u e n x u e n r p e l b e l d l o u r e h l d l f e x e n f f e r e n b l e e c n r e e r e l d l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l i n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l d l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l d r b u n l e r n n g f r e a f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n y 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l d l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 t h e g l f h e l d l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l t a e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r y e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 t b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r m r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r t h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 d r e u e f r c n r u c n g r e e e e r n e r n l a n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 t h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b y l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n t n l n t n r r e p e c v e l t h e u p u f h e r e e t w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 t p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 t r e e o p z n g v e n r n n g e s x n 1 u r g l l e r n e c n r e e t e c r b e n s e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 t l r f r e c h p l e x t h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 t n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 s x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 t x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 t h e c h u p u u n f g x x c e x 9 8 t l e r n n g h e r e e t r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e t h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 s 4 x c e x 9 8 q t l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 t h e n w e f x x c e x 9 8 n p z e q t h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g s p l n e i n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 s w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n t h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 t n l x 1 1 g c x x c e x 9 8 t n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 t g c x x c e x 9 8 t p p w h e r e g c x x c e x 9 8 t n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 t n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e t n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 t n g c x x c e x 9 8 t n l g c x x c e x 9 8 t n r t h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r t h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 s x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q i n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r a u p p e r b u n f r h e l f u n c n r q x c e x 9 8 s c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 s x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x a 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b e q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 t x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b e q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b e q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b e q x c c x 8 4 c x l g n 1 c 1 x c e x b e q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l t h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 s w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x a 5 r q x c e x 9 8 s n x c f x 8 6 q q r q x c e x 9 8 s a u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 s i n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x a 4 r q x c e x 9 8 s w h c h p l e r q 1 x c e x 9 8 s x e 2 x 8 9 x a 4 r q x c e x 9 8 s 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 s f e r e n u r n g h r q x c e x 9 8 s x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q s w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e t h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b e q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b e q c x p c 1 p n c x c e x b e q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e a f r e n e n e b l e f e c n r e e f t 1 t k i n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q t h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r t k w h e r e r t k h e l f u n c n f r r e e t k e f n e b e q n 3 t l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n s e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r t k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f t k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f t h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g a f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e s b u n e f n b c h e b t h e r n n g p r c e u r e f l d l f h w n n a l g r h 1 a l g r h 1 t h e r n n g p r c e u r e f l d l f r e q u r e s r n n g e n b h e n u b e r f n b c h e u p e q i n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r s u p e s x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e i n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 t k 6 x 0 c 4 e x p e r e n l r e u l o u r r e l z n f l d l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 d 1 8 i u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l d l f r n e g r e w h n e e p n e w r k l d l f w e e v l u e l d l f n f f e r e n l d l k n c p r e w h h e r n l n e l d l e h a l d l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l d l f n c p u e r v n p p l c n e f c l g e e n t h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l d l f s n l n e l d l m e h w e c p r e u r h l l w e l l d l f w h h e r e f h e r n l n e l d l e h f r l d l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l d l e n 6 m v e h u n g e n e n n u r l s c e n e 1 t h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l t h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l d l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n s x c f x 8 6 r e n e n s q u r e x c f x 8 7 2 n w l r e u r e f e l i n e r e c n w e e v l u e u r h l l w e l l d l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l d l e h t h e r e u l f l d l f n h e c p e r r e u r z e n t b l e 1 f r m v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e i n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 d n e r l e h w r n n g n e n g g e v e a c n b e e e n f r t b l e 1 l d l f p e r f r b e n l l f h e x e u r e t b l e 1 c p r n r e u l n h r e e l d l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 d n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 d n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l d e m e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 s x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 s q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 i n e r e c n x e 2 x 8 6 x 9 1 m v e l d l f u r a o s o l d l g b 2 7 l d l g b 2 7 l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l d l f u r l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l d l f u r l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l s c e n e 4 2 e v l u n f l d l f n f c l a g e e n i n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l d l p r b l e w e c n u c f c l g e e n e x p e r e n n m r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e t g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 t h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l d l n e x h 7 x 0 c l b e l r b u n t h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r m a e b e w e e n p r e c e g e n c h r n l g c l g e a h e c u r r e n e f h e r r e u l n m r p h b n b f n e u n n g d l d l 5 n v g g f c e 2 3 w e l b u l l d l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l d l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n t b l e 2 w h c h h w l d l f c h e v e h e e f h e r p e r f r n c e n m r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l d l e l d l d l n l d l f n n n e e p l d l e l i i s l d l c p n n b f g s l d l n h e u p e r r f l d l f c p r e w h d l d l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l d l r e p e c v e l t b l e 2 m a e f g e e n c p r n n m r p h 2 4 m e h i i s l d l 1 1 c p n n 1 1 b f g s l d l 6 d l d l v g g f c e 5 l d l f v g g f c e u r m a e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 a h e r b u n f g e n e r n e h n c v e r u n b l n c e n m r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f m r p h c l l e m r p h s u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n t h e b e p e r f r n c e r e p r e n m r p h s u b g v e n b d 2 l d l 1 5 e p e n e n l d l e h a d 2 l d l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 d l e r n a l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l d l f w h a l e x n e f l l w n g h e e x p e r e n e n g u e n d 2 l d l w e e v l u e u r l d l f n h e c p e r n c l u n g b h s l l n l d l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 a l l f h e c p e r r e r n e n h e e e e p f e u r e u e b d 2 l d l a c n b e e e n f r t b l e 3 u r l d l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 m a e f g e e n c p r n n b u n r e u n l r b u n m r p h s u b n h e l b e l r b u n u e n t r n n g e r m e h s e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 t h e p r p e e h l d l f c h e v e a a s 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l a r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 i i s a l d l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l d 2 l d l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l d l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 t e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e d 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e o d d 1 x c 3 x 9 7 c o d x c 3 x 9 7 c n o d 1 x c 3 x 9 7 c d x c 3 x 9 7 c o d x c 3 x 9 7 c r e p e c v e l s f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b t h e c p l e x f n e r n u p e l e f n e r e o n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 d 1 o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b t h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n o d x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l d l f r e e f f c e n o n m r p h s u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r d c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n m v e e u r e b k l n g e e n n m r p h s u b w h 6 0 r n n g e r e u r e b m a e f r f f e r e n p r e e r e n g n h e c n t r e e n u b e r a f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n s e c 2 h e e n e b l e r e g l e r n f r e p r p e n n d f 2 0 f f e r e n f r u r t h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e t w r h e n w e r e p l c e u r e n e b l e r e g n l d l f b h e n e u e n n d f n n e h e h n d f l d l t h e c r r e p n n g h l l w e l n e b n d f l d l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g a h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n d f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e o b e r v e f r f g 4 h e p e r f r n c e f l d l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r t h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n o n m v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g s h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e t r e e e p h t r e e e p h n h e r p r n p r e e r f r e c n r e e i n l d l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x a 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 t c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l d l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h t h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l s u n g u c h l r g e r e p h l e b p e r f r n c e o n m v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 t h e p e r f r n c e c h n g e f g e e n n m r p h s u b n r n g p r e c n n m v e b v r n g r e e n u b e r n b r e e e p h o u r p p r c h l d l f l d l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n d f n d f l d l n d f l d l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l d l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n a c k n w l e g e e n t h w r k w u p p r e n p r b h e n n l n u r l s c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 d p r j e c u p p r e b s h n g h m u n c p l e u c n c n n s h n g h e u c n d e v e l p e n f u n n n 1 5 c g 4 3 n n p r b o n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 y a n d g e n s h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 a l b e r g e r s d p e r n v j d p e r a x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e m c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 a c r n n j s h n d e c n f r e f r c p u e r v n n m e c l i g e a n l s p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g d e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g i e e e t r n k n w l d e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g i n p r i j c a i p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k s h m l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n i n p r c a a a i 2 0 1 0 9 x g e n g q w n g n y x f c l g e e n b p v e l b e l r b u n l e r n n g i n p r c i c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n y x h e p e e n b e n u l v r e l b e l r b u n i n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c y n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n i e e e t r n p e r n a n l m c h i n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u y f u c r d e r n t s h u n g i g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n i e e e t r n i g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g m u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r i n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g a u n c r p p u l n g e e n i n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g y z h n g m h y n g n y z h u n g d e p e n e n l b e l r b u n l e r n n g f r g e e n i e e e t r n n i g e p r c e n g 2 0 1 7 1 6 t k h r n e c n f r e i n p r c i c d a r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 t k h t h e r n u b p c e e h f r c n r u c n g e c n f r e i e e e t r n p e r n a n l m c h i n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 y j e s h e l h e r j d n h u e s k r e v j l n g r g r h c k s g u r r n t d r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 m i j r n z g h h r n t s j k k l n l k s u l a n n r u c n v r n l e h f r g r p h c l e l m c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r m f e r u a c r n n s r b u l x c 3 x b 2 d e e p n e u r l e c n f r e i n p r c i c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 a k r z h e v k i s u k e v e r n g e h n n i g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k i n p r c n i p s p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 a l n c d r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n i e e e t r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 o m p r k h a v e l n a z e r n d e e p f c e r e c g n n i n p r c b m v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n t t e f e m o r p h a l n g u n l g e b e f n r l u l g e p r g r e n i n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j s h n a w f z g b b n m c k t s h r p m f n c c h r m r e a k p n n a b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e i n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g t u k n i k k m u l l b e l c l f c n a n v e r v e w i n e r n n l j u r n l f d w r e h u n g n m n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g i n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x y n g x g e n g n d z h u s p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n i n p r c i j c a i p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 a l y u l l e n a r n g r j n t h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 y z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n i n p r c m m p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.32\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_LOW AS PREPROCESSING FUNCTION * * * *\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-02 05:43:07.023724: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       s       w       e       i       s       h       e       n       1       2       k       a       i       z       h       a       o       1       y       i       l       u       g       u       o       1       a       l       a       n       y       u       i       l       l       e       2       k       e       y       l       a       b       o       r       a       t       o       r       y       o       f       s       p       e       c       i       a       l       t       y       f       i       b       e       r       o       p       t       i       c       s       a       n       d       o       p       t       i       c       a       l       a       c       c       e       s       s       n       e       t       w       o       r       k       s       s       h       a       n       g       h       a       i       i       n       s       t       i       t       u       t       e       f       o       r       a       d       v       a       n       c       e       d       c       o       m       m       u       n       i       c       a       t       i       o       n       a       n       d       d       a       t       a       s       c       i       e       n       c       e       s       c       h       o       o       l       o       f       c       o       m       m       u       n       i       c       a       t       i       o       n       a       n       d       i       n       f       o       r       m       a       t       i       o       n       e       n       g       i       n       e       e       r       i       n       g       s       h       a       n       g       h       a       i       u       n       i       v       e       r       s       i       t       y       2       d       e       p       a       r       t       m       e       n       t       o       f       c       o       m       p       u       t       e       r       s       c       i       e       n       c       e       j       o       h       n       s       h       o       p       k       i       n       s       u       n       i       v       e       r       s       i       t       y       a       r       x       i       v       1       7       0       2       0       6       0       8       6       v       4       c       s       l       g       1       6       o       c       t       2       0       1       7       1       s       h       e       n       w       e       i       1       2       3       1       z       h       a       o       k       1       2       0       6       g       y       l       l       u       a       n       0       a       l       a       n       l       y       u       i       l       l       e       g       m       a       i       l       c       o       m       a       b       s       t       r       a       c       t       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       l       d       l       i       s       a       g       e       n       e       r       a       l       l       e       a       r       n       i       n       g       f       r       a       m       e       w       o       r       k       w       h       i       c       h       a       s       s       i       g       n       s       t       o       a       n       i       n       s       t       a       n       c       e       a       d       i       s       t       r       i       b       u       t       i       o       n       o       v       e       r       a       s       e       t       o       f       l       a       b       e       l       s       r       a       t       h       e       r       t       h       a       n       a       s       i       n       g       l       e       l       a       b       e       l       o       r       m       u       l       t       i       p       l       e       l       a       b       e       l       s       c       u       r       r       e       n       t       l       d       l       m       e       t       h       o       d       s       h       a       v       e       e       i       t       h       e       r       r       e       s       t       r       i       c       t       e       d       a       s       s       u       m       p       t       i       o       n       s       o       n       t       h       e       e       x       p       r       e       s       s       i       o       n       f       o       r       m       o       f       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       o       r       l       i       m       i       t       a       t       i       o       n       s       i       n       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       e       g       t       o       l       e       a       r       n       d       e       e       p       f       e       a       t       u       r       e       s       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       t       h       i       s       p       a       p       e       r       p       r       e       s       e       n       t       s       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       s       l       d       l       f       s       a       n       o       v       e       l       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       b       a       s       e       d       o       n       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       w       h       i       c       h       h       a       v       e       s       e       v       e       r       a       l       a       d       v       a       n       t       a       g       e       s       1       d       e       c       i       s       i       o       n       t       r       e       e       s       h       a       v       e       t       h       e       p       o       t       e       n       t       i       a       l       t       o       m       o       d       e       l       a       n       y       g       e       n       e       r       a       l       f       o       r       m       o       f       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       b       y       a       m       i       x       t       u       r       e       o       f       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       2       t       h       e       l       e       a       r       n       i       n       g       o       f       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       c       a       n       b       e       c       o       m       b       i       n       e       d       w       i       t       h       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       w       e       d       e       f       i       n       e       a       d       i       s       t       r       i       b       u       t       i       o       n       b       a       s       e       d       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       a       f       o       r       e       s       t       e       n       a       b       l       i       n       g       a       l       l       t       h       e       t       r       e       e       s       t       o       b       e       l       e       a       r       n       e       d       j       o       i       n       t       l       y       a       n       d       s       h       o       w       t       h       a       t       a       n       u       p       d       a       t       e       f       u       n       c       t       i       o       n       f       o       r       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       w       h       i       c       h       g       u       a       r       a       n       t       e       e       s       a       s       t       r       i       c       t       d       e       c       r       e       a       s       e       o       f       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       c       a       n       b       e       d       e       r       i       v       e       d       b       y       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       t       h       e       e       f       f       e       c       t       i       v       e       n       e       s       s       o       f       t       h       e       p       r       o       p       o       s       e       d       l       d       l       f       s       i       s       v       e       r       i       f       i       e       d       o       n       s       e       v       e       r       a       l       l       d       l       t       a       s       k       s       a       n       d       a       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       p       p       l       i       c       a       t       i       o       n       s       h       o       w       i       n       g       s       i       g       n       i       f       i       c       a       n       t       i       m       p       r       o       v       e       m       e       n       t       s       t       o       t       h       e       s       t       a       t       e       o       f       t       h       e       a       r       t       l       d       l       m       e       t       h       o       d       s       1       i       n       t       r       o       d       u       c       t       i       o       n       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       l       d       l       6       1       1       i       s       a       l       e       a       r       n       i       n       g       f       r       a       m       e       w       o       r       k       t       o       d       e       a       l       w       i       t       h       p       r       o       b       l       e       m       s       o       f       l       a       b       e       l       a       m       b       i       g       u       i       t       y       u       n       l       i       k       e       s       i       n       g       l       e       l       a       b       e       l       l       e       a       r       n       i       n       g       s       l       l       a       n       d       m       u       l       t       i       l       a       b       e       l       l       e       a       r       n       i       n       g       m       l       l       2       6       w       h       i       c       h       a       s       s       u       m       e       a       n       i       n       s       t       a       n       c       e       i       s       a       s       s       i       g       n       e       d       t       o       a       s       i       n       g       l       e       l       a       b       e       l       o       r       m       u       l       t       i       p       l       e       l       a       b       e       l       s       l       d       l       a       i       m       s       a       t       l       e       a       r       n       i       n       g       t       h       e       r       e       l       a       t       i       v       e       i       m       p       o       r       t       a       n       c       e       o       f       e       a       c       h       l       a       b       e       l       i       n       v       o       l       v       e       d       i       n       t       h       e       d       e       s       c       r       i       p       t       i       o       n       o       f       a       n       i       n       s       t       a       n       c       e       i       e       a       d       i       s       t       r       i       b       u       t       i       o       n       o       v       e       r       t       h       e       s       e       t       o       f       l       a       b       e       l       s       s       u       c       h       a       l       e       a       r       n       i       n       g       s       t       r       a       t       e       g       y       i       s       s       u       i       t       a       b       l       e       f       o       r       m       a       n       y       r       e       a       l       w       o       r       l       d       p       r       o       b       l       e       m       s       w       h       i       c       h       h       a       v       e       l       a       b       e       l       a       m       b       i       g       u       i       t       y       a       n       e       x       a       m       p       l       e       i       s       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       8       e       v       e       n       h       u       m       a       n       s       c       a       n       n       o       t       p       r       e       d       i       c       t       t       h       e       p       r       e       c       i       s       e       a       g       e       f       r       o       m       a       s       i       n       g       l       e       f       a       c       i       a       l       i       m       a       g       e       t       h       e       y       m       a       y       s       a       y       t       h       a       t       t       h       e       p       e       r       s       o       n       i       s       p       r       o       b       a       b       l       y       i       n       o       n       e       a       g       e       g       r       o       u       p       a       n       d       l       e       s       s       l       i       k       e       l       y       t       o       b       e       i       n       a       n       o       t       h       e       r       h       e       n       c       e       i       t       i       s       m       o       r       e       n       a       t       u       r       a       l       t       o       a       s       s       i       g       n       a       d       i       s       t       r       i       b       u       t       i       o       n       o       f       a       g       e       l       a       b       e       l       s       t       o       e       a       c       h       f       a       c       i       a       l       i       m       a       g       e       f       i       g       1       a       i       n       s       t       e       a       d       o       f       u       s       i       n       g       a       s       i       n       g       l       e       a       g       e       l       a       b       e       l       a       n       o       t       h       e       r       e       x       a       m       p       l       e       i       s       m       o       v       i       e       r       a       t       i       n       g       p       r       e       d       i       c       t       i       o       n       7       m       a       n       y       f       a       m       o       u       s       m       o       v       i       e       r       e       v       i       e       w       w       e       b       s       i       t       e       s       s       u       c       h       a       s       n       e       t       f       l       i       x       i       m       d       b       a       n       d       d       o       u       b       a       n       p       r       o       v       i       d       e       a       c       r       o       w       d       o       p       i       n       i       o       n       f       o       r       e       a       c       h       m       o       v       i       e       s       p       e       c       i       f       i       e       d       b       y       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       o       f       r       a       t       i       n       g       s       c       o       l       l       e       c       t       e       d       f       r       o       m       t       h       e       i       r       u       s       e       r       s       f       i       g       1       b       i       f       a       s       y       s       t       e       m       c       o       u       l       d       p       r       e       c       i       s       e       l       y       p       r       e       d       i       c       t       s       u       c       h       a       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       e       v       e       r       y       m       o       v       i       e       b       e       f       o       r       e       i       t       i       s       r       e       l       e       a       s       e       d       m       o       v       i       e       p       r       o       d       u       c       e       r       s       c       a       n       r       e       d       u       c       e       t       h       e       i       r       i       n       v       e       s       t       m       e       n       t       r       i       s       k       a       n       d       t       h       e       a       u       d       i       e       n       c       e       c       a       n       b       e       t       t       e       r       c       h       o       o       s       e       w       h       i       c       h       m       o       v       i       e       s       t       o       w       a       t       c       h       m       a       n       y       l       d       l       m       e       t       h       o       d       s       a       s       s       u       m       e       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       c       a       n       b       e       r       e       p       r       e       s       e       n       t       e       d       b       y       a       m       a       x       i       m       u       m       e       n       t       r       o       p       y       m       o       d       e       l       2       a       n       d       l       e       a       r       n       i       t       b       y       o       p       t       i       m       i       z       i       n       g       a       n       e       n       e       r       g       y       f       u       n       c       t       i       o       n       b       a       s       e       d       o       n       t       h       e       m       o       d       e       l       8       1       1       2       8       6       b       u       t       t       h       e       e       x       p       o       n       e       n       t       i       a       l       p       a       r       t       o       f       t       h       i       s       m       o       d       e       l       r       e       s       t       r       i       c       t       s       t       h       e       g       e       n       e       r       a       l       i       t       y       o       f       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       m       e       g       i       t       h       a       s       d       i       f       f       i       c       u       l       t       y       i       n       r       e       p       r       e       s       e       n       t       i       n       g       m       i       x       t       u       r       e       d       i       s       t       r       i       b       u       t       i       o       n       s       s       o       m       e       o       t       h       e       r       l       d       l       m       e       t       h       o       d       s       e       x       t       e       n       d       t       h       e       e       x       i       s       t       i       n       g       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       s       e       g       b       y       b       o       o       s       t       i       n       g       a       n       d       s       u       p       p       o       r       t       v       e       c       t       o       r       r       e       g       r       e       s       s       i       o       n       t       o       d       e       a       l       w       i       t       h       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       7       2       7       w       h       i       c       h       a       v       o       i       d       m       a       k       i       n       g       t       h       i       s       a       s       s       u       m       p       t       i       o       n       b       u       t       h       a       v       e       l       i       m       i       t       a       t       i       o       n       s       i       n       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       e       g       t       h       e       y       d       o       n       o       t       l       e       a       r       n       d       e       e       p       f       e       a       t       u       r       e       s       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       3       1       s       t       c       o       n       f       e       r       e       n       c       e       o       n       n       e       u       r       a       l       i       n       f       o       r       m       a       t       i       o       n       p       r       o       c       e       s       s       i       n       g       s       y       s       t       e       m       s       n       i       p       s       2       0       1       7       l       o       n       g       b       e       a       c       h       c       a       u       s       a       x   0   c       f       i       g       u       r       e       1       t       h       e       r       e       a       l       w       o       r       l       d       d       a       t       a       w       h       i       c       h       a       r       e       s       u       i       t       a       b       l       e       t       o       b       e       m       o       d       e       l       e       d       b       y       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       a       e       s       t       i       m       a       t       e       d       f       a       c       i       a       l       a       g       e       s       a       u       n       i       m       o       d       a       l       d       i       s       t       r       i       b       u       t       i       o       n       b       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       o       f       c       r       o       w       d       o       p       i       n       i       o       n       o       n       a       m       o       v       i       e       a       m       u       l       t       i       m       o       d       a       l       d       i       s       t       r       i       b       u       t       i       o       n       i       n       t       h       i       s       p       a       p       e       r       w       e       p       r       e       s       e       n       t       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       s       l       d       l       f       s       a       n       o       v       e       l       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       i       n       s       p       i       r       e       d       b       y       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       2       0       e       x       t       e       n       d       i       n       g       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       t       o       d       e       a       l       w       i       t       h       t       h       e       l       d       l       t       a       s       k       h       a       s       t       w       o       a       d       v       a       n       t       a       g       e       s       o       n       e       i       s       t       h       a       t       d       e       c       i       s       i       o       n       t       r       e       e       s       h       a       v       e       t       h       e       p       o       t       e       n       t       i       a       l       t       o       m       o       d       e       l       a       n       y       g       e       n       e       r       a       l       f       o       r       m       o       f       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       b       y       m       i       x       t       u       r       e       o       f       t       h       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       w       h       i       c       h       a       v       o       i       d       m       a       k       i       n       g       s       t       r       o       n       g       a       s       s       u       m       p       t       i       o       n       o       n       t       h       e       f       o       r       m       o       f       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       t       h       e       s       e       c       o       n       d       i       s       t       h       a       t       t       h       e       s       p       l       i       t       n       o       d       e       p       a       r       a       m       e       t       e       r       s       i       n       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       c       a       n       b       e       l       e       a       r       n       e       d       b       y       b       a       c       k       p       r       o       p       a       g       a       t       i       o       n       w       h       i       c       h       e       n       a       b       l       e       s       a       c       o       m       b       i       n       a       t       i       o       n       o       f       t       r       e       e       l       e       a       r       n       i       n       g       a       n       d       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       w       e       d       e       f       i       n       e       a       d       i       s       t       r       i       b       u       t       i       o       n       b       a       s       e       d       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       a       t       r       e       e       b       y       t       h       e       k       u       l       l       b       a       c       k       l       e       i       b       l       e       r       d       i       v       e       r       g       e       n       c       e       k       l       b       e       t       w       e       e       n       t       h       e       g       r       o       u       n       d       t       r       u       t       h       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       a       n       d       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       p       r       e       d       i       c       t       e       d       b       y       t       h       e       t       r       e       e       b       y       f       i       x       i       n       g       s       p       l       i       t       n       o       d       e       s       w       e       s       h       o       w       t       h       a       t       t       h       e       o       p       t       i       m       i       z       a       t       i       o       n       o       f       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       t       o       m       i       n       i       m       i       z       e       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       o       f       t       h       e       t       r       e       e       c       a       n       b       e       a       d       d       r       e       s       s       e       d       b       y       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       1       9       2       9       i       n       w       h       i       c       h       t       h       e       o       r       i       g       i       n       a       l       l       o       s       s       f       u       n       c       t       i       o       n       t       o       b       e       m       i       n       i       m       i       z       e       d       g       e       t       s       i       t       e       r       a       t       i       v       e       l       y       r       e       p       l       a       c       e       d       b       y       a       d       e       c       r       e       a       s       i       n       g       s       e       q       u       e       n       c       e       o       f       u       p       p       e       r       b       o       u       n       d       s       f       o       l       l       o       w       i       n       g       t       h       i       s       o       p       t       i       m       i       z       a       t       i       o       n       s       t       r       a       t       e       g       y       w       e       d       e       r       i       v       e       a       d       i       s       c       r       e       t       e       i       t       e       r       a       t       i       v       e       f       u       n       c       t       i       o       n       t       o       u       p       d       a       t       e       t       h       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       t       o       l       e       a       r       n       a       f       o       r       e       s       t       w       e       a       v       e       r       a       g       e       t       h       e       l       o       s       s       e       s       o       f       a       l       l       t       h       e       i       n       d       i       v       i       d       u       a       l       t       r       e       e       s       t       o       b       e       t       h       e       l       o       s       s       f       o       r       t       h       e       f       o       r       e       s       t       a       n       d       a       l       l       o       w       t       h       e       s       p       l       i       t       n       o       d       e       s       f       r       o       m       d       i       f       f       e       r       e       n       t       t       r       e       e       s       t       o       b       e       c       o       n       n       e       c       t       e       d       t       o       t       h       e       s       a       m       e       o       u       t       p       u       t       u       n       i       t       o       f       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       i       n       t       h       i       s       w       a       y       t       h       e       s       p       l       i       t       n       o       d       e       p       a       r       a       m       e       t       e       r       s       o       f       a       l       l       t       h       e       i       n       d       i       v       i       d       u       a       l       t       r       e       e       s       c       a       n       b       e       l       e       a       r       n       e       d       j       o       i       n       t       l       y       o       u       r       l       d       l       f       s       c       a       n       b       e       u       s       e       d       a       s       a       s       h       a       l       l       o       w       s       t       a       n       d       a       l       o       n       e       m       o       d       e       l       a       n       d       c       a       n       a       l       s       o       b       e       i       n       t       e       g       r       a       t       e       d       w       i       t       h       a       n       y       d       e       e       p       n       e       t       w       o       r       k       s       i       e       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       c       a       n       b       e       a       l       i       n       e       a       r       t       r       a       n       s       f       o       r       m       a       t       i       o       n       a       n       d       a       d       e       e       p       n       e       t       w       o       r       k       r       e       s       p       e       c       t       i       v       e       l       y       f       i       g       2       i       l       l       u       s       t       r       a       t       e       s       a       s       k       e       t       c       h       c       h       a       r       t       o       f       o       u       r       l       d       l       f       s       w       h       e       r       e       a       f       o       r       e       s       t       c       o       n       s       i       s       t       s       o       f       t       w       o       t       r       e       e       s       i       s       s       h       o       w       n       w       e       v       e       r       i       f       y       t       h       e       e       f       f       e       c       t       i       v       e       n       e       s       s       o       f       o       u       r       m       o       d       e       l       o       n       s       e       v       e       r       a       l       l       d       l       t       a       s       k       s       s       u       c       h       a       s       c       r       o       w       d       o       p       i       n       i       o       n       p       r       e       d       i       c       t       i       o       n       o       n       m       o       v       i       e       s       a       n       d       d       i       s       e       a       s       e       p       r       e       d       i       c       t       i       o       n       b       a       s       e       d       o       n       h       u       m       a       n       g       e       n       e       s       a       s       w       e       l       l       a       s       o       n       e       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       p       p       l       i       c       a       t       i       o       n       i       e       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       s       h       o       w       i       n       g       s       i       g       n       i       f       i       c       a       n       t       i       m       p       r       o       v       e       m       e       n       t       s       t       o       t       h       e       s       t       a       t       e       o       f       t       h       e       a       r       t       l       d       l       m       e       t       h       o       d       s       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       f       o       r       t       h       e       s       e       t       a       s       k       s       i       n       c       l       u       d       e       b       o       t       h       u       n       i       m       o       d       a       l       d       i       s       t       r       i       b       u       t       i       o       n       s       e       g       t       h       e       a       g       e       d       i       s       t       r       i       b       u       t       i       o       n       i       n       f       i       g       1       a       a       n       d       m       i       x       t       u       r       e       d       i       s       t       r       i       b       u       t       i       o       n       s       t       h       e       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       o       n       a       m       o       v       i       e       i       n       f       i       g       1       b       t       h       e       s       u       p       e       r       i       o       r       i       t       y       o       f       o       u       r       m       o       d       e       l       o       n       b       o       t       h       o       f       t       h       e       m       v       e       r       i       f       i       e       s       i       t       s       a       b       i       l       i       t       y       t       o       m       o       d       e       l       a       n       y       g       e       n       e       r       a       l       f       o       r       m       o       f       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       f       i       g       u       r       e       2       i       l       l       u       s       t       r       a       t       i       o       n       o       f       a       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       t       h       e       t       o       p       c       i       r       c       l       e       s       d       e       n       o       t       e       t       h       e       o       u       t       p       u       t       u       n       i       t       s       o       f       t       h       e       f       u       n       c       t       i       o       n       f       p       a       r       a       m       e       t       e       r       i       z       e       d       b       y       x   c   e   \\\\   x   9   8       w       h       i       c       h       c       a       n       b       e       a       f       e       a       t       u       r       e       v       e       c       t       o       r       o       r       a       f       u       l       l       y       c       o       n       n       e       c       t       e       d       l       a       y       e       r       o       f       a       d       e       e       p       n       e       t       w       o       r       k       t       h       e       b       l       u       e       a       n       d       g       r       e       e       n       c       i       r       c       l       e       s       a       r       e       s       p       l       i       t       n       o       d       e       s       a       n       d       l       e       a       f       n       o       d       e       s       r       e       s       p       e       c       t       i       v       e       l       y       t       w       o       i       n       d       e       x       f       u       n       c       t       i       o       n       x   c   f   \\\\   x   9   5       1       a       n       d       x   c   f   \\\\   x   9   5       2       a       r       e       a       s       s       i       g       n       e       d       t       o       t       h       e       s       e       t       w       o       t       r       e       e       s       r       e       s       p       e       c       t       i       v       e       l       y       t       h       e       b       l       a       c       k       d       a       s       h       a       r       r       o       w       s       i       n       d       i       c       a       t       e       t       h       e       c       o       r       r       e       s       p       o       n       d       e       n       c       e       b       e       t       w       e       e       n       t       h       e       s       p       l       i       t       n       o       d       e       s       o       f       t       h       e       s       e       t       w       o       t       r       e       e       s       a       n       d       t       h       e       o       u       t       p       u       t       u       n       i       t       s       o       f       f       u       n       c       t       i       o       n       f       n       o       t       e       t       h       a       t       o       n       e       o       u       t       p       u       t       u       n       i       t       m       a       y       c       o       r       r       e       s       p       o       n       d       t       o       t       h       e       s       p       l       i       t       n       o       d       e       s       b       e       l       o       n       g       i       n       g       t       o       d       i       f       f       e       r       e       n       t       t       r       e       e       s       e       a       c       h       t       r       e       e       h       a       s       i       n       d       e       p       e       n       d       e       n       t       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       q       d       e       n       o       t       e       d       b       y       h       i       s       t       o       g       r       a       m       s       i       n       l       e       a       f       n       o       d       e       s       t       h       e       o       u       t       p       u       t       o       f       t       h       e       f       o       r       e       s       t       i       s       a       m       i       x       t       u       r       e       o       f       t       h       e       t       r       e       e       p       r       e       d       i       c       t       i       o       n       s       f       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       a       n       d       q       a       r       e       l       e       a       r       n       e       d       j       o       i       n       t       l       y       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       2       x   0   c       2       r       e       l       a       t       e       d       w       o       r       k       s       i       n       c       e       o       u       r       l       d       l       a       l       g       o       r       i       t       h       m       i       s       i       n       s       p       i       r       e       d       b       y       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       i       t       i       s       n       e       c       e       s       s       a       r       y       t       o       f       i       r       s       t       r       e       v       i       e       w       s       o       m       e       t       y       p       i       c       a       l       t       e       c       h       n       i       q       u       e       s       o       f       d       e       c       i       s       i       o       n       t       r       e       e       s       t       h       e       n       w       e       d       i       s       c       u       s       s       c       u       r       r       e       n       t       l       d       l       m       e       t       h       o       d       s       d       e       c       i       s       i       o       n       t       r       e       e       s       r       a       n       d       o       m       f       o       r       e       s       t       s       o       r       r       a       n       d       o       m       i       z       e       d       d       e       c       i       s       i       o       n       t       r       e       e       s       1       6       1       3       4       a       r       e       a       p       o       p       u       l       a       r       e       n       s       e       m       b       l       e       p       r       e       d       i       c       t       i       v       e       m       o       d       e       l       s       u       i       t       a       b       l       e       f       o       r       m       a       n       y       m       a       c       h       i       n       e       l       e       a       r       n       i       n       g       t       a       s       k       s       i       n       t       h       e       p       a       s       t       l       e       a       r       n       i       n       g       o       f       a       d       e       c       i       s       i       o       n       t       r       e       e       w       a       s       b       a       s       e       d       o       n       h       e       u       r       i       s       t       i       c       s       s       u       c       h       a       s       a       g       r       e       e       d       y       a       l       g       o       r       i       t       h       m       w       h       e       r       e       l       o       c       a       l       l       y       o       p       t       i       m       a       l       h       a       r       d       d       e       c       i       s       i       o       n       s       a       r       e       m       a       d       e       a       t       e       a       c       h       s       p       l       i       t       n       o       d       e       1       a       n       d       t       h       u       s       c       a       n       n       o       t       b       e       i       n       t       e       g       r       a       t       e       d       i       n       t       o       i       n       a       d       e       e       p       l       e       a       r       n       i       n       g       f       r       a       m       e       w       o       r       k       i       e       b       e       c       o       m       b       i       n       e       d       w       i       t       h       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       t       h       e       n       e       w       l       y       p       r       o       p       o       s       e       d       d       e       e       p       n       e       u       r       a       l       d       e       c       i       s       i       o       n       f       o       r       e       s       t       s       d       n       d       f       s       2       0       o       v       e       r       c       o       m       e       s       t       h       i       s       p       r       o       b       l       e       m       b       y       i       n       t       r       o       d       u       c       i       n       g       a       s       o       f       t       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       f       u       n       c       t       i       o       n       a       t       t       h       e       s       p       l       i       t       n       o       d       e       s       a       n       d       a       g       l       o       b       a       l       l       o       s       s       f       u       n       c       t       i       o       n       d       e       f       i       n       e       d       o       n       a       t       r       e       e       t       h       i       s       e       n       s       u       r       e       s       t       h       a       t       t       h       e       s       p       l       i       t       n       o       d       e       p       a       r       a       m       e       t       e       r       s       c       a       n       b       e       l       e       a       r       n       e       d       b       y       b       a       c       k       p       r       o       p       a       g       a       t       i       o       n       a       n       d       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       c       a       n       b       e       u       p       d       a       t       e       d       b       y       a       d       i       s       c       r       e       t       e       i       t       e       r       a       t       i       v       e       f       u       n       c       t       i       o       n       o       u       r       m       e       t       h       o       d       e       x       t       e       n       d       s       d       n       d       f       s       t       o       a       d       d       r       e       s       s       l       d       l       p       r       o       b       l       e       m       s       b       u       t       t       h       i       s       e       x       t       e       n       s       i       o       n       i       s       n       o       n       t       r       i       v       i       a       l       b       e       c       a       u       s       e       l       e       a       r       n       i       n       g       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       i       s       a       c       o       n       s       t       r       a       i       n       e       d       c       o       n       v       e       x       o       p       t       i       m       i       z       a       t       i       o       n       p       r       o       b       l       e       m       a       l       t       h       o       u       g       h       a       s       t       e       p       s       i       z       e       f       r       e       e       u       p       d       a       t       e       f       u       n       c       t       i       o       n       w       a       s       g       i       v       e       n       i       n       d       n       d       f       s       t       o       u       p       d       a       t       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       i       t       w       a       s       o       n       l       y       p       r       o       v       e       d       t       o       c       o       n       v       e       r       g       e       f       o       r       a       c       l       a       s       s       i       f       i       c       a       t       i       o       n       l       o       s       s       c       o       n       s       e       q       u       e       n       t       l       y       i       t       w       a       s       u       n       c       l       e       a       r       h       o       w       t       o       o       b       t       a       i       n       s       u       c       h       a       n       u       p       d       a       t       e       f       u       n       c       t       i       o       n       f       o       r       o       t       h       e       r       l       o       s       s       e       s       w       e       o       b       s       e       r       v       e       d       h       o       w       e       v       e       r       t       h       a       t       t       h       e       u       p       d       a       t       e       f       u       n       c       t       i       o       n       i       n       d       n       d       f       s       c       a       n       b       e       d       e       r       i       v       e       d       f       r       o       m       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       w       h       i       c       h       a       l       l       o       w       s       u       s       t       o       e       x       t       e       n       d       i       t       t       o       o       u       r       l       d       l       l       o       s       s       i       n       a       d       d       i       t       i       o       n       t       h       e       s       t       r       a       t       e       g       i       e       s       u       s       e       d       i       n       l       d       l       f       s       a       n       d       d       n       d       f       s       t       o       l       e       a       r       n       i       n       g       t       h       e       e       n       s       e       m       b       l       e       o       f       m       u       l       t       i       p       l       e       t       r       e       e       s       f       o       r       e       s       t       s       a       r       e       d       i       f       f       e       r       e       n       t       1       w       e       e       x       p       l       i       c       i       t       l       y       d       e       f       i       n       e       a       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       f       o       r       e       s       t       s       w       h       i       l       e       o       n       l       y       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       a       s       i       n       g       l       e       t       r       e       e       w       a       s       d       e       f       i       n       e       d       i       n       d       n       d       f       s       2       w       e       a       l       l       o       w       t       h       e       s       p       l       i       t       n       o       d       e       s       f       r       o       m       d       i       f       f       e       r       e       n       t       t       r       e       e       s       t       o       b       e       c       o       n       n       e       c       t       e       d       t       o       t       h       e       s       a       m       e       o       u       t       p       u       t       u       n       i       t       o       f       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       w       h       i       l       e       d       n       d       f       s       d       i       d       n       o       t       3       a       l       l       t       r       e       e       s       i       n       l       d       l       f       s       c       a       n       b       e       l       e       a       r       n       e       d       j       o       i       n       t       l       y       w       h       i       l       e       t       r       e       e       s       i       n       d       n       d       f       s       w       e       r       e       l       e       a       r       n       e       d       a       l       t       e       r       n       a       t       i       v       e       l       y       t       h       e       s       e       c       h       a       n       g       e       s       i       n       t       h       e       e       n       s       e       m       b       l       e       l       e       a       r       n       i       n       g       a       r       e       i       m       p       o       r       t       a       n       t       b       e       c       a       u       s       e       a       s       s       h       o       w       n       i       n       o       u       r       e       x       p       e       r       i       m       e       n       t       s       s       e       c       4       4       l       d       l       f       s       c       a       n       g       e       t       b       e       t       t       e       r       r       e       s       u       l       t       s       b       y       u       s       i       n       g       m       o       r       e       t       r       e       e       s       b       u       t       b       y       u       s       i       n       g       t       h       e       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       p       r       o       p       o       s       e       d       i       n       d       n       d       f       s       t       h       e       r       e       s       u       l       t       s       o       f       f       o       r       e       s       t       s       a       r       e       e       v       e       n       w       o       r       s       e       t       h       a       n       t       h       o       s       e       f       o       r       a       s       i       n       g       l       e       t       r       e       e       t       o       s       u       m       u       p       w       r       t       d       n       d       f       s       2       0       t       h       e       c       o       n       t       r       i       b       u       t       i       o       n       s       o       f       l       d       l       f       s       a       r       e       f       i       r       s       t       w       e       e       x       t       e       n       d       f       r       o       m       c       l       a       s       s       i       f       i       c       a       t       i       o       n       2       0       t       o       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       b       y       p       r       o       p       o       s       i       n       g       a       d       i       s       t       r       i       b       u       t       i       o       n       b       a       s       e       d       l       o       s       s       f       o       r       t       h       e       f       o       r       e       s       t       s       a       n       d       d       e       r       i       v       e       t       h       e       g       r       a       d       i       e       n       t       t       o       l       e       a       r       n       s       p       l       i       t       s       n       o       d       e       s       w       r       t       t       h       i       s       l       o       s       s       s       e       c       o       n       d       w       e       d       e       r       i       v       e       d       t       h       e       u       p       d       a       t       e       f       u       n       c       t       i       o       n       f       o       r       l       e       a       f       n       o       d       e       s       b       y       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       h       a       v       i       n       g       o       b       s       e       r       v       e       d       t       h       a       t       t       h       e       u       p       d       a       t       e       f       u       n       c       t       i       o       n       i       n       2       0       w       a       s       a       s       p       e       c       i       a       l       c       a       s       e       o       f       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       l       a       s       t       b       u       t       n       o       t       t       h       e       l       e       a       s       t       w       e       p       r       o       p       o       s       e       a       b       o       v       e       t       h       r       e       e       s       t       r       a       t       e       g       i       e       s       t       o       l       e       a       r       n       i       n       g       t       h       e       e       n       s       e       m       b       l       e       o       f       m       u       l       t       i       p       l       e       t       r       e       e       s       w       h       i       c       h       a       r       e       d       i       f       f       e       r       e       n       t       f       r       o       m       2       0       b       u       t       w       e       s       h       o       w       a       r       e       e       f       f       e       c       t       i       v       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       a       n       u       m       b       e       r       o       f       s       p       e       c       i       a       l       i       z       e       d       a       l       g       o       r       i       t       h       m       s       h       a       v       e       b       e       e       n       p       r       o       p       o       s       e       d       t       o       a       d       d       r       e       s       s       t       h       e       l       d       l       t       a       s       k       a       n       d       h       a       v       e       s       h       o       w       n       t       h       e       i       r       e       f       f       e       c       t       i       v       e       n       e       s       s       i       n       m       a       n       y       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       p       p       l       i       c       a       t       i       o       n       s       s       u       c       h       a       s       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       8       1       1       2       8       e       x       p       r       e       s       s       i       o       n       r       e       c       o       g       n       i       t       i       o       n       3       0       a       n       d       h       a       n       d       o       r       i       e       n       t       a       t       i       o       n       e       s       t       i       m       a       t       i       o       n       1       0       g       e       n       g       e       t       a       l       8       d       e       f       i       n       e       d       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       a       n       i       n       s       t       a       n       c       e       a       s       a       v       e       c       t       o       r       c       o       n       t       a       i       n       i       n       g       t       h       e       p       r       o       b       a       b       i       l       i       t       i       e       s       o       f       t       h       e       i       n       s       t       a       n       c       e       h       a       v       i       n       g       e       a       c       h       l       a       b       e       l       t       h       e       y       a       l       s       o       g       a       v       e       a       s       t       r       a       t       e       g       y       t       o       a       s       s       i       g       n       a       p       r       o       p       e       r       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       t       o       a       n       i       n       s       t       a       n       c       e       w       i       t       h       a       s       i       n       g       l       e       l       a       b       e       l       i       e       a       s       s       i       g       n       i       n       g       a       g       a       u       s       s       i       a       n       o       r       t       r       i       a       n       g       l       e       d       i       s       t       r       i       b       u       t       i       o       n       w       h       o       s       e       p       e       a       k       i       s       t       h       e       s       i       n       g       l       e       l       a       b       e       l       a       n       d       p       r       o       p       o       s       e       d       a       n       a       l       g       o       r       i       t       h       m       c       a       l       l       e       d       i       i       s       l       l       d       w       h       i       c       h       i       s       a       n       i       t       e       r       a       t       i       v       e       o       p       t       i       m       i       z       a       t       i       o       n       p       r       o       c       e       s       s       b       a       s       e       d       o       n       a       t       w       o       l       a       y       e       r       e       n       e       r       g       y       b       a       s       e       d       m       o       d       e       l       y       a       n       g       e       t       a       l       2       8       t       h       e       n       d       e       f       i       n       e       d       a       t       h       r       e       e       l       a       y       e       r       e       n       e       r       g       y       b       a       s       e       d       m       o       d       e       l       c       a       l       l       e       d       s       c       e       l       d       l       i       n       w       h       i       c       h       t       h       e       a       b       i       l       i       t       y       t       o       p       e       r       f       o       r       m       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       i       s       i       m       p       r       o       v       e       d       b       y       a       d       d       i       n       g       t       h       e       e       x       t       r       a       h       i       d       d       e       n       l       a       y       e       r       a       n       d       s       p       a       r       s       i       t       y       c       o       n       s       t       r       a       i       n       t       s       a       r       e       a       l       s       o       i       n       c       o       r       p       o       r       a       t       e       d       t       o       a       m       e       l       i       o       r       a       t       e       t       h       e       m       o       d       e       l       g       e       n       g       6       d       e       v       e       l       o       p       e       d       a       n       a       c       c       e       l       e       r       a       t       e       d       v       e       r       s       i       o       n       o       f       i       i       s       l       l       d       c       a       l       l       e       d       b       f       g       s       l       d       l       b       y       u       s       i       n       g       q       u       a       s       i       n       e       w       t       o       n       o       p       t       i       m       i       z       a       t       i       o       n       a       l       l       t       h       e       a       b       o       v       e       l       d       l       m       e       t       h       o       d       s       a       s       s       u       m       e       t       h       a       t       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       c       a       n       b       e       r       e       p       r       e       s       e       n       t       e       d       b       y       a       m       a       x       i       m       u       m       e       n       t       r       o       p       y       m       o       d       e       l       2       b       u       t       t       h       e       e       x       p       o       n       e       n       t       i       a       l       p       a       r       t       o       f       t       h       i       s       m       o       d       e       l       r       e       s       t       r       i       c       t       s       t       h       e       g       e       n       e       r       a       l       i       t       y       o       f       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       m       a       n       o       t       h       e       r       w       a       y       t       o       a       d       d       r       e       s       s       t       h       e       l       d       l       t       a       s       k       i       s       t       o       e       x       t       e       n       d       e       x       i       s       t       i       n       g       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       s       t       o       d       e       a       l       w       i       t       h       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       g       e       n       g       a       n       d       h       o       u       7       p       r       o       p       o       s       e       d       l       d       s       v       r       a       l       d       l       m       e       t       h       o       d       b       y       e       x       t       e       n       d       i       n       g       s       u       p       p       o       r       t       v       e       c       t       o       r       r       e       g       r       e       s       s       o       r       w       h       i       c       h       f       i       t       a       s       i       g       m       o       i       d       f       u       n       c       t       i       o       n       t       o       e       a       c       h       c       o       m       p       o       n       e       n       t       o       f       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       i       m       u       l       t       a       n       e       o       u       s       l       y       b       y       a       s       u       p       p       o       r       t       v       e       c       t       o       r       m       a       c       h       i       n       e       x       i       n       g       e       t       a       l       2       7       t       h       e       n       e       x       t       e       n       d       e       d       b       o       o       s       t       i       n       g       t       o       a       d       d       r       e       s       s       t       h       e       l       d       l       t       a       s       k       b       y       a       d       d       i       t       i       v       e       w       e       i       g       h       t       e       d       r       e       g       r       e       s       s       o       r       s       t       h       e       y       s       h       o       w       e       d       t       h       a       t       u       s       i       n       g       t       h       e       v       e       c       t       o       r       t       r       e       e       m       o       d       e       l       a       s       t       h       e       w       e       a       k       r       e       g       r       e       s       s       o       r       c       a       n       l       e       a       d       t       o       b       e       t       t       e       r       p       e       r       f       o       r       m       a       n       c       e       a       n       d       n       a       m       e       d       t       h       i       s       m       e       t       h       o       d       a       o       s       o       l       d       l       l       o       g       i       t       b       o       o       s       t       a       s       t       h       e       l       e       a       r       n       i       n       g       o       f       t       h       i       s       t       r       e       e       m       o       d       e       l       i       s       b       a       s       e       d       o       n       l       o       c       a       l       l       y       o       p       t       i       m       a       l       h       a       r       d       d       a       t       a       p       a       r       t       i       t       i       o       n       f       u       n       c       t       i       o       n       s       a       t       e       a       c       h       s       p       l       i       t       n       o       d       e       a       o       s       o       l       d       l       l       o       g       i       t       b       o       o       s       t       i       s       u       n       a       b       l       e       t       o       b       e       c       o       m       b       i       n       e       d       w       i       t       h       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       e       x       t       e       n       d       i       n       g       c       u       r       r       e       n       t       d       e       e       p       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       s       t       o       3       x   0   c       a       d       d       r       e       s       s       t       h       e       l       d       l       t       a       s       k       i       s       a       n       i       n       t       e       r       e       s       t       i       n       g       t       o       p       i       c       b       u       t       t       h       e       e       x       i       s       t       i       n       g       s       u       c       h       a       m       e       t       h       o       d       c       a       l       l       e       d       d       l       d       l       5       s       t       i       l       l       f       o       c       u       s       e       s       o       n       m       a       x       i       m       u       m       e       n       t       r       o       p       y       m       o       d       e       l       b       a       s       e       d       l       d       l       o       u       r       m       e       t       h       o       d       l       d       l       f       s       e       x       t       e       n       d       s       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       t       o       a       d       d       r       e       s       s       l       d       l       t       a       s       k       s       i       n       w       h       i       c       h       t       h       e       p       r       e       d       i       c       t       e       d       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       a       s       a       m       p       l       e       c       a       n       b       e       e       x       p       r       e       s       s       e       d       b       y       a       l       i       n       e       a       r       c       o       m       b       i       n       a       t       i       o       n       o       f       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       o       f       t       h       e       t       r       a       i       n       i       n       g       d       a       t       a       a       n       d       t       h       u       s       h       a       v       e       n       o       r       e       s       t       r       i       c       t       i       o       n       s       o       n       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       e       g       n       o       r       e       q       u       i       r       e       m       e       n       t       o       f       t       h       e       m       a       x       i       m       u       m       e       n       t       r       o       p       y       m       o       d       e       l       i       n       a       d       d       i       t       i       o       n       t       h       a       n       k       s       t       o       t       h       e       i       n       t       r       o       d       u       c       t       i       o       n       o       f       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       f       u       n       c       t       i       o       n       s       l       d       l       f       s       c       a       n       b       e       c       o       m       b       i       n       e       d       w       i       t       h       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       e       g       t       o       l       e       a       r       n       d       e       e       p       f       e       a       t       u       r       e       s       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       3       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       s       a       f       o       r       e       s       t       i       s       a       n       e       n       s       e       m       b       l       e       o       f       d       e       c       i       s       i       o       n       t       r       e       e       s       w       e       f       i       r       s       t       i       n       t       r       o       d       u       c       e       h       o       w       t       o       l       e       a       r       n       a       s       i       n       g       l       e       d       e       c       i       s       i       o       n       t       r       e       e       b       y       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       t       h       e       n       d       e       s       c       r       i       b       e       t       h       e       l       e       a       r       n       i       n       g       o       f       a       f       o       r       e       s       t       3       1       p       r       o       b       l       e       m       f       o       r       m       u       l       a       t       i       o       n       l       e       t       x       r       m       d       e       n       o       t       e       t       h       e       i       n       p       u       t       s       p       a       c       e       a       n       d       y       y       1       y       2       y       c       d       e       n       o       t       e       t       h       e       c       o       m       p       l       e       t       e       s       e       t       o       f       l       a       b       e       l       s       w       h       e       r       e       c       i       s       t       h       e       n       u       m       b       e       r       o       f       p       o       s       s       i       b       l       e       l       a       b       e       l       v       a       l       u       e       s       w       e       c       o       n       s       i       d       e       r       a       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       l       d       l       p       r       o       b       l       e       m       w       h       e       r       e       f       o       r       e       a       c       h       i       n       p       u       t       s       a       m       p       l       e       x       x   e   2   \\\\   x   8   8   \\\\   x   8   8       x       t       h       e       r       e       i       s       a       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       d       d       x       y       1       d       y       x       2       d       y       x       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       r       c       h       e       r       e       d       y       x       c       e       x       p       r       e       s       s       e       s       t       h       e       p       r       o       b       a       b       i       l       i       t       y       o       f       t       h       e       s       a       m       p       l       e       x       h       a       v       i       n       g       t       h       e       c       t       h       l       a       b       e       l       y       c       a       n       d       t       h       u       s       h       a       s       t       h       e       p       c       c       o       n       s       t       r       a       i       n       t       s       t       h       a       t       d       y       x       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       a       n       d       c       1       d       y       x       c       1       t       h       e       g       o       a       l       o       f       t       h       e       l       d       l       p       r       o       b       l       e       m       i       s       t       o       l       e       a       r       n       a       m       a       p       p       i       n       g       f       u       n       c       t       i       o       n       g       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       d       b       e       t       w       e       e       n       a       n       i       n       p       u       t       s       a       m       p       l       e       x       a       n       d       i       t       s       c       o       r       r       e       s       p       o       n       d       i       n       g       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       d       h       e       r       e       w       e       w       a       n       t       t       o       l       e       a       r       n       t       h       e       m       a       p       p       i       n       g       f       u       n       c       t       i       o       n       g       x       b       y       a       d       e       c       i       s       i       o       n       t       r       e       e       b       a       s       e       d       m       o       d       e       l       t       a       d       e       c       i       s       i       o       n       t       r       e       e       c       o       n       s       i       s       t       s       o       f       a       s       e       t       o       f       s       p       l       i       t       n       o       d       e       s       n       a       n       d       a       s       e       t       o       f       l       e       a       f       n       o       d       e       s       l       e       a       c       h       s       p       l       i       t       n       o       d       e       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       d       e       f       i       n       e       s       a       s       p       l       i       t       f       u       n       c       t       i       o       n       s       n       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       0       1       p       a       r       a       m       e       t       e       r       i       z       e       d       b       y       x   c   e   \\\\   x   9   8       t       o       d       e       t       e       r       m       i       n       e       w       h       e       t       h       e       r       a       s       a       m       p       l       e       i       s       s       e       n       t       t       o       t       h       e       l       e       f       t       o       r       r       i       g       h       t       s       u       b       t       r       e       e       e       a       c       h       l       e       a       f       n       o       d       e       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       h       o       l       d       s       a       d       i       s       t       r       i       b       u       t       i       o       n       q       q       1       q       2       q       c       p       c       o       v       e       r       y       i       e       q       c       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       a       n       d       c       1       q       c       1       t       o       b       u       i       l       d       a       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       f       o       l       l       o       w       i       n       g       2       0       w       e       u       s       e       a       p       r       o       b       a       b       i       l       i       s       t       i       c       s       p       l       i       t       f       u       n       c       t       i       o       n       s       n       x       x   c   e   \\\\   x   9   8       x   c   f   \\\\   x   8   3       f       x   c   f   \\\\   x   9   5       n       x       x   c   e   \\\\   x   9   8       w       h       e       r       e       x   c   f   \\\\   x   8   3       x   c   2   \\\\   x   b   7       i       s       a       s       i       g       m       o       i       d       f       u       n       c       t       i       o       n       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       i       s       a       n       i       n       d       e       x       f       u       n       c       t       i       o       n       t       o       b       r       i       n       g       t       h       e       x   c   f   \\\\   x   9   5       n       t       h       o       u       t       p       u       t       o       f       f       u       n       c       t       i       o       n       f       x       x   c   e   \\\\   x   9   8       i       n       c       o       r       r       e       s       p       o       n       d       e       n       c       e       w       i       t       h       s       p       l       i       t       n       o       d       e       n       a       n       d       f       x       x   e   2   \\\\   x   8   6   \\\\   x   9   2       r       m       i       s       a       r       e       a       l       v       a       l       u       e       d       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       d       e       p       e       n       d       i       n       g       o       n       t       h       e       s       a       m       p       l       e       x       a       n       d       t       h       e       p       a       r       a       m       e       t       e       r       x   c   e   \\\\   x   9   8       a       n       d       c       a       n       t       a       k       e       a       n       y       f       o       r       m       f       o       r       a       s       i       m       p       l       e       f       o       r       m       i       t       c       a       n       b       e       a       l       i       n       e       a       r       t       r       a       n       s       f       o       r       m       a       t       i       o       n       o       f       x       w       h       e       r       e       x   c   e   \\\\   x   9   8       i       s       t       h       e       t       r       a       n       s       f       o       r       m       a       t       i       o       n       m       a       t       r       i       x       f       o       r       a       c       o       m       p       l       e       x       f       o       r       m       i       t       c       a       n       b       e       a       d       e       e       p       n       e       t       w       o       r       k       t       o       p       e       r       f       o       r       m       r       e       p       r       e       s       e       n       t       a       t       i       o       n       l       e       a       r       n       i       n       g       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       t       h       e       n       x   c   e   \\\\   x   9   8       i       s       t       h       e       n       e       t       w       o       r       k       p       a       r       a       m       e       t       e       r       t       h       e       c       o       r       r       e       s       p       o       n       d       e       n       c       e       b       e       t       w       e       e       n       t       h       e       s       p       l       i       t       n       o       d       e       s       a       n       d       t       h       e       o       u       t       p       u       t       u       n       i       t       s       o       f       f       u       n       c       t       i       o       n       f       i       n       d       i       c       a       t       e       d       b       y       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       t       h       a       t       i       s       r       a       n       d       o       m       l       y       g       e       n       e       r       a       t       e       d       b       e       f       o       r       e       t       r       e       e       l       e       a       r       n       i       n       g       i       e       w       h       i       c       h       o       u       t       p       u       t       u       n       i       t       s       f       r       o       m       x   e   2   \\\\   x   8   0   \\\\   x   9   c       f       x   e   2   \\\\   x   8   0   \\\\   x   9   d       a       r       e       u       s       e       d       f       o       r       c       o       n       s       t       r       u       c       t       i       n       g       a       t       r       e       e       i       s       d       e       t       e       r       m       i       n       e       d       r       a       n       d       o       m       l       y       a       n       e       x       a       m       p       l       e       t       o       d       e       m       o       n       s       t       r       a       t       e       x   c   f   \\\\   x   9   5       x   c   2   \\\\   x   b   7       i       s       s       h       o       w       n       i       n       f       i       g       2       t       h       e       n       t       h       e       p       r       o       b       a       b       i       l       i       t       y       o       f       t       h       e       s       a       m       p       l       e       x       f       a       l       l       i       n       g       i       n       t       o       l       e       a       f       n       o       d       e       i       s       g       i       v       e       n       b       y       y       l       r       p       x       x   c   e   \\\\   x   9   8       s       n       x       x   c   e   \\\\   x   9   8       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       n       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       s       n       x       x   c   e   \\\\   x   9   8       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       n       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       w       h       e       r       e       1       x   c   2   \\\\   x   b   7       i       s       a       n       i       n       d       i       c       a       t       o       r       f       u       n       c       t       i       o       n       a       n       d       l       l       n       a       n       d       l       r       n       d       e       n       o       t       e       t       h       e       s       e       t       s       o       f       l       e       a       f       n       o       d       e       s       h       e       l       d       b       y       t       h       e       l       e       f       t       a       n       d       r       i       g       h       t       s       u       b       t       r       e       e       s       o       f       n       o       d       e       n       t       n       l       a       n       d       t       n       r       r       e       s       p       e       c       t       i       v       e       l       y       t       h       e       o       u       t       p       u       t       o       f       t       h       e       t       r       e       e       t       w       r       t       x       i       e       t       h       e       m       a       p       p       i       n       g       f       u       n       c       t       i       o       n       g       i       s       d       e       f       i       n       e       d       b       y       x       g       x       x   c   e   \\\\   x   9   8       t       p       x       x   c   e   \\\\   x   9   8       q       2       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       3       2       t       r       e       e       o       p       t       i       m       i       z       a       t       i       o       n       g       i       v       e       n       a       t       r       a       i       n       i       n       g       s       e       t       s       x       i       d       i       n       i       1       o       u       r       g       o       a       l       i       s       t       o       l       e       a       r       n       a       d       e       c       i       s       i       o       n       t       r       e       e       t       d       e       s       c       r       i       b       e       d       i       n       s       e       c       3       1       w       h       i       c       h       c       a       n       o       u       t       p       u       t       a       d       i       s       t       r       i       b       u       t       i       o       n       g       x       i       x   c   e   \\\\   x   9   8       t       s       i       m       i       l       a       r       t       o       d       i       f       o       r       e       a       c       h       s       a       m       p       l       e       x       i       t       o       t       h       i       s       e       n       d       a       s       t       r       a       i       g       h       t       f       o       r       w       a       r       d       w       a       y       i       s       t       o       m       i       n       i       m       i       z       e       t       h       e       k       u       l       l       b       a       c       k       l       e       i       b       l       e       r       k       l       d       i       v       e       r       g       e       n       c       e       b       e       t       w       e       e       n       e       a       c       h       g       x       i       x   c   e   \\\\   x   9   8       t       a       n       d       d       i       o       r       e       q       u       i       v       a       l       e       n       t       l       y       t       o       m       i       n       i       m       i       z       e       t       h       e       f       o       l       l       o       w       i       n       g       c       r       o       s       s       e       n       t       r       o       p       y       l       o       s       s       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       c       n       c       x   1   0       x       x   1   1       1       x       x       y       c       1       x       x       y       c       d       x       i       l       o       g       g       c       x       i       x   c   e   \\\\   x   9   8       t       x   e   2   \\\\   x   8   8   \\\\   x   9   2       d       x       i       l       o       g       p       x       i       x   c   e   \\\\   x   9   8       q       c       3       n       i       1       c       1       n       i       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       4       x   0   c       w       h       e       r       e       q       d       e       n       o       t       e       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       h       e       l       d       b       y       a       l       l       t       h       e       l       e       a       f       n       o       d       e       s       l       a       n       d       g       c       x       i       x   c   e   \\\\   x   9   8       t       i       s       t       h       e       c       t       h       o       u       t       p       u       t       u       n       i       t       o       f       g       x       i       x   c   e   \\\\   x   9   8       t       l       e       a       r       n       i       n       g       t       h       e       t       r       e       e       t       r       e       q       u       i       r       e       s       t       h       e       e       s       t       i       m       a       t       i       o       n       o       f       t       w       o       p       a       r       a       m       e       t       e       r       s       1       t       h       e       s       p       l       i       t       n       o       d       e       p       a       r       a       m       e       t       e       r       x   c   e   \\\\   x   9   8       a       n       d       2       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       q       h       e       l       d       b       y       t       h       e       l       e       a       f       n       o       d       e       s       t       h       e       b       e       s       t       p       a       r       a       m       e       t       e       r       s       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   7       q       x   e   2   \\\\   x   8   8   \\\\   x   9   7       a       r       e       d       e       t       e       r       m       i       n       e       d       b       y       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   7       q       x   e   2   \\\\   x   8   8   \\\\   x   9   7       a       r       g       m       i       n       r       q       x   c   e   \\\\   x   9   8       s       4       x   c   e   \\\\   x   9   8       q       t       o       s       o       l       v       e       e       q       n       4       w       e       c       o       n       s       i       d       e       r       a       n       a       l       t       e       r       n       a       t       i       n       g       o       p       t       i       m       i       z       a       t       i       o       n       s       t       r       a       t       e       g       y       f       i       r       s       t       w       e       f       i       x       q       a       n       d       o       p       t       i       m       i       z       e       x   c   e   \\\\   x   9   8       t       h       e       n       w       e       f       i       x       x   c   e   \\\\   x   9   8       a       n       d       o       p       t       i       m       i       z       e       q       t       h       e       s       e       t       w       o       l       e       a       r       n       i       n       g       s       t       e       p       s       a       r       e       a       l       t       e       r       n       a       t       i       v       e       l       y       p       e       r       f       o       r       m       e       d       u       n       t       i       l       c       o       n       v       e       r       g       e       n       c       e       o       r       a       m       a       x       i       m       u       m       n       u       m       b       e       r       o       f       i       t       e       r       a       t       i       o       n       s       i       s       r       e       a       c       h       e       d       d       e       f       i       n       e       d       i       n       t       h       e       e       x       p       e       r       i       m       e       n       t       s       3       2       1       l       e       a       r       n       i       n       g       s       p       l       i       t       n       o       d       e       s       i       n       t       h       i       s       s       e       c       t       i       o       n       w       e       d       e       s       c       r       i       b       e       h       o       w       t       o       l       e       a       r       n       t       h       e       p       a       r       a       m       e       t       e       r       x   c   e   \\\\   x   9   8       f       o       r       s       p       l       i       t       n       o       d       e       s       w       h       e       n       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       h       e       l       d       b       y       t       h       e       l       e       a       f       n       o       d       e       s       q       a       r       e       f       i       x       e       d       w       e       c       o       m       p       u       t       e       t       h       e       g       r       a       d       i       e       n       t       o       f       t       h       e       l       o       s       s       r       q       x   c   e   \\\\   x   9   8       s       w       r       t       x   c   e   \\\\   x   9   8       b       y       t       h       e       c       h       a       i       n       r       u       l       e       n       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       x       x       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       i       x   c   e   \\\\   x   9   8       5       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       i       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       i       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       w       h       e       r       e       o       n       l       y       t       h       e       f       i       r       s       t       t       e       r       m       d       e       p       e       n       d       s       o       n       t       h       e       t       r       e       e       a       n       d       t       h       e       s       e       c       o       n       d       t       e       r       m       d       e       p       e       n       d       s       o       n       t       h       e       s       p       e       c       i       f       i       c       t       y       p       e       o       f       t       h       e       f       u       n       c       t       i       o       n       f       x   c   f   \\\\   x   9   5       n       t       h       e       f       i       r       s       t       t       e       r       m       i       s       g       i       v       e       n       b       y       c       x   0   1       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       l       x   1   1       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       r       1       x       y       c       x   1   0       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       q       x   c   e   \\\\   x   9   8       s       d       x       i       s       n       x       i       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       s       n       x       i       x   c   e   \\\\   x   9   8       6       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       n       x       i       x   c   e   \\\\   x   9   8       n       c       1       g       c       x       i       x   c   e   \\\\   x   9   8       t       g       c       x       i       x   c   e   \\\\   x   9   8       t       p       p       w       h       e       r       e       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       l       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       l       n       p       x       i       x   c   e   \\\\   x   9   8       q       c       a       n       d       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       r       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       r       n       p       x       i       x   c   e   \\\\   x   9   8       q       c       n       o       t       e       t       h       a       t       l       e       t       t       n       b       e       t       h       e       t       r       e       e       r       o       o       t       e       d       a       t       t       h       e       n       o       d       e       n       t       h       e       n       w       e       h       a       v       e       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       l       g       c       x       i       x   c   e   \\\\   x   9   8       t       n       r       t       h       i       s       m       e       a       n       s       t       h       e       g       r       a       d       i       e       n       t       c       o       m       p       u       t       a       t       i       o       n       i       n       e       q       n       6       c       a       n       b       e       s       t       a       r       t       e       d       a       t       t       h       e       l       e       a       f       n       o       d       e       s       a       n       d       c       a       r       r       i       e       d       o       u       t       i       n       a       b       o       t       t       o       m       u       p       m       a       n       n       e       r       t       h       u       s       t       h       e       s       p       l       i       t       n       o       d       e       p       a       r       a       m       e       t       e       r       s       c       a       n       b       e       l       e       a       r       n       e       d       b       y       s       t       a       n       d       a       r       d       b       a       c       k       p       r       o       p       a       g       a       t       i       o       n       3       2       2       l       e       a       r       n       i       n       g       l       e       a       f       n       o       d       e       s       n       o       w       f       i       x       i       n       g       t       h       e       p       a       r       a       m       e       t       e       r       x   c   e   \\\\   x   9   8       w       e       s       h       o       w       h       o       w       t       o       l       e       a       r       n       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       s       h       e       l       d       b       y       t       h       e       l       e       a       f       n       o       d       e       s       q       w       h       i       c       h       i       s       a       c       o       n       s       t       r       a       i       n       e       d       o       p       t       i       m       i       z       a       t       i       o       n       p       r       o       b       l       e       m       m       i       n       r       q       x   c   e   \\\\   x   9   8       s       s       t       x   e   2   \\\\   x   8   8   \\\\   x   8   0       q       c       x       q       c       1       7       c       1       h       e       r       e       w       e       p       r       o       p       o       s       e       t       o       a       d       d       r       e       s       s       t       h       i       s       c       o       n       s       t       r       a       i       n       e       d       c       o       n       v       e       x       o       p       t       i       m       i       z       a       t       i       o       n       p       r       o       b       l       e       m       b       y       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       1       9       2       9       w       h       i       c       h       l       e       a       d       s       t       o       a       s       t       e       p       s       i       z       e       f       r       e       e       a       n       d       f       a       s       t       c       o       n       v       e       r       g       e       d       u       p       d       a       t       e       r       u       l       e       f       o       r       q       i       n       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       a       n       o       r       i       g       i       n       a       l       o       b       j       e       c       t       i       v       e       f       u       n       c       t       i       o       n       t       o       b       e       m       i       n       i       m       i       z       e       d       g       e       t       s       r       e       p       l       a       c       e       d       b       y       i       t       s       b       o       u       n       d       i       n       a       n       i       t       e       r       a       t       i       v       e       m       a       n       n       e       r       a       u       p       p       e       r       b       o       u       n       d       f       o       r       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       r       q       x   c   e   \\\\   x   9   8       s       c       a       n       b       e       o       b       t       a       i       n       e       d       b       y       j       e       n       s       e       n       x   e   2   \\\\   x   8   0   \\\\   x   9   9       s       i       n       e       q       u       a       l       i       t       y       r       q       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   8   \\\\   x   9   2       n       c       x   1   0       x       x   1   1       1       x       x       y       c       d       x       i       l       o       g       p       x       i       x   c   e   \\\\   x   9   8       q       c       n       i       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       x   e   2   \\\\   x   8   9   \\\\   x   a   4       x   e   2   \\\\   x   8   8   \\\\   x   9   2       w       h       e       r       e       x   c   e   \\\\   x   b       q       c       x       i       1       n       n       x       c       x       i       1       c       1       p       x       i       x   c   e   \\\\   x   9   8       q       c       g       c       x       i       x   c   e   \\\\   x   9   8       t       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       x   e   2   \\\\   x   8   8   \\\\   x   9   2       d       y       x       c       i       x       x   c   e   \\\\   x   b       q       x   c   c   \\\\   x   8   4       c       x       i       l       o       g       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       x   1   0       p       x       x   c   e   \\\\   x   9   8       q       x   1   1       i       c       x   c   e   \\\\   x   b       q       x   c   c   \\\\   x   8   4       c       x       i       8       w       e       d       e       f       i       n       e       n       c       x   1   0       p       x       x   c   e   \\\\   x   9   8       q       x   1   1       1       x       x       y       c       x       i       c       d       x       i       x   c   e   \\\\   x   b       q       x   c   c   \\\\   x   8   4       c       x       i       l       o       g       n       i       1       c       1       x   c   e   \\\\   x   b       q       x   c   c   \\\\   x   8   4       c       x       i       9       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       t       h       e       n       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       i       s       a       n       u       p       p       e       r       b       o       u       n       d       f       o       r       r       q       x   c   e   \\\\   x   9   8       s       w       h       i       c       h       h       a       s       t       h       e       p       r       o       p       e       r       t       y       t       h       a       t       f       o       r       a       n       y       q       a       n       d       q       x   c   c   \\\\   x   8   4       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       x   e   2   \\\\   x   8   9   \\\\   x   a   5       r       q       x   c   e   \\\\   x   9   8       s       a       n       d       x   c   f   \\\\   x   8   6       q       q       r       q       x   c   e   \\\\   x   9   8       s       a       s       s       u       m       e       t       h       a       t       w       e       a       r       e       a       t       a       p       o       i       n       t       q       t       c       o       r       r       e       s       p       o       n       d       i       n       g       t       o       t       h       e       t       t       h       i       t       e       r       a       t       i       o       n       t       h       e       n       x   c   f   \\\\   x   8   6       q       q       t       i       s       a       n       u       p       p       e       r       b       o       u       n       d       f       o       r       r       q       x   c   e   \\\\   x   9   8       s       i       n       t       h       e       n       e       x       t       i       t       e       r       a       t       i       o       n       q       t       1       i       s       c       h       o       s       e       n       s       u       c       h       t       h       a       t       x   c   f   \\\\   x   8   6       q       t       1       q       x   e   2   \\\\   x   8   9   \\\\   x   a   4       r       q       t       x   c   e   \\\\   x   9   8       s       w       h       i       c       h       i       m       p       l       i       e       s       r       q       t       1       x   c   e   \\\\   x   9   8       s       x   e   2   \\\\   x   8   9   \\\\   x   a   4       r       q       t       x   c   e   \\\\   x   9   8       s       5       x   0   c       c       o       n       s       e       q       u       e       n       t       l       y       w       e       c       a       n       m       i       n       i       m       i       z       e       x   c   f   \\\\   x   8   6       q       q       x   c   c   \\\\   x   8   4       i       n       s       t       e       a       d       o       f       r       q       x   c   e   \\\\   x   9   8       s       a       f       t       e       r       e       n       s       u       r       i       n       g       t       h       a       t       r       q       t       x   c   e   \\\\   x   9   8       s       x   c   f   \\\\   x   8   6       q       t       q       x   c   c   \\\\   x   8   4       i       e       q       x   c   c   \\\\   x   8   4       q       t       s       o       w       e       h       a       v       e       q       t       1       a       r       g       m       i       n       x   c   f   \\\\   x   8   6       q       q       t       s       t       x   e   2   \\\\   x   8   8   \\\\   x   8   0       q       c       x       q       c       1       1       0       c       1       w       h       i       c       h       l       e       a       d       s       t       o       m       i       n       i       m       i       z       i       n       g       t       h       e       l       a       g       r       a       n       g       i       a       n       d       e       f       i       n       e       d       b       y       x   c   f   \\\\   x   9   5       q       q       t       x   c   f   \\\\   x   8   6       q       q       t       x       x   c   e   \\\\   x   b   b       x   e   2   \\\\   x   8   8   \\\\   x   8   8       l       w       h       e       r       e       x   c   e   \\\\   x   b   b       i       s       t       h       e       l       a       g       r       a       n       g       e       m       u       l       t       i       p       l       i       e       r       b       y       s       e       t       t       i       n       g       x   c   e   \\\\   x   b   b       t       1       n       o       t       e       t       h       a       t       q       c       t       1       x   e   2   \\\\   x   8   8   \\\\   x   8   8       0       1       a       n       d       d       i       s       t       r       i       b       u       t       i       o       n       s       h       e       l       d       b       y       t       h       e       l       e       a       f       n       o       d       e       s       t       h       e       s       t       a       r       t       i       n       g       0       d       i       s       t       r       i       b       u       t       i       o       n       q       c       c       1       3       3       q       c       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       1       1       c       1       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   f   \\\\   x   9   5       q       q       t       x   e   2   \\\\   x   8   8   \\\\   x   8   2       q       c       n       c       1       x       x       y       c       t       t       1       d       x   c   e   \\\\   x   b       q       c       x       i       a       n       d       q       c       n       i       1       c       1       x       i       s       a       t       i       s       f       i       e       s       t       h       a       t       q       c       c       x       0       w       e       h       a       v       e       p       n       y       c       t       d       x       i       x   c   e   \\\\   x   b       q       c       x       i       p       c       i       1       p       n       y       c       t       x   c   e   \\\\   x   b       q       x       d       x       i       i       c       1       i       1       c       1       2       t       1       1       e       q       n       1       2       i       s       t       h       e       u       p       d       a       t       e       s       c       h       e       m       e       f       o       r       c       1       q       c       0       p       o       i       n       t       q       c       a       n       b       e       s       i       m       p       l       y       i       n       i       t       i       a       l       i       z       e       d       b       y       t       h       e       u       n       i       f       o       r       m       p       c       l       e       a       r       n       i       n       g       a       f       o       r       e       s       t       a       f       o       r       e       s       t       i       s       a       n       e       n       s       e       m       b       l       e       o       f       d       e       c       i       s       i       o       n       t       r       e       e       s       f       t       1       t       k       i       n       t       h       e       t       r       a       i       n       i       n       g       s       t       a       g       e       a       l       l       t       r       e       e       s       i       n       t       h       e       f       o       r       e       s       t       f       u       s       e       t       h       e       s       a       m       e       p       a       r       a       m       e       t       e       r       s       x   c   e   \\\\   x   9   8       f       o       r       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       f       x   c   2   \\\\   x   b   7       x   c   e   \\\\   x   9   8       b       u       t       c       o       r       r       e       s       p       o       n       d       t       o       d       i       f       f       e       r       e       n       t       o       u       t       p       u       t       u       n       i       t       s       o       f       f       a       s       s       i       g       n       e       d       b       y       x   c   f   \\\\   x   9   5       s       e       e       f       i       g       2       b       u       t       e       a       c       h       t       r       e       e       h       a       s       i       n       d       e       p       e       n       d       e       n       t       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       q       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       a       f       o       r       e       s       t       i       s       g       i       v       e       n       b       y       a       v       e       r       a       g       i       n       g       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       s       f       o       r       a       l       l       i       n       d       i       v       i       d       u       a       l       t       r       e       e       s       p       k       1       r       f       k       k       1       r       t       k       w       h       e       r       e       r       t       k       i       s       t       h       e       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       t       r       e       e       t       k       d       e       f       i       n       e       d       b       y       e       q       n       3       t       o       l       e       a       r       n       x   c   e   \\\\   x   9   8       b       y       f       i       x       i       n       g       t       h       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       q       o       f       a       l       l       t       h       e       t       r       e       e       s       i       n       t       h       e       f       o       r       e       s       t       f       b       a       s       e       d       o       n       t       h       e       d       e       r       i       v       a       t       i       o       n       i       n       s       e       c       3       2       a       n       d       r       e       f       e       r       r       i       n       g       t       o       f       i       g       2       w       e       h       a       v       e       n       k       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       k       n       x       i       x   c   e   \\\\   x   9   8       1       x       x       x       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       f       x   e   2   \\\\   x   8   8   \\\\   x   8   2       r       t       k       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       k       i       1       x   e   2   \\\\   x   8   8   \\\\   x   8   2       f       x   c   f   \\\\   x   9   5       k       n       x       i       x   c   e   \\\\   x   9   8       x   e   2   \\\\   x   8   8   \\\\   x   8   2       x   c   e   \\\\   x   9   8       1       3       k       1       n       x   e   2   \\\\   x   8   8   \\\\   x   8   8       n       k       w       h       e       r       e       n       k       a       n       d       x   c   f   \\\\   x   9   5       k       x   c   2   \\\\   x   b   7       a       r       e       t       h       e       s       p       l       i       t       n       o       d       e       s       e       t       a       n       d       t       h       e       i       n       d       e       x       f       u       n       c       t       i       o       n       o       f       t       k       r       e       s       p       e       c       t       i       v       e       l       y       n       o       t       e       t       h       a       t       t       h       e       i       n       d       e       x       f       u       n       c       t       i       o       n       x   c   f   \\\\   x   9   5       k       x   c   2   \\\\   x   b   7       f       o       r       e       a       c       h       t       r       e       e       i       s       r       a       n       d       o       m       l       y       a       s       s       i       g       n       e       d       b       e       f       o       r       e       t       r       e       e       l       e       a       r       n       i       n       g       a       n       d       t       h       u       s       s       p       l       i       t       n       o       d       e       s       c       o       r       r       e       s       p       o       n       d       t       o       a       s       u       b       s       e       t       o       f       o       u       t       p       u       t       u       n       i       t       s       o       f       f       t       h       i       s       s       t       r       a       t       e       g       y       i       s       s       i       m       i       l       a       r       t       o       t       h       e       r       a       n       d       o       m       s       u       b       s       p       a       c       e       m       e       t       h       o       d       1       7       w       h       i       c       h       i       n       c       r       e       a       s       e       s       t       h       e       r       a       n       d       o       m       n       e       s       s       i       n       t       r       a       i       n       i       n       g       t       o       r       e       d       u       c       e       t       h       e       r       i       s       k       o       f       o       v       e       r       f       i       t       t       i       n       g       a       s       f       o       r       q       s       i       n       c       e       e       a       c       h       t       r       e       e       i       n       t       h       e       f       o       r       e       s       t       f       h       a       s       i       t       s       o       w       n       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       q       w       e       c       a       n       u       p       d       a       t       e       t       h       e       m       i       n       d       e       p       e       n       d       e       n       t       l       y       b       y       e       q       n       1       2       g       i       v       e       n       b       y       x   c   e   \\\\   x   9   8       f       o       r       i       m       p       l       e       m       e       n       t       a       t       i       o       n       a       l       c       o       n       v       e       n       i       e       n       c       e       w       e       d       o       n       o       t       c       o       n       d       u       c       t       t       h       i       s       u       p       d       a       t       e       s       c       h       e       m       e       o       n       t       h       e       w       h       o       l       e       d       a       t       a       s       e       t       s       b       u       t       o       n       a       s       e       t       o       f       m       i       n       i       b       a       t       c       h       e       s       b       t       h       e       t       r       a       i       n       i       n       g       p       r       o       c       e       d       u       r       e       o       f       a       l       d       l       f       i       s       s       h       o       w       n       i       n       a       l       g       o       r       i       t       h       m       1       a       l       g       o       r       i       t       h       m       1       t       h       e       t       r       a       i       n       i       n       g       p       r       o       c       e       d       u       r       e       o       f       a       l       d       l       f       r       e       q       u       i       r       e       s       t       r       a       i       n       i       n       g       s       e       t       n       b       t       h       e       n       u       m       b       e       r       o       f       m       i       n       i       b       a       t       c       h       e       s       t       o       u       p       d       a       t       e       q       i       n       i       t       i       a       l       i       z       e       x   c   e   \\\\   x   9   8       r       a       n       d       o       m       l       y       a       n       d       q       u       n       i       f       o       r       m       l       y       s       e       t       b       x   e   2   \\\\   x   8   8   \\\\   x   8   5       w       h       i       l       e       n       o       t       c       o       n       v       e       r       g       e       d       o       w       h       i       l       e       b       n       b       d       o       r       a       n       d       o       m       l       y       s       e       l       e       c       t       a       m       i       n       i       b       a       t       c       h       b       f       r       o       m       s       u       p       d       a       t       e       s       x   c   e   \\\\   x   9   8       b       y       c       o       m       p       u       t       i       n       g       g       r       a       d       i       e       n       t       e       q       n       1       3       o       n       b       b       b       b       e       n       d       w       h       i       l       e       u       p       d       a       t       e       q       b       y       i       t       e       r       a       t       i       n       g       e       q       n       1       2       o       n       b       b       x   e   2   \\\\   x   8   8   \\\\   x   8   5       e       n       d       w       h       i       l       e       i       n       t       h       e       t       e       s       t       i       n       g       s       t       a       g       e       t       h       e       o       u       t       p       u       t       o       f       t       h       e       f       o       r       e       s       t       f       i       s       g       i       v       e       n       b       y       a       v       e       r       a       g       i       n       g       t       h       e       p       r       e       d       i       c       t       i       o       n       s       f       r       o       m       a       l       l       t       h       e       p       k       1       i       n       d       i       v       i       d       u       a       l       t       r       e       e       s       g       x       x   c   e   \\\\   x   9   8       f       k       k       1       g       x       x   c   e   \\\\   x   9   8       t       k       6       x   0   c       4       e       x       p       e       r       i       m       e       n       t       a       l       r       e       s       u       l       t       s       o       u       r       r       e       a       l       i       z       a       t       i       o       n       o       f       l       d       l       f       s       i       s       b       a       s       e       d       o       n       x   e   2   \\\\   x   8   0   \\\\   x   9   c       c       a       f       f       e       x   e   2   \\\\   x   8   0   \\\\   x   9   d       1       8       i       t       i       s       m       o       d       u       l       a       r       a       n       d       i       m       p       l       e       m       e       n       t       e       d       a       s       a       s       t       a       n       d       a       r       d       n       e       u       r       a       l       n       e       t       w       o       r       k       l       a       y       e       r       w       e       c       a       n       e       i       t       h       e       r       u       s       e       i       t       a       s       a       s       h       a       l       l       o       w       s       t       a       n       d       a       l       o       n       e       m       o       d       e       l       s       l       d       l       f       s       o       r       i       n       t       e       g       r       a       t       e       i       t       w       i       t       h       a       n       y       d       e       e       p       n       e       t       w       o       r       k       s       d       l       d       l       f       s       w       e       e       v       a       l       u       a       t       e       s       l       d       l       f       s       o       n       d       i       f       f       e       r       e       n       t       l       d       l       t       a       s       k       s       a       n       d       c       o       m       p       a       r       e       i       t       w       i       t       h       o       t       h       e       r       s       t       a       n       d       a       l       o       n       e       l       d       l       m       e       t       h       o       d       s       a       s       d       l       d       l       f       s       c       a       n       b       e       l       e       a       r       n       e       d       f       r       o       m       r       a       w       i       m       a       g       e       d       a       t       a       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       w       e       v       e       r       i       f       y       d       l       d       l       f       s       o       n       a       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       p       p       l       i       c       a       t       i       o       n       i       e       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       t       h       e       d       e       f       a       u       l       t       s       e       t       t       i       n       g       s       f       o       r       t       h       e       p       a       r       a       m       e       t       e       r       s       o       f       o       u       r       f       o       r       e       s       t       s       a       r       e       t       r       e       e       n       u       m       b       e       r       5       t       r       e       e       d       e       p       t       h       7       o       u       t       p       u       t       u       n       i       t       n       u       m       b       e       r       o       f       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       6       4       i       t       e       r       a       t       i       o       n       t       i       m       e       s       t       o       u       p       d       a       t       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       2       0       t       h       e       n       u       m       b       e       r       o       f       m       i       n       i       b       a       t       c       h       e       s       t       o       u       p       d       a       t       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       1       0       0       m       a       x       i       m       u       m       i       t       e       r       a       t       i       o       n       2       5       0       0       0       4       1       c       o       m       p       a       r       i       s       o       n       o       f       s       l       d       l       f       s       t       o       s       t       a       n       d       a       l       o       n       e       l       d       l       m       e       t       h       o       d       s       w       e       c       o       m       p       a       r       e       o       u       r       s       h       a       l       l       o       w       m       o       d       e       l       s       l       d       l       f       s       w       i       t       h       o       t       h       e       r       s       t       a       t       e       o       f       t       h       e       a       r       t       s       t       a       n       d       a       l       o       n       e       l       d       l       m       e       t       h       o       d       s       f       o       r       s       l       d       l       f       s       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       f       x       x   c   e   \\\\   x   9   8       i       s       a       l       i       n       e       a       r       t       r       a       n       s       f       o       r       m       a       t       i       o       n       o       f       x       i       e       t       h       e       i       t       h       o       u       t       p       u       t       u       n       i       t       f       i       x       x   c   e   \\\\   x   b   8       i       x   c   e   \\\\   x   b   8       i       x       w       h       e       r       e       x   c   e   \\\\   x   b   8       i       i       s       t       h       e       i       t       h       c       o       l       u       m       n       o       f       t       h       e       t       r       a       n       s       f       o       r       m       a       t       i       o       n       m       a       t       r       i       x       x   c   e   \\\\   x   9   8       w       e       u       s       e       d       3       p       o       p       u       l       a       r       l       d       l       d       a       t       a       s       e       t       s       i       n       6       m       o       v       i       e       h       u       m       a       n       g       e       n       e       a       n       d       n       a       t       u       r       a       l       s       c       e       n       e       1       t       h       e       s       a       m       p       l       e       s       i       n       t       h       e       s       e       3       d       a       t       a       s       e       t       s       a       r       e       r       e       p       r       e       s       e       n       t       e       d       b       y       n       u       m       e       r       i       c       a       l       d       e       s       c       r       i       p       t       o       r       s       a       n       d       t       h       e       g       r       o       u       n       d       t       r       u       t       h       s       f       o       r       t       h       e       m       a       r       e       t       h       e       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       s       o       f       c       r       o       w       d       o       p       i       n       i       o       n       o       n       m       o       v       i       e       s       t       h       e       d       i       s       e       a       s       e       s       d       i       s       t       r       i       b       u       t       i       o       n       s       r       e       l       a       t       e       d       t       o       h       u       m       a       n       g       e       n       e       s       a       n       d       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       o       n       s       c       e       n       e       s       s       u       c       h       a       s       p       l       a       n       t       s       k       y       a       n       d       c       l       o       u       d       r       e       s       p       e       c       t       i       v       e       l       y       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       o       f       t       h       e       s       e       3       d       a       t       a       s       e       t       s       a       r       e       m       i       x       t       u       r       e       d       i       s       t       r       i       b       u       t       i       o       n       s       s       u       c       h       a       s       t       h       e       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       s       h       o       w       n       i       n       f       i       g       1       b       f       o       l       l       o       w       i       n       g       7       2       7       w       e       u       s       e       6       m       e       a       s       u       r       e       s       t       o       e       v       a       l       u       a       t       e       t       h       e       p       e       r       f       o       r       m       a       n       c       e       s       o       f       l       d       l       m       e       t       h       o       d       s       w       h       i       c       h       c       o       m       p       u       t       e       t       h       e       a       v       e       r       a       g       e       s       i       m       i       l       a       r       i       t       y       d       i       s       t       a       n       c       e       b       e       t       w       e       e       n       t       h       e       p       r       e       d       i       c       t       e       d       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       s       a       n       d       t       h       e       r       e       a       l       r       a       t       i       n       g       d       i       s       t       r       i       b       u       t       i       o       n       s       i       n       c       l       u       d       i       n       g       4       d       i       s       t       a       n       c       e       m       e       a       s       u       r       e       s       k       l       e       u       c       l       i       d       e       a       n       s       x   c   f   \\\\   x   8   6       r       e       n       s       e       n       s       q       u       a       r       e       d       x   c   f   \\\\   x   8   7       2       a       n       d       t       w       o       s       i       m       i       l       a       r       i       t       y       m       e       a       s       u       r       e       s       f       i       d       e       l       i       t       y       i       n       t       e       r       s       e       c       t       i       o       n       w       e       e       v       a       l       u       a       t       e       o       u       r       s       h       a       l       l       o       w       m       o       d       e       l       s       l       d       l       f       s       o       n       t       h       e       s       e       3       d       a       t       a       s       e       t       s       a       n       d       c       o       m       p       a       r       e       i       t       w       i       t       h       o       t       h       e       r       s       t       a       t       e       o       f       t       h       e       a       r       t       s       t       a       n       d       a       l       o       n       e       l       d       l       m       e       t       h       o       d       s       t       h       e       r       e       s       u       l       t       s       o       f       s       l       d       l       f       s       a       n       d       t       h       e       c       o       m       p       e       t       i       t       o       r       s       a       r       e       s       u       m       m       a       r       i       z       e       d       i       n       t       a       b       l       e       1       f       o       r       m       o       v       i       e       w       e       q       u       o       t       e       t       h       e       r       e       s       u       l       t       s       r       e       p       o       r       t       e       d       i       n       2       7       a       s       t       h       e       c       o       d       e       o       f       2       7       i       s       n       o       t       p       u       b       l       i       c       l       y       a       v       a       i       l       a       b       l       e       f       o       r       t       h       e       r       e       s       u       l       t       s       o       f       t       h       e       o       t       h       e       r       s       t       w       o       w       e       r       u       n       c       o       d       e       t       h       a       t       t       h       e       a       u       t       h       o       r       s       h       a       d       m       a       d       e       a       v       a       i       l       a       b       l       e       i       n       a       l       l       c       a       s       e       f       o       l       l       o       w       i       n       g       2       7       6       w       e       s       p       l       i       t       e       a       c       h       d       a       t       a       s       e       t       i       n       t       o       1       0       f       i       x       e       d       f       o       l       d       s       a       n       d       d       o       s       t       a       n       d       a       r       d       t       e       n       f       o       l       d       c       r       o       s       s       v       a       l       i       d       a       t       i       o       n       w       h       i       c       h       r       e       p       r       e       s       e       n       t       s       t       h       e       r       e       s       u       l       t       b       y       x   e   2   \\\\   x   8   0   \\\\   x   9   c       m       e       a       n       x   c   2   \\\\   x   b   1       s       t       a       n       d       a       r       d       d       e       v       i       a       t       i       o       n       x   e   2   \\\\   x   8   0   \\\\   x   9   d       a       n       d       m       a       t       t       e       r       s       l       e       s       s       h       o       w       t       r       a       i       n       i       n       g       a       n       d       t       e       s       t       i       n       g       d       a       t       a       g       e       t       d       i       v       i       d       e       d       a       s       c       a       n       b       e       s       e       e       n       f       r       o       m       t       a       b       l       e       1       s       l       d       l       f       s       p       e       r       f       o       r       m       b       e       s       t       o       n       a       l       l       o       f       t       h       e       s       i       x       m       e       a       s       u       r       e       s       t       a       b       l       e       1       c       o       m       p       a       r       i       s       o       n       r       e       s       u       l       t       s       o       n       t       h       r       e       e       l       d       l       d       a       t       a       s       e       t       s       6       x   e   2   \\\\   x   8   0   \\\\   x   9   c       x   e   2   \\\\   x   8   6   \\\\   x   9   1       x   e   2   \\\\   x   8   0   \\\\   x   9   d       a       n       d       x   e   2   \\\\   x   8   0   \\\\   x   9   c       x   e   2   \\\\   x   8   6   \\\\   x   9   3       x   e   2   \\\\   x   8   0   \\\\   x   9   d       i       n       d       i       c       a       t       e       t       h       e       l       a       r       g       e       r       a       n       d       t       h       e       s       m       a       l       l       e       r       t       h       e       b       e       t       t       e       r       r       e       s       p       e       c       t       i       v       e       l       y       d       a       t       a       s       e       t       m       e       t       h       o       d       k       l       x   e   2   \\\\   x   8   6   \\\\   x   9   3       e       u       c       l       i       d       e       a       n       x   e   2   \\\\   x   8   6   \\\\   x   9   3       s       x   c   f   \\\\   x   8   6       r       e       n       s       e       n       x   e   2   \\\\   x   8   6   \\\\   x   9   3       s       q       u       a       r       e       d       x   c   f   \\\\   x   8   7       2       x   e   2   \\\\   x   8   6   \\\\   x   9   3       f       i       d       e       l       i       t       y       x   e   2   \\\\   x   8   6   \\\\   x   9   1       i       n       t       e       r       s       e       c       t       i       o       n       x   e   2   \\\\   x   8   6   \\\\   x   9   1       m       o       v       i       e       s       l       d       l       f       o       u       r       s       a       o       s       o       l       d       l       o       g       i       t       b       o       o       s       t       2       7       l       d       l       o       g       i       t       b       o       o       s       t       2       7       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       0       7       3       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       8       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       0       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       2       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       9       9       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       2       9       x   c   2   \\\\   x   b   1       0       0       0       7       0       1       3       3       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       9       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       8       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       6       7       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       8       7       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       3       0       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       2       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       5       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       6       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       1       8       3       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       7       0       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       8       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       0       8       8       x   c   2   \\\\   x   b   1       0       0       0       3       0       0       8       8       x   c   2   \\\\   x   b   1       0       0       0       4       0       0       9       6       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       2       0       x   c   2   \\\\   x   b   1       0       0       0       5       0       9       8       1       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       8       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       7       4       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       6       7       x   c   2   \\\\   x   b   1       0       0       0       1       0       8       7       0       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       8       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       5       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       4       4       x   c   2   \\\\   x   b   1       0       0       0       4       0       8       3       6       x   c   2   \\\\   x   b   1       0       0       0       3       0       8       1       7       x   c   2   \\\\   x   b   1       0       0       0       4       s       l       d       l       f       o       u       r       s       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       2       2       8       x   c   2   \\\\   x   b   1       0       0       0       6       0       2       4       5       x   c   2   \\\\   x   b   1       0       0       1       9       0       2       3       1       x   c   2   \\\\   x   b   1       0       0       2       1       0       2       3       9       x   c   2   \\\\   x   b   1       0       0       1       8       0       0       8       5       x   c   2   \\\\   x   b   1       0       0       0       2       0       0       9       9       x   c   2   \\\\   x   b   1       0       0       0       5       0       0       7       6       x   c   2   \\\\   x   b   1       0       0       0       6       0       0       8       9       x   c   2   \\\\   x   b   1       0       0       0       6       0       2       1       2       x   c   2   \\\\   x   b   1       0       0       0       2       0       2       2       9       x   c   2   \\\\   x   b   1       0       0       1       5       0       2       3       1       x   c   2   \\\\   x   b   1       0       0       1       2       0       2       5       3       x   c   2   \\\\   x   b   1       0       0       0       9       0       1       7       9       x   c   2   \\\\   x   b   1       0       0       0       4       0       1       8       9       x   c   2   \\\\   x   b   1       0       0       2       1       0       2       1       1       x   c   2   \\\\   x   b   1       0       0       1       8       0       2       0       5       x   c   2   \\\\   x   b   1       0       0       1       2       0       9       4       8       x   c   2   \\\\   x   b   1       0       0       0       1       0       9       4       0       x   c   2   \\\\   x   b   1       0       0       0       6       0       9       3       8       x   c   2   \\\\   x   b   1       0       0       0       8       0       9       4       4       x   c   2   \\\\   x   b   1       0       0       0       3       0       7       8       8       x   c   2   \\\\   x   b   1       0       0       0       2       0       7       7       1       x   c   2   \\\\   x   b   1       0       0       1       5       0       7       6       9       x   c   2   \\\\   x   b   1       0       0       1       2       0       7       4       7       x   c   2   \\\\   x   b   1       0       0       0       9       s       l       d       l       f       o       u       r       s       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       5       3       4       x   c   2   \\\\   x   b   1       0       0       1       3       0       8       5       2       x   c   2   \\\\   x   b   1       0       0       2       3       0       8       5       6       x   c   2   \\\\   x   b   1       0       0       6       1       0       8       7       9       x   c   2   \\\\   x   b   1       0       0       2       3       0       3       1       7       x   c   2   \\\\   x   b   1       0       0       1       4       0       5       1       1       x   c   2   \\\\   x   b   1       0       0       2       1       0       4       7       5       x   c   2   \\\\   x   b   1       0       0       2       9       0       4       5       8       x   c   2   \\\\   x   b   1       0       0       1       4       0       3       3       6       x   c   2   \\\\   x   b   1       0       0       1       0       0       4       9       2       x   c   2   \\\\   x   b   1       0       0       1       6       0       5       0       8       x   c   2   \\\\   x   b   1       0       0       2       6       0       5       3       9       x   c   2   \\\\   x   b   1       0       0       1       1       0       4       4       8       x   c   2   \\\\   x   b   1       0       0       1       7       0       5       9       5       x   c   2   \\\\   x   b   1       0       0       2       6       0       7       1       6       x   c   2   \\\\   x   b   1       0       0       4       1       0       7       9       2       x   c   2   \\\\   x   b   1       0       0       1       9       0       8       2       4       x   c   2   \\\\   x   b   1       0       0       0       8       0       8       1       3       x   c   2   \\\\   x   b   1       0       0       0       8       0       7       2       2       x   c   2   \\\\   x   b   1       0       0       2       1       0       6       8       6       x   c   2   \\\\   x   b   1       0       0       0       9       0       6       6       4       x   c   2   \\\\   x   b   1       0       0       1       0       0       5       0       9       x   c   2   \\\\   x   b   1       0       0       1       6       0       4       9       2       x   c   2   \\\\   x   b   1       0       0       2       6       0       4       6       1       x   c   2   \\\\   x   b   1       0       0       1       1       h       u       m       a       n       g       e       n       e       n       a       t       u       r       a       l       s       c       e       n       e       4       2       e       v       a       l       u       a       t       i       o       n       o       f       d       l       d       l       f       s       o       n       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       i       n       s       o       m       e       l       i       t       e       r       a       t       u       r       e       8       1       1       2       8       1       5       5       a       g       e       e       s       t       i       m       a       t       i       o       n       i       s       f       o       r       m       u       l       a       t       e       d       a       s       a       l       d       l       p       r       o       b       l       e       m       w       e       c       o       n       d       u       c       t       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       e       x       p       e       r       i       m       e       n       t       s       o       n       m       o       r       p       h       2       4       w       h       i       c       h       c       o       n       t       a       i       n       s       m       o       r       e       t       h       a       n       5       0       0       0       0       f       a       c       i       a       l       i       m       a       g       e       s       f       r       o       m       a       b       o       u       t       1       3       0       0       0       p       e       o       p       l       e       o       f       d       i       f       f       e       r       e       n       t       r       a       c       e       s       e       a       c       h       f       a       c       i       a       l       i       m       a       g       e       i       s       a       n       n       o       t       a       t       e       d       w       i       t       h       a       c       h       r       o       n       o       l       o       g       i       c       a       l       a       g       e       t       o       g       e       n       e       r       a       t       e       a       n       a       g       e       d       i       s       t       r       i       b       u       t       i       o       n       f       o       r       e       a       c       h       f       a       c       e       i       m       a       g       e       w       e       f       o       l       l       o       w       t       h       e       s       a       m       e       s       t       r       a       t       e       g       y       u       s       e       d       i       n       8       2       8       5       w       h       i       c       h       u       s       e       s       a       g       a       u       s       s       i       a       n       d       i       s       t       r       i       b       u       t       i       o       n       w       h       o       s       e       m       e       a       n       i       s       t       h       e       c       h       r       o       n       o       l       o       g       i       c       a       l       a       g       e       o       f       t       h       e       f       a       c       e       i       m       a       g       e       f       i       g       1       a       t       h       e       p       r       e       d       i       c       t       e       d       a       g       e       f       o       r       a       f       a       c       e       i       m       a       g       e       i       s       s       i       m       p       l       y       t       h       e       a       g       e       h       a       v       i       n       g       t       h       e       h       i       g       h       e       s       t       p       r       o       b       a       b       i       l       i       t       y       i       n       t       h       e       p       r       e       d       i       c       t       e       d       1       w       e       d       o       w       n       l       o       a       d       t       h       e       s       e       d       a       t       a       s       e       t       s       f       r       o       m       h       t       t       p       c       s       e       s       e       u       e       d       u       c       n       p       e       o       p       l       e       x       g       e       n       g       l       d       l       i       n       d       e       x       h       t       m       7       x   0   c       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       t       h       e       p       e       r       f       o       r       m       a       n       c       e       o       f       a       g       e       e       s       t       i       m       a       t       i       o       n       i       s       e       v       a       l       u       a       t       e       d       b       y       t       h       e       m       e       a       n       a       b       s       o       l       u       t       e       e       r       r       o       r       m       a       e       b       e       t       w       e       e       n       p       r       e       d       i       c       t       e       d       a       g       e       s       a       n       d       c       h       r       o       n       o       l       o       g       i       c       a       l       a       g       e       s       a       s       t       h       e       c       u       r       r       e       n       t       s       t       a       t       e       o       f       t       h       e       a       r       t       r       e       s       u       l       t       o       n       m       o       r       p       h       i       s       o       b       t       a       i       n       b       y       f       i       n       e       t       u       n       i       n       g       d       l       d       l       5       o       n       v       g       g       f       a       c       e       2       3       w       e       a       l       s       o       b       u       i       l       d       a       d       l       d       l       f       o       n       v       g       g       f       a       c       e       b       y       r       e       p       l       a       c       i       n       g       t       h       e       s       o       f       t       m       a       x       l       a       y       e       r       i       n       v       g       g       n       e       t       b       y       a       l       d       l       f       f       o       l       l       o       w       i       n       g       5       w       e       d       o       s       t       a       n       d       a       r       d       1       0       t       e       n       f       o       l       d       c       r       o       s       s       v       a       l       i       d       a       t       i       o       n       a       n       d       t       h       e       r       e       s       u       l       t       s       a       r       e       s       u       m       m       a       r       i       z       e       d       i       n       t       a       b       l       e       2       w       h       i       c       h       s       h       o       w       s       d       l       d       l       f       a       c       h       i       e       v       e       t       h       e       s       t       a       t       e       o       f       t       h       e       a       r       t       p       e       r       f       o       r       m       a       n       c       e       o       n       m       o       r       p       h       n       o       t       e       t       h       a       t       t       h       e       s       i       g       n       i       f       i       c       a       n       t       p       e       r       f       o       r       m       a       n       c       e       g       a       i       n       b       e       t       w       e       e       n       d       e       e       p       l       d       l       m       o       d       e       l       s       d       l       d       l       a       n       d       d       l       d       l       f       a       n       d       n       o       n       d       e       e       p       l       d       l       m       o       d       e       l       s       i       i       s       l       d       l       c       p       n       n       b       f       g       s       l       d       l       a       n       d       t       h       e       s       u       p       e       r       i       o       r       i       t       y       o       f       d       l       d       l       f       c       o       m       p       a       r       e       d       w       i       t       h       d       l       d       l       v       e       r       i       f       i       e       s       t       h       e       e       f       f       e       c       t       i       v       e       n       e       s       s       o       f       e       n       d       t       o       e       n       d       l       e       a       r       n       i       n       g       a       n       d       o       u       r       t       r       e       e       b       a       s       e       d       m       o       d       e       l       f       o       r       l       d       l       r       e       s       p       e       c       t       i       v       e       l       y       t       a       b       l       e       2       m       a       e       o       f       a       g       e       e       s       t       i       m       a       t       i       o       n       c       o       m       p       a       r       i       s       o       n       o       n       m       o       r       p       h       2       4       m       e       t       h       o       d       i       i       s       l       d       l       1       1       c       p       n       n       1       1       b       f       g       s       l       d       l       6       d       l       d       l       v       g       g       f       a       c       e       5       d       l       d       l       f       v       g       g       f       a       c       e       o       u       r       s       m       a       e       5       6       7       x   c   2   \\\\   x   b   1       0       1       5       4       8       7       x   c   2   \\\\   x   b   1       0       3       1       3       9       4       x   c   2   \\\\   x   b   1       0       0       5       2       4       2       x   c   2   \\\\   x   b   1       0       0       1       2       2       4       x   c   2   \\\\   x   b   1       0       0       2       a       s       t       h       e       d       i       s       t       r       i       b       u       t       i       o       n       o       f       g       e       n       d       e       r       a       n       d       e       t       h       n       i       c       i       t       y       i       s       v       e       r       y       u       n       b       a       l       a       n       c       e       d       i       n       m       o       r       p       h       m       a       n       y       a       g       e       e       s       t       i       m       a       t       i       o       n       m       e       t       h       o       d       s       1       3       1       4       1       5       a       r       e       e       v       a       l       u       a       t       e       d       o       n       a       s       u       b       s       e       t       o       f       m       o       r       p       h       c       a       l       l       e       d       m       o       r       p       h       s       u       b       f       o       r       s       h       o       r       t       w       h       i       c       h       c       o       n       s       i       s       t       s       o       f       2       0       1       6       0       s       e       l       e       c       t       e       d       f       a       c       i       a       l       i       m       a       g       e       s       t       o       a       v       o       i       d       t       h       e       i       n       f       l       u       e       n       c       e       o       f       u       n       b       a       l       a       n       c       e       d       d       i       s       t       r       i       b       u       t       i       o       n       t       h       e       b       e       s       t       p       e       r       f       o       r       m       a       n       c       e       r       e       p       o       r       t       e       d       o       n       m       o       r       p       h       s       u       b       i       s       g       i       v       e       n       b       y       d       2       l       d       l       1       5       a       d       a       t       a       d       e       p       e       n       d       e       n       t       l       d       l       m       e       t       h       o       d       a       s       d       2       l       d       l       u       s       e       d       t       h       e       o       u       t       p       u       t       o       f       t       h       e       x   e   2   \\\\   x   8   0   \\\\   x   9   c       f       c       7       x   e   2   \\\\   x   8   0   \\\\   x   9   d       l       a       y       e       r       i       n       a       l       e       x       n       e       t       2       1       a       s       t       h       e       f       a       c       e       i       m       a       g       e       f       e       a       t       u       r       e       s       h       e       r       e       w       e       i       n       t       e       g       r       a       t       e       a       l       d       l       f       w       i       t       h       a       l       e       x       n       e       t       f       o       l       l       o       w       i       n       g       t       h       e       e       x       p       e       r       i       m       e       n       t       s       e       t       t       i       n       g       u       s       e       d       i       n       d       2       l       d       l       w       e       e       v       a       l       u       a       t       e       o       u       r       d       l       d       l       f       a       n       d       t       h       e       c       o       m       p       e       t       i       t       o       r       s       i       n       c       l       u       d       i       n       g       b       o       t       h       s       l       l       a       n       d       l       d       l       b       a       s       e       d       m       e       t       h       o       d       s       u       n       d       e       r       s       i       x       d       i       f       f       e       r       e       n       t       t       r       a       i       n       i       n       g       s       e       t       r       a       t       i       o       s       1       0       t       o       6       0       a       l       l       o       f       t       h       e       c       o       m       p       e       t       i       t       o       r       s       a       r       e       t       r       a       i       n       e       d       o       n       t       h       e       s       a       m       e       d       e       e       p       f       e       a       t       u       r       e       s       u       s       e       d       b       y       d       2       l       d       l       a       s       c       a       n       b       e       s       e       e       n       f       r       o       m       t       a       b       l       e       3       o       u       r       d       l       d       l       f       s       s       i       g       n       i       f       i       c       a       n       t       l       y       o       u       t       p       e       r       f       o       r       m       o       t       h       e       r       s       f       o       r       a       l       l       t       r       a       i       n       i       n       g       s       e       t       r       a       t       i       o       s       n       o       t       e       t       h       a       t       t       h       e       g       e       n       e       r       a       t       e       d       a       g       e       d       i       s       t       r       i       f       i       g       u       r       e       3       m       a       e       o       f       a       g       e       e       s       t       i       m       a       t       i       o       n       c       o       m       p       a       r       i       s       o       n       o       n       b       u       t       i       o       n       s       a       r       e       u       n       i       m       o       d       a       l       d       i       s       t       r       i       b       u       t       i       o       n       s       m       o       r       p       h       s       u       b       a       n       d       t       h       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       u       s       e       d       i       n       t       r       a       i       n       i       n       g       s       e       t       r       a       t       i       o       m       e       t       h       o       d       s       e       c       4       1       a       r       e       m       i       x       t       u       r       e       d       i       s       t       r       i       b       u       t       i       o       n       s       1       0       2       0       3       0       4       0       5       0       6       0       t       h       e       p       r       o       p       o       s       e       d       m       e       t       h       o       d       l       d       l       f       s       a       c       h       i       e       v       e       a       a       s       2       2       4       9       0       8       1       4       7       6       1       6       4       6       5       0       7       4       5       5       5       3       4       4       6       9       0       4       4       0       6       1       t       h       e       s       t       a       t       e       o       f       t       h       e       a       r       t       r       e       s       u       l       t       s       o       n       b       o       t       h       o       f       l       a       r       r       1       2       4       7       5       0       1       4       6       1       1       2       4       5       1       3       1       4       4       2       7       3       4       3       5       0       0       4       2       9       4       9       i       i       s       a       l       d       l       9       4       1       7       9       1       4       1       6       8       3       4       1       2       2       8       4       1       1       0       7       4       1       0       2       4       4       0       9       0       2       t       h       e       m       w       h       i       c       h       v       e       r       i       f       i       e       s       t       h       a       t       o       u       r       m       o       d       e       l       d       2       l       d       l       1       5       4       1       0       8       0       3       9       8       5       7       3       9       2       0       4       3       8       7       1       2       3       8       5       6       0       3       8       3       8       5       h       a       s       t       h       e       a       b       i       l       i       t       y       t       o       m       o       d       e       l       a       n       y       g       e       n       e       r       a       l       d       l       d       l       f       o       u       r       s       3       8       4       9       5       3       6       2       2       0       3       3       9       9       1       3       2       4       0       1       3       1       9       1       7       3       1       2       2       4       f       o       r       m       o       f       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       4       3       t       i       m       e       c       o       m       p       l       e       x       i       t       y       l       e       t       h       a       n       d       s       b       b       e       t       h       e       t       r       e       e       d       e       p       t       h       a       n       d       t       h       e       b       a       t       c       h       s       i       z       e       r       e       s       p       e       c       t       i       v       e       l       y       e       a       c       h       t       r       e       e       h       a       s       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       s       p       l       i       t       n       o       d       e       s       a       n       d       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       l       e       a       f       n       o       d       e       s       l       e       t       d       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       f       o       r       o       n       e       t       r       e       e       a       n       d       o       n       e       s       a       m       p       l       e       t       h       e       c       o       m       p       l       e       x       i       t       y       o       f       a       f       o       r       w       a       r       d       p       a       s       s       a       n       d       a       b       a       c       k       w       a       r       d       p       a       s       s       a       r       e       o       d       d       1       x   c   3   \\\\   x   9   7       c       o       d       x   c   3   \\\\   x   9   7       c       a       n       d       o       d       1       x   c   3   \\\\   x   9   7       c       d       x   c   3   \\\\   x   9   7       c       o       d       x   c   3   \\\\   x   9   7       c       r       e       s       p       e       c       t       i       v       e       l       y       s       o       f       o       r       k       t       r       e       e       s       a       n       d       n       b       b       a       t       c       h       e       s       t       h       e       c       o       m       p       l       e       x       i       t       y       o       f       a       f       o       r       w       a       r       d       a       n       d       b       a       c       k       w       a       r       d       p       a       s       s       i       s       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       s       b       t       h       e       c       o       m       p       l       e       x       i       t       y       o       f       a       n       i       t       e       r       a       t       i       o       n       t       o       u       p       d       a       t       e       l       e       a       f       n       o       d       e       s       a       r       e       o       n       b       x   c   3   \\\\   x   9   7       s       b       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       d       1       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       s       b       t       h       u       s       t       h       e       c       o       m       p       l       e       x       i       t       y       f       o       r       t       h       e       t       r       a       i       n       i       n       g       p       r       o       c       e       d       u       r       e       o       n       e       e       p       o       c       h       n       b       b       a       t       c       h       e       s       a       n       d       t       h       e       t       e       s       t       i       n       g       p       r       o       c       e       d       u       r       e       o       n       e       s       a       m       p       l       e       a       r       e       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       x   c   3   \\\\   x   9   7       n       b       x   c   3   \\\\   x   9   7       s       b       a       n       d       o       d       x   c   3   \\\\   x   9   7       c       x   c   3   \\\\   x   9   7       k       r       e       s       p       e       c       t       i       v       e       l       y       l       d       l       f       s       a       r       e       e       f       f       i       c       i       e       n       t       o       n       m       o       r       p       h       s       u       b       1       2       6       3       6       t       r       a       i       n       i       n       g       i       m       a       g       e       s       8       4       2       4       t       e       s       t       i       n       g       i       m       a       g       e       s       o       u       r       m       o       d       e       l       o       n       l       y       t       a       k       e       s       5       2       5       0       s       f       o       r       t       r       a       i       n       i       n       g       2       5       0       0       0       i       t       e       r       a       t       i       o       n       s       a       n       d       8       s       f       o       r       t       e       s       t       i       n       g       a       l       l       8       4       2       4       i       m       a       g       e       s       4       4       p       a       r       a       m       e       t       e       r       d       i       s       c       u       s       s       i       o       n       n       o       w       w       e       d       i       s       c       u       s       s       t       h       e       i       n       f       l       u       e       n       c       e       o       f       p       a       r       a       m       e       t       e       r       s       e       t       t       i       n       g       s       o       n       p       e       r       f       o       r       m       a       n       c       e       w       e       r       e       p       o       r       t       t       h       e       r       e       s       u       l       t       s       o       f       r       a       t       i       n       g       p       r       e       d       i       c       t       i       o       n       o       n       m       o       v       i       e       m       e       a       s       u       r       e       d       b       y       k       l       a       n       d       a       g       e       e       s       t       i       m       a       t       i       o       n       o       n       m       o       r       p       h       s       u       b       w       i       t       h       6       0       t       r       a       i       n       i       n       g       s       e       t       r       a       t       i       o       m       e       a       s       u       r       e       d       b       y       m       a       e       f       o       r       d       i       f       f       e       r       e       n       t       p       a       r       a       m       e       t       e       r       s       e       t       t       i       n       g       s       i       n       t       h       i       s       s       e       c       t       i       o       n       t       r       e       e       n       u       m       b       e       r       a       s       a       f       o       r       e       s       t       i       s       a       n       e       n       s       e       m       b       l       e       m       o       d       e       l       i       t       i       s       n       e       c       e       s       s       a       r       y       t       o       i       n       v       e       s       t       i       g       a       t       e       h       o       w       p       e       r       f       o       r       m       a       n       c       e       s       c       h       a       n       g       e       b       y       v       a       r       y       i       n       g       t       h       e       t       r       e       e       n       u       m       b       e       r       u       s       e       d       i       n       a       f       o       r       e       s       t       n       o       t       e       t       h       a       t       a       s       w       e       d       i       s       c       u       s       s       e       d       i       n       s       e       c       2       t       h       e       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       t       o       l       e       a       r       n       a       f       o       r       e       s       t       p       r       o       p       o       s       e       d       i       n       d       n       d       f       s       2       0       i       s       d       i       f       f       e       r       e       n       t       f       r       o       m       o       u       r       s       t       h       e       r       e       f       o       r       e       i       t       i       s       n       e       c       e       s       s       a       r       y       t       o       s       e       e       w       h       i       c       h       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       i       s       b       e       t       t       e       r       t       o       l       e       a       r       n       a       f       o       r       e       s       t       t       o       w       a       r       d       s       t       h       i       s       e       n       d       w       e       r       e       p       l       a       c       e       o       u       r       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       i       n       d       l       d       l       f       s       b       y       t       h       e       o       n       e       u       s       e       d       i       n       d       n       d       f       s       a       n       d       n       a       m       e       t       h       i       s       m       e       t       h       o       d       d       n       d       f       s       l       d       l       t       h       e       c       o       r       r       e       s       p       o       n       d       i       n       g       s       h       a       l       l       o       w       m       o       d       e       l       i       s       n       a       m       e       d       b       y       s       n       d       f       s       l       d       l       w       e       f       i       x       o       t       h       e       r       p       a       r       a       m       e       t       e       r       s       i       e       t       r       e       e       d       e       p       t       h       a       n       d       8       x   0   c       o       u       t       p       u       t       u       n       i       t       n       u       m       b       e       r       o       f       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       a       s       t       h       e       d       e       f       a       u       l       t       s       e       t       t       i       n       g       a       s       s       h       o       w       n       i       n       f       i       g       4       a       o       u       r       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       c       a       n       i       m       p       r       o       v       e       t       h       e       p       e       r       f       o       r       m       a       n       c       e       b       y       u       s       i       n       g       m       o       r       e       t       r       e       e       s       w       h       i       l       e       t       h       e       o       n       e       u       s       e       d       i       n       d       n       d       f       s       e       v       e       n       l       e       a       d       s       t       o       a       w       o       r       s       e       p       e       r       f       o       r       m       a       n       c       e       t       h       a       n       o       n       e       f       o       r       a       s       i       n       g       l       e       t       r       e       e       o       b       s       e       r       v       e       d       f       r       o       m       f       i       g       4       t       h       e       p       e       r       f       o       r       m       a       n       c       e       o       f       l       d       l       f       s       c       a       n       b       e       i       m       p       r       o       v       e       d       b       y       u       s       i       n       g       m       o       r       e       t       r       e       e       s       b       u       t       t       h       e       i       m       p       r       o       v       e       m       e       n       t       b       e       c       o       m       e       s       i       n       c       r       e       a       s       i       n       g       l       y       s       m       a       l       l       e       r       a       n       d       s       m       a       l       l       e       r       t       h       e       r       e       f       o       r       e       u       s       i       n       g       m       u       c       h       l       a       r       g       e       r       e       n       s       e       m       b       l       e       s       d       o       e       s       n       o       t       y       i       e       l       d       a       b       i       g       i       m       p       r       o       v       e       m       e       n       t       o       n       m       o       v       i       e       t       h       e       n       u       m       b       e       r       o       f       t       r       e       e       s       k       1       0       0       k       l       0       0       7       0       v       s       k       2       0       k       l       0       0       7       1       n       o       t       e       t       h       a       t       n       o       t       a       l       l       r       a       n       d       o       m       f       o       r       e       s       t       s       b       a       s       e       d       m       e       t       h       o       d       s       u       s       e       a       l       a       r       g       e       n       u       m       b       e       r       o       f       t       r       e       e       s       e       g       s       h       o       t       t       o       n       e       t       a       l       2       5       o       b       t       a       i       n       e       d       v       e       r       y       g       o       o       d       p       o       s       e       e       s       t       i       m       a       t       i       o       n       r       e       s       u       l       t       s       f       r       o       m       d       e       p       t       h       i       m       a       g       e       s       b       y       o       n       l       y       3       d       e       c       i       s       i       o       n       t       r       e       e       s       t       r       e       e       d       e       p       t       h       t       r       e       e       d       e       p       t       h       i       s       a       n       o       t       h       e       r       i       m       p       o       r       t       a       n       t       p       a       r       a       m       e       t       e       r       f       o       r       d       e       c       i       s       i       o       n       t       r       e       e       s       i       n       l       d       l       f       s       t       h       e       r       e       i       s       a       n       i       m       p       l       i       c       i       t       c       o       n       s       t       r       a       i       n       t       b       e       t       w       e       e       n       t       r       e       e       d       e       p       t       h       h       a       n       d       o       u       t       p       u       t       u       n       i       t       n       u       m       b       e       r       o       f       t       h       e       f       e       a       t       u       r       e       l       e       a       r       n       i       n       g       f       u       n       c       t       i       o       n       x   c   f   \\\\   x   8   4       x   c   f   \\\\   x   8   4       x   e   2   \\\\   x   8   9   \\\\   x   a   5       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       t       o       d       i       s       c       u       s       s       t       h       e       i       n       f       l       u       e       n       c       e       o       f       t       r       e       e       d       e       p       t       h       t       o       t       h       e       p       e       r       f       o       r       m       a       n       c       e       o       f       d       l       d       l       f       s       w       e       s       e       t       x   c   f   \\\\   x   8   4       2       h       x   e   2   \\\\   x   8   8   \\\\   x   9   2       1       a       n       d       f       i       x       t       r       e       e       n       u       m       b       e       r       k       1       a       n       d       t       h       e       p       e       r       f       o       r       m       a       n       c       e       c       h       a       n       g       e       b       y       v       a       r       y       i       n       g       t       r       e       e       d       e       p       t       h       i       s       s       h       o       w       n       i       n       f       i       g       4       b       w       e       s       e       e       t       h       a       t       t       h       e       p       e       r       f       o       r       m       a       n       c       e       f       i       r       s       t       i       m       p       r       o       v       e       s       t       h       e       n       d       e       c       r       e       a       s       e       s       w       i       t       h       t       h       e       i       n       c       r       e       a       s       e       o       f       t       h       e       t       r       e       e       d       e       p       t       h       t       h       e       r       e       a       s       o       n       i       s       a       s       t       h       e       t       r       e       e       d       e       p       t       h       i       n       c       r       e       a       s       e       s       t       h       e       d       i       m       e       n       s       i       o       n       o       f       l       e       a       r       n       e       d       f       e       a       t       u       r       e       s       i       n       c       r       e       a       s       e       s       e       x       p       o       n       e       n       t       i       a       l       l       y       w       h       i       c       h       g       r       e       a       t       l       y       i       n       c       r       e       a       s       e       s       t       h       e       t       r       a       i       n       i       n       g       d       i       f       f       i       c       u       l       t       y       s       o       u       s       i       n       g       m       u       c       h       l       a       r       g       e       r       d       e       p       t       h       s       m       a       y       l       e       a       d       t       o       b       a       d       p       e       r       f       o       r       m       a       n       c       e       o       n       m       o       v       i       e       t       r       e       e       d       e       p       t       h       h       1       8       k       l       0       1       1       6       2       v       s       h       9       k       l       0       0       8       3       1       f       i       g       u       r       e       4       t       h       e       p       e       r       f       o       r       m       a       n       c       e       c       h       a       n       g       e       o       f       a       g       e       e       s       t       i       m       a       t       i       o       n       o       n       m       o       r       p       h       s       u       b       a       n       d       r       a       t       i       n       g       p       r       e       d       i       c       t       i       o       n       o       n       m       o       v       i       e       b       y       v       a       r       y       i       n       g       a       t       r       e       e       n       u       m       b       e       r       a       n       d       b       t       r       e       e       d       e       p       t       h       o       u       r       a       p       p       r       o       a       c       h       d       l       d       l       f       s       s       l       d       l       f       s       c       a       n       i       m       p       r       o       v       e       t       h       e       p       e       r       f       o       r       m       a       n       c       e       b       y       u       s       i       n       g       m       o       r       e       t       r       e       e       s       w       h       i       l       e       u       s       i       n       g       t       h       e       e       n       s       e       m       b       l       e       s       t       r       a       t       e       g       y       p       r       o       p       o       s       e       d       i       n       d       n       d       f       s       d       n       d       f       s       l       d       l       s       n       d       f       s       l       d       l       e       v       e       n       l       e       a       d       s       t       o       a       w       o       r       s       e       p       e       r       f       o       r       m       a       n       c       e       t       h       a       n       o       n       e       f       o       r       a       s       i       n       g       l       e       t       r       e       e       5       c       o       n       c       l       u       s       i       o       n       w       e       p       r       e       s       e       n       t       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       e       s       t       s       a       n       o       v       e       l       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       a       l       g       o       r       i       t       h       m       i       n       s       p       i       r       e       d       b       y       d       i       f       f       e       r       e       n       t       i       a       b       l       e       d       e       c       i       s       i       o       n       t       r       e       e       s       w       e       d       e       f       i       n       e       d       a       d       i       s       t       r       i       b       u       t       i       o       n       b       a       s       e       d       l       o       s       s       f       u       n       c       t       i       o       n       f       o       r       t       h       e       f       o       r       e       s       t       s       a       n       d       f       o       u       n       d       t       h       a       t       t       h       e       l       e       a       f       n       o       d       e       p       r       e       d       i       c       t       i       o       n       s       c       a       n       b       e       o       p       t       i       m       i       z       e       d       v       i       a       v       a       r       i       a       t       i       o       n       a       l       b       o       u       n       d       i       n       g       w       h       i       c       h       e       n       a       b       l       e       s       a       l       l       t       h       e       t       r       e       e       s       a       n       d       t       h       e       f       e       a       t       u       r       e       t       h       e       y       u       s       e       t       o       b       e       l       e       a       r       n       e       d       j       o       i       n       t       l       y       i       n       a       n       e       n       d       t       o       e       n       d       m       a       n       n       e       r       e       x       p       e       r       i       m       e       n       t       a       l       r       e       s       u       l       t       s       s       h       o       w       e       d       t       h       e       s       u       p       e       r       i       o       r       i       t       y       o       f       o       u       r       a       l       g       o       r       i       t       h       m       f       o       r       s       e       v       e       r       a       l       l       d       l       t       a       s       k       s       a       n       d       a       r       e       l       a       t       e       d       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       p       p       l       i       c       a       t       i       o       n       a       n       d       v       e       r       i       f       i       e       d       o       u       r       m       o       d       e       l       h       a       s       t       h       e       a       b       i       l       i       t       y       t       o       m       o       d       e       l       a       n       y       g       e       n       e       r       a       l       f       o       r       m       o       f       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       a       c       k       n       o       w       l       e       d       g       e       m       e       n       t       t       h       i       s       w       o       r       k       w       a       s       s       u       p       p       o       r       t       e       d       i       n       p       a       r       t       b       y       t       h       e       n       a       t       i       o       n       a       l       n       a       t       u       r       a       l       s       c       i       e       n       c       e       f       o       u       n       d       a       t       i       o       n       o       f       c       h       i       n       a       n       o       6       1       6       7       2       3       3       6       i       n       p       a       r       t       b       y       x   e   2   \\\\   x   8   0   \\\\   x   9   c       c       h       e       n       g       u       a       n       g       x   e   2   \\\\   x   8   0   \\\\   x   9   d       p       r       o       j       e       c       t       s       u       p       p       o       r       t       e       d       b       y       s       h       a       n       g       h       a       i       m       u       n       i       c       i       p       a       l       e       d       u       c       a       t       i       o       n       c       o       m       m       i       s       s       i       o       n       a       n       d       s       h       a       n       g       h       a       i       e       d       u       c       a       t       i       o       n       d       e       v       e       l       o       p       m       e       n       t       f       o       u       n       d       a       t       i       o       n       n       o       1       5       c       g       4       3       a       n       d       i       n       p       a       r       t       b       y       o       n       r       n       0       0       0       1       4       1       5       1       2       3       5       6       r       e       f       e       r       e       n       c       e       s       1       y       a       m       i       t       a       n       d       d       g       e       m       a       n       s       h       a       p       e       q       u       a       n       t       i       z       a       t       i       o       n       a       n       d       r       e       c       o       g       n       i       t       i       o       n       w       i       t       h       r       a       n       d       o       m       i       z       e       d       t       r       e       e       s       n       e       u       r       a       l       c       o       m       p       u       t       a       t       i       o       n       9       7       1       5       4       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       5       8       8       1       9       9       7       2       a       l       b       e       r       g       e       r       s       d       p       i       e       t       r       a       a       n       d       v       j       d       p       i       e       t       r       a       a       m       a       x       i       m       u       m       e       n       t       r       o       p       y       a       p       p       r       o       a       c       h       t       o       n       a       t       u       r       a       l       l       a       n       g       u       a       g       e       p       r       o       c       e       s       s       i       n       g       c       o       m       p       u       t       a       t       i       o       n       a       l       l       i       n       g       u       i       s       t       i       c       s       2       2       1       3       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       7       1       1       9       9       6       3       l       b       r       e       i       m       a       n       r       a       n       d       o       m       f       o       r       e       s       t       s       m       a       c       h       i       n       e       l       e       a       r       n       i       n       g       4       5       1       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       2       2       0       0       1       4       a       c       r       i       m       i       n       i       s       i       a       n       d       j       s       h       o       t       t       o       n       d       e       c       i       s       i       o       n       f       o       r       e       s       t       s       f       o       r       c       o       m       p       u       t       e       r       v       i       s       i       o       n       a       n       d       m       e       d       i       c       a       l       i       m       a       g       e       a       n       a       l       y       s       i       s       s       p       r       i       n       g       e       r       2       0       1       3       5       b       b       g       a       o       c       x       i       n       g       c       w       x       i       e       j       w       u       a       n       d       x       g       e       n       g       d       e       e       p       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       w       i       t       h       l       a       b       e       l       a       m       b       i       g       u       i       t       y       a       r       x       i       v       1       6       1       1       0       1       7       3       1       2       0       1       7       6       x       g       e       n       g       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       i       e       e       e       t       r       a       n       s       k       n       o       w       l       d       a       t       a       e       n       g       2       8       7       1       7       3       4       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       7       4       8       2       0       1       6       9       x   0   c       7       x       g       e       n       g       a       n       d       p       h       o       u       p       r       e       r       e       l       e       a       s       e       p       r       e       d       i       c       t       i       o       n       o       f       c       r       o       w       d       o       p       i       n       i       o       n       o       n       m       o       v       i       e       s       b       y       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       i       n       p       r       o       i       j       c       a       i       p       a       g       e       s       3       5       1       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       5       1       7       2       0       1       5       8       x       g       e       n       g       k       s       m       i       t       h       m       i       l       e       s       a       n       d       z       z       h       o       u       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       b       y       l       e       a       r       n       i       n       g       f       r       o       m       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       i       n       p       r       o       c       a       a       a       i       2       0       1       0       9       x       g       e       n       g       q       w       a       n       g       a       n       d       y       x       i       a       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       b       y       a       d       a       p       t       i       v       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       i       n       p       r       o       c       i       c       p       r       p       a       g       e       s       4       4       6       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       4       7       0       2       0       1       4       1       0       x       g       e       n       g       a       n       d       y       x       i       a       h       e       a       d       p       o       s       e       e       s       t       i       m       a       t       i       o       n       b       a       s       e       d       o       n       m       u       l       t       i       v       a       r       i       a       t       e       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       i       n       p       r       o       c       c       v       p       r       p       a       g       e       s       1       8       3       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       8       4       2       2       0       1       4       1       1       x       g       e       n       g       c       y       i       n       a       n       d       z       z       h       o       u       f       a       c       i       a       l       a       g       e       e       s       t       i       m       a       t       i       o       n       b       y       l       e       a       r       n       i       n       g       f       r       o       m       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       s       i       e       e       e       t       r       a       n       s       p       a       t       t       e       r       n       a       n       a       l       m       a       c       h       i       n       t       e       l       l       3       5       1       0       2       4       0       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       4       1       2       2       0       1       3       1       2       g       g       u       o       y       f       u       c       r       d       y       e       r       a       n       d       t       s       h       u       a       n       g       i       m       a       g       e       b       a       s       e       d       h       u       m       a       n       a       g       e       e       s       t       i       m       a       t       i       o       n       b       y       m       a       n       i       f       o       l       d       l       e       a       r       n       i       n       g       a       n       d       l       o       c       a       l       l       y       a       d       j       u       s       t       e       d       r       o       b       u       s       t       r       e       g       r       e       s       s       i       o       n       i       e       e       e       t       r       a       n       s       i       m       a       g       e       p       r       o       c       e       s       s       i       n       g       1       7       7       1       1       7       8       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       1       8       8       2       0       0       8       1       3       g       g       u       o       a       n       d       g       m       u       h       u       m       a       n       a       g       e       e       s       t       i       m       a       t       i       o       n       w       h       a       t       i       s       t       h       e       i       n       f       l       u       e       n       c       e       a       c       r       o       s       s       r       a       c       e       a       n       d       g       e       n       d       e       r       i       n       c       v       p       r       w       o       r       k       s       h       o       p       s       p       a       g       e       s       7       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       7       8       2       0       1       0       1       4       g       g       u       o       a       n       d       c       z       h       a       n       g       a       s       t       u       d       y       o       n       c       r       o       s       s       p       o       p       u       l       a       t       i       o       n       a       g       e       e       s       t       i       m       a       t       i       o       n       i       n       p       r       o       c       c       v       p       r       p       a       g       e       s       4       2       5       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       2       6       3       2       0       1       4       1       5       z       h       e       x       l       i       z       z       h       a       n       g       f       w       u       x       g       e       n       g       y       z       h       a       n       g       m       h       y       a       n       g       a       n       d       y       z       h       u       a       n       g       d       a       t       a       d       e       p       e       n       d       e       n       t       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       a       g       e       e       s       t       i       m       a       t       i       o       n       i       e       e       e       t       r       a       n       s       o       n       i       m       a       g       e       p       r       o       c       e       s       s       i       n       g       2       0       1       7       1       6       t       k       h       o       r       a       n       d       o       m       d       e       c       i       s       i       o       n       f       o       r       e       s       t       s       i       n       p       r       o       c       i       c       d       a       r       p       a       g       e       s       2       7       8       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       8       2       1       9       9       5       1       7       t       k       h       o       t       h       e       r       a       n       d       o       m       s       u       b       s       p       a       c       e       m       e       t       h       o       d       f       o       r       c       o       n       s       t       r       u       c       t       i       n       g       d       e       c       i       s       i       o       n       f       o       r       e       s       t       s       i       e       e       e       t       r       a       n       s       p       a       t       t       e       r       n       a       n       a       l       m       a       c       h       i       n       t       e       l       l       2       0       8       8       3       2       x   e   2   \\\\   x   8   0   \\\\   x   9   3       8       4       4       1       9       9       8       1       8       y       j       i       a       e       s       h       e       l       h       a       m       e       r       j       d       o       n       a       h       u       e       s       k       a       r       a       y       e       v       j       l       o       n       g       r       g       i       r       s       h       i       c       k       s       g       u       a       d       a       r       r       a       m       a       a       n       d       t       d       a       r       r       e       l       l       c       a       f       f       e       c       o       n       v       o       l       u       t       i       o       n       a       l       a       r       c       h       i       t       e       c       t       u       r       e       f       o       r       f       a       s       t       f       e       a       t       u       r       e       e       m       b       e       d       d       i       n       g       a       r       x       i       v       p       r       e       p       r       i       n       t       a       r       x       i       v       1       4       0       8       5       0       9       3       2       0       1       4       1       9       m       i       j       o       r       d       a       n       z       g       h       a       h       r       a       m       a       n       i       t       s       j       a       a       k       k       o       l       a       a       n       d       l       k       s       a       u       l       a       n       i       n       t       r       o       d       u       c       t       i       o       n       t       o       v       a       r       i       a       t       i       o       n       a       l       m       e       t       h       o       d       s       f       o       r       g       r       a       p       h       i       c       a       l       m       o       d       e       l       s       m       a       c       h       i       n       e       l       e       a       r       n       i       n       g       3       7       2       1       8       3       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       3       3       1       9       9       9       2       0       p       k       o       n       t       s       c       h       i       e       d       e       r       m       f       i       t       e       r       a       u       a       c       r       i       m       i       n       i       s       i       a       n       d       s       r       b       u       l       x   c   3   \\\\   x   b   2       d       e       e       p       n       e       u       r       a       l       d       e       c       i       s       i       o       n       f       o       r       e       s       t       s       i       n       p       r       o       c       i       c       c       v       p       a       g       e       s       1       4       6       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       4       7       5       2       0       1       5       2       1       a       k       r       i       z       h       e       v       s       k       y       i       s       u       t       s       k       e       v       e       r       a       n       d       g       e       h       i       n       t       o       n       i       m       a       g       e       n       e       t       c       l       a       s       s       i       f       i       c       a       t       i       o       n       w       i       t       h       d       e       e       p       c       o       n       v       o       l       u       t       i       o       n       a       l       n       e       u       r       a       l       n       e       t       w       o       r       k       s       i       n       p       r       o       c       n       i       p       s       p       a       g       e       s       1       1       0       6       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       1       1       4       2       0       1       2       2       2       a       l       a       n       i       t       i       s       c       d       r       a       g       a       n       o       v       a       a       n       d       c       c       h       r       i       s       t       o       d       o       u       l       o       u       c       o       m       p       a       r       i       n       g       d       i       f       f       e       r       e       n       t       c       l       a       s       s       i       f       i       e       r       s       f       o       r       a       u       t       o       m       a       t       i       c       a       g       e       e       s       t       i       m       a       t       i       o       n       i       e       e       e       t       r       a       n       s       o       n       c       y       b       e       r       n       e       t       i       c       s       3       4       1       6       2       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       6       2       8       2       0       0       4       2       3       o       m       p       a       r       k       h       i       a       v       e       d       a       l       d       i       a       n       d       a       z       i       s       s       e       r       m       a       n       d       e       e       p       f       a       c       e       r       e       c       o       g       n       i       t       i       o       n       i       n       p       r       o       c       b       m       v       c       p       a       g       e       s       4       1       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       1       1       2       2       0       1       5       2       4       k       r       i       c       a       n       e       k       a       n       d       t       t       e       s       a       f       a       y       e       m       o       r       p       h       a       l       o       n       g       i       t       u       d       i       n       a       l       i       m       a       g       e       d       a       t       a       b       a       s       e       o       f       n       o       r       m       a       l       a       d       u       l       t       a       g       e       p       r       o       g       r       e       s       s       i       o       n       i       n       p       r       o       c       f       g       p       a       g       e       s       3       4       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       3       4       5       2       0       0       6       2       5       j       s       h       o       t       t       o       n       a       w       f       i       t       z       g       i       b       b       o       n       m       c       o       o       k       t       s       h       a       r       p       m       f       i       n       o       c       c       h       i       o       r       m       o       o       r       e       a       k       i       p       m       a       n       a       n       d       a       b       l       a       k       e       r       e       a       l       t       i       m       e       h       u       m       a       n       p       o       s       e       r       e       c       o       g       n       i       t       i       o       n       i       n       p       a       r       t       s       f       r       o       m       s       i       n       g       l       e       d       e       p       t       h       i       m       a       g       e       s       i       n       p       r       o       c       c       v       p       r       p       a       g       e       s       1       2       9       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       3       0       4       2       0       1       1       2       6       g       t       s       o       u       m       a       k       a       s       a       n       d       i       k       a       t       a       k       i       s       m       u       l       t       i       l       a       b       e       l       c       l       a       s       s       i       f       i       c       a       t       i       o       n       a       n       o       v       e       r       v       i       e       w       i       n       t       e       r       n       a       t       i       o       n       a       l       j       o       u       r       n       a       l       o       f       d       a       t       a       w       a       r       e       h       o       u       s       i       n       g       a       n       d       m       i       n       i       n       g       3       3       1       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       3       2       0       0       7       2       7       c       x       i       n       g       x       g       e       n       g       a       n       d       h       x       u       e       l       o       g       i       s       t       i       c       b       o       o       s       t       i       n       g       r       e       g       r       e       s       s       i       o       n       f       o       r       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       i       n       p       r       o       c       c       v       p       r       p       a       g       e       s       4       4       8       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       4       4       9       7       2       0       1       6       2       8       x       y       a       n       g       x       g       e       n       g       a       n       d       d       z       h       o       u       s       p       a       r       s       i       t       y       c       o       n       d       i       t       i       o       n       a       l       e       n       e       r       g       y       l       a       b       e       l       d       i       s       t       r       i       b       u       t       i       o       n       l       e       a       r       n       i       n       g       f       o       r       a       g       e       e       s       t       i       m       a       t       i       o       n       i       n       p       r       o       c       i       j       c       a       i       p       a       g       e       s       2       2       5       9       x   e   2   \\\\   x   8   0   \\\\   x   9   3       2       2       6       5       2       0       1       6       2       9       a       l       y       u       i       l       l       e       a       n       d       a       r       a       n       g       a       r       a       j       a       n       t       h       e       c       o       n       c       a       v       e       c       o       n       v       e       x       p       r       o       c       e       d       u       r       e       n       e       u       r       a       l       c       o       m       p       u       t       a       t       i       o       n       1       5       4       9       1       5       x   e   2   \\\\   x   8   0   \\\\   x   9   3       9       3       6       2       0       0       3       3       0       y       z       h       o       u       h       x       u       e       a       n       d       x       g       e       n       g       e       m       o       t       i       o       n       d       i       s       t       r       i       b       u       t       i       o       n       r       e       c       o       g       n       i       t       i       o       n       f       r       o       m       f       a       c       i       a       l       e       x       p       r       e       s       s       i       o       n       s       i       n       p       r       o       c       m       m       p       a       g       e       s       1       2       4       7       x   e   2   \\\\   x   8   0   \\\\   x   9   3       1       2       5       0       2       0       1       5       1       0       x   0   c   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.2\n",
      "RandomForest Accuracy on Test set -> 0.4\n",
      "DecisionTree Accuracy on Test set -> 0.2\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l r b u n l e r n n g f r e w e h e n 1 2 k z h 1 l u g u 1 l n u l l e 2 k e l b r r f p e c l f b e r p c n p c l c c e n e w r k h n g h n u e f r v n c e c u n c n n c e n c e c h l f c u n c n n n f r n e n g n e e r n g h n g h u n v e r 2 e p r e n f c p u e r c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c b r c l b e l r b u n l e r n n g l l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r h p p e r p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g h e e f f e c v e n e f h e p r p e l l f v e r f e n e v e r l l l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l l e h 1 n r u c n l b e l r b u n l e r n n g l l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g l l n u l l b e l l e r n n g l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l n h e r e x p l e v e r n g p r e c n 7 n f u v e r e v e w w e b e u c h n e f l x b n u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h n l l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n e h e r l l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l n f r n p r c e n g e n p 2 0 1 7 l n g b e c h c u x 0 c f g u r e 1 h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l l k h w v n g e n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l u r l l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l l e h h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 l l u r n f l b e l r b u n l e r n n g f r e h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k n c e u r l l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e h e n w e c u c u r r e n l l e h e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r h e n e w l p r p e e e p n e u r l e c n f r e n f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n u r e h e x e n n f r e l l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e l h u g h e p z e f r e e u p e f u n c n w g v e n n n f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l l l n n h e r e g e u e n l l f n n f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n f n 3 l l r e e n l l f c n b e l e r n e j n l w h l e r e e n n f w e r e l e r n e l e r n v e l h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n e c 4 4 l l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e u u p w r n f 2 0 h e c n r b u n f l l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e l l w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e c e l l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f l l c l l e b f g l l b u n g q u n e w n p z n l l h e b v e l l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r n h e r w r e h e l l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l v r l l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l l k b v e w e g h e r e g r e r h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h l l l g b h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e l l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l l k n n e r e n g p c b u h e e x n g u c h e h c l l e l l 5 l l f c u e n x u e n r p e l b e l l u r e h l l f e x e n f f e r e n b l e e c n r e e r e l l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l r b u n l e r n n g f r e f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 h e g l f h e l l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 r e u e f r c n r u c n g r e e e e r n e r n l n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n n l n n r r e p e c v e l h e u p u f h e r e e w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 r e e p z n g v e n r n n g e x n 1 u r g l l e r n e c n r e e e c r b e n e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 l r f r e c h p l e x h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 h e c h u p u u n f g x x c e x 9 8 l e r n n g h e r e e r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 4 x c e x 9 8 q l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 h e n w e f x x c e x 9 8 n p z e q h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g p l n e n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 n l x 1 1 g c x x c e x 9 8 n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 g c x x c e x 9 8 p p w h e r e g c x x c e x 9 8 n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 n g c x x c e x 9 8 n l g c x x c e x 9 8 n r h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r u p p e r b u n f r h e l f u n c n r q x c e x 9 8 c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b q x c c x 8 4 c x l g n 1 c 1 x c e x b q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x 5 r q x c e x 9 8 n x c f x 8 6 q q r q x c e x 9 8 u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x 4 r q x c e x 9 8 w h c h p l e r q 1 x c e x 9 8 x e 2 x 8 9 x 4 r q x c e x 9 8 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 f e r e n u r n g h r q x c e x 9 8 x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b q c x p c 1 p n c x c e x b q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e f r e n e n e b l e f e c n r e e f 1 k n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r k w h e r e r k h e l f u n c n f r r e e k e f n e b e q n 3 l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e b u n e f n b c h e b h e r n n g p r c e u r e f l l f h w n n l g r h 1 l g r h 1 h e r n n g p r c e u r e f l l f r e q u r e r n n g e n b h e n u b e r f n b c h e u p e q n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r u p e x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 k 6 x 0 c 4 e x p e r e n l r e u l u r r e l z n f l l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 1 8 u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l l f r n e g r e w h n e e p n e w r k l l f w e e v l u e l l f n f f e r e n l l k n c p r e w h h e r n l n e l l e h l l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l l f n c p u e r v n p p l c n e f c l g e e n h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l l f n l n e l l e h w e c p r e u r h l l w e l l l f w h h e r e f h e r n l n e l l e h f r l l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l l e n 6 v e h u n g e n e n n u r l c e n e 1 h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n x c f x 8 6 r e n e n q u r e x c f x 8 7 2 n w l r e u r e f e l n e r e c n w e e v l u e u r h l l w e l l l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l l e h h e r e u l f l l f n h e c p e r r e u r z e n b l e 1 f r v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 n e r l e h w r n n g n e n g g e v e c n b e e e n f r b l e 1 l l f p e r f r b e n l l f h e x e u r e b l e 1 c p r n r e u l n h r e e l l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l e e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 n e r e c n x e 2 x 8 6 x 9 1 v e l l f u r l l g b 2 7 l l g b 2 7 l v r 7 b f g l l 6 l l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l l f u r l v r 7 b f g l l 6 l l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l l f u r l v r 7 b f g l l 6 l l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l c e n e 4 2 e v l u n f l l f n f c l g e e n n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l l p r b l e w e c n u c f c l g e e n e x p e r e n n r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l l n e x h 7 x 0 c l b e l r b u n h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r e b e w e e n p r e c e g e n c h r n l g c l g e h e c u r r e n e f h e r r e u l n r p h b n b f n e u n n g l l 5 n v g g f c e 2 3 w e l b u l l l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n b l e 2 w h c h h w l l f c h e v e h e e f h e r p e r f r n c e n r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l l e l l l n l l f n n n e e p l l e l l l c p n n b f g l l n h e u p e r r f l l f c p r e w h l l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l l r e p e c v e l b l e 2 e f g e e n c p r n n r p h 2 4 e h l l 1 1 c p n n 1 1 b f g l l 6 l l v g g f c e 5 l l f v g g f c e u r e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 h e r b u n f g e n e r n e h n c v e r u n b l n c e n r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f r p h c l l e r p h u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n h e b e p e r f r n c e r e p r e n r p h u b g v e n b 2 l l 1 5 e p e n e n l l e h 2 l l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 l e r n l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l l f w h l e x n e f l l w n g h e e x p e r e n e n g u e n 2 l l w e e v l u e u r l l f n h e c p e r n c l u n g b h l l n l l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 l l f h e c p e r r e r n e n h e e e e p f e u r e u e b 2 l l c n b e e e n f r b l e 3 u r l l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 e f g e e n c p r n n b u n r e u n l r b u n r p h u b n h e l b e l r b u n u e n r n n g e r e h e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 h e p r p e e h l l f c h e v e 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 l l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l 2 l l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e 1 x c 3 x 9 7 c x c 3 x 9 7 c n 1 x c 3 x 9 7 c x c 3 x 9 7 c x c 3 x 9 7 c r e p e c v e l f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h e c p l e x f n e r n u p e l e f n e r e n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 1 x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l l f r e e f f c e n n r p h u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n v e e u r e b k l n g e e n n r p h u b w h 6 0 r n n g e r e u r e b e f r f f e r e n p r e e r e n g n h e c n r e e n u b e r f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n e c 2 h e e n e b l e r e g l e r n f r e p r p e n n f 2 0 f f e r e n f r u r h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e w r h e n w e r e p l c e u r e n e b l e r e g n l l f b h e n e u e n n f n n e h e h n f l l h e c r r e p n n g h l l w e l n e b n f l l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e b e r v e f r f g 4 h e p e r f r n c e f l l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n n v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e r e e e p h r e e e p h n h e r p r n p r e e r f r e c n r e e n l l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l u n g u c h l r g e r e p h l e b p e r f r n c e n v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 h e p e r f r n c e c h n g e f g e e n n r p h u b n r n g p r e c n n v e b v r n g r e e n u b e r n b r e e e p h u r p p r c h l l f l l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n f n f l l n f l l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n c k n w l e g e e n h w r k w u p p r e n p r b h e n n l n u r l c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 p r j e c u p p r e b h n g h u n c p l e u c n c n n h n g h e u c n e v e l p e n f u n n n 1 5 c g 4 3 n n p r b n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 n g e n h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 l b e r g e r p e r n v j p e r x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 c r n n j h n e c n f r e f r c p u e r v n n e c l g e n l p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g e e e r n k n w l e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g n p r j c p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k h l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n n p r c 2 0 1 0 9 x g e n g q w n g n x f c l g e e n b p v e l b e l r b u n l e r n n g n p r c c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n x h e p e e n b e n u l v r e l b e l r b u n n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n e e e r n p e r n n l c h n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u f u c r e r n h u n g g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n e e e r n g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g u n c r p p u l n g e e n n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g z h n g h n g n z h u n g e p e n e n l b e l r b u n l e r n n g f r g e e n e e e r n n g e p r c e n g 2 0 1 7 1 6 k h r n e c n f r e n p r c c r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 k h h e r n u b p c e e h f r c n r u c n g e c n f r e e e e r n p e r n n l c h n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 j e h e l h e r j n h u e k r e v j l n g r g r h c k g u r r n r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 j r n z g h h r n j k k l n l k u l n n r u c n v r n l e h f r g r p h c l e l c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r f e r u c r n n r b u l x c 3 x b 2 e e p n e u r l e c n f r e n p r c c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 k r z h e v k u k e v e r n g e h n n g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k n p r c n p p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 l n c r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n e e e r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 p r k h v e l n z e r n e e p f c e r e c g n n n p r c b v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n e f e r p h l n g u n l g e b e f n r l u l g e p r g r e n n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j h n w f z g b b n c k h r p f n c c h r r e k p n n b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g u k n k k u l l b e l c l f c n n v e r v e w n e r n n l j u r n l f w r e h u n g n n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x n g x g e n g n z h u p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n n p r c j c p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 l u l l e n r n g r j n h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n n p r c p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.18\n",
      "XGBoost Accuracy on Test set -> 0.34\n",
      "RandomForest Accuracy on Test set -> 0.32\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_STM_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l r b u n l e r n n g f r e w e h e n 1 2 k z h 1 l u g u 1 l n u l l e 2 k e l b r r f p e c l f b e r p c n p c l c c e n e w r k h n g h n u e f r v n c e c u n c n n c e n c e c h l f c u n c n n n f r n e n g n e e r n g h n g h u n v e r 2 e p r e n f c p u e r c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c b r c l b e l r b u n l e r n n g l l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r h p p e r p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g h e e f f e c v e n e f h e p r p e l l f v e r f e n e v e r l l l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l l e h 1 n r u c n l b e l r b u n l e r n n g l l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g l l n u l l b e l l e r n n g l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l n h e r e x p l e v e r n g p r e c n 7 n f u v e r e v e w w e b e u c h n e f l x b n u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h n l l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n e h e r l l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l n f r n p r c e n g e n p 2 0 1 7 l n g b e c h c u x 0 c f g u r e 1 h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l l k h w v n g e n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l u r l l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l l e h h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 l l u r n f l b e l r b u n l e r n n g f r e h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k n c e u r l l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e h e n w e c u c u r r e n l l e h e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r h e n e w l p r p e e e p n e u r l e c n f r e n f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n u r e h e x e n n f r e l l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e l h u g h e p z e f r e e u p e f u n c n w g v e n n n f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l l l n n h e r e g e u e n l l f n n f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n f n 3 l l r e e n l l f c n b e l e r n e j n l w h l e r e e n n f w e r e l e r n e l e r n v e l h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n e c 4 4 l l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e u u p w r n f 2 0 h e c n r b u n f l l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e l l w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e c e l l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f l l c l l e b f g l l b u n g q u n e w n p z n l l h e b v e l l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r n h e r w r e h e l l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l v r l l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l l k b v e w e g h e r e g r e r h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h l l l g b h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e l l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l l k n n e r e n g p c b u h e e x n g u c h e h c l l e l l 5 l l f c u e n x u e n r p e l b e l l u r e h l l f e x e n f f e r e n b l e e c n r e e r e l l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l r b u n l e r n n g f r e f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 h e g l f h e l l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 r e u e f r c n r u c n g r e e e e r n e r n l n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n n l n n r r e p e c v e l h e u p u f h e r e e w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 r e e p z n g v e n r n n g e x n 1 u r g l l e r n e c n r e e e c r b e n e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 l r f r e c h p l e x h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 h e c h u p u u n f g x x c e x 9 8 l e r n n g h e r e e r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 4 x c e x 9 8 q l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 h e n w e f x x c e x 9 8 n p z e q h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g p l n e n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 n l x 1 1 g c x x c e x 9 8 n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 g c x x c e x 9 8 p p w h e r e g c x x c e x 9 8 n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 n g c x x c e x 9 8 n l g c x x c e x 9 8 n r h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r u p p e r b u n f r h e l f u n c n r q x c e x 9 8 c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b q x c c x 8 4 c x l g n 1 c 1 x c e x b q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x 5 r q x c e x 9 8 n x c f x 8 6 q q r q x c e x 9 8 u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x 4 r q x c e x 9 8 w h c h p l e r q 1 x c e x 9 8 x e 2 x 8 9 x 4 r q x c e x 9 8 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 f e r e n u r n g h r q x c e x 9 8 x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b q c x p c 1 p n c x c e x b q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e f r e n e n e b l e f e c n r e e f 1 k n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r k w h e r e r k h e l f u n c n f r r e e k e f n e b e q n 3 l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e b u n e f n b c h e b h e r n n g p r c e u r e f l l f h w n n l g r h 1 l g r h 1 h e r n n g p r c e u r e f l l f r e q u r e r n n g e n b h e n u b e r f n b c h e u p e q n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r u p e x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 k 6 x 0 c 4 e x p e r e n l r e u l u r r e l z n f l l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 1 8 u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l l f r n e g r e w h n e e p n e w r k l l f w e e v l u e l l f n f f e r e n l l k n c p r e w h h e r n l n e l l e h l l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l l f n c p u e r v n p p l c n e f c l g e e n h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l l f n l n e l l e h w e c p r e u r h l l w e l l l f w h h e r e f h e r n l n e l l e h f r l l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l l e n 6 v e h u n g e n e n n u r l c e n e 1 h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n x c f x 8 6 r e n e n q u r e x c f x 8 7 2 n w l r e u r e f e l n e r e c n w e e v l u e u r h l l w e l l l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l l e h h e r e u l f l l f n h e c p e r r e u r z e n b l e 1 f r v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 n e r l e h w r n n g n e n g g e v e c n b e e e n f r b l e 1 l l f p e r f r b e n l l f h e x e u r e b l e 1 c p r n r e u l n h r e e l l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l e e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 n e r e c n x e 2 x 8 6 x 9 1 v e l l f u r l l g b 2 7 l l g b 2 7 l v r 7 b f g l l 6 l l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l l f u r l v r 7 b f g l l 6 l l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l l f u r l v r 7 b f g l l 6 l l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l c e n e 4 2 e v l u n f l l f n f c l g e e n n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l l p r b l e w e c n u c f c l g e e n e x p e r e n n r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l l n e x h 7 x 0 c l b e l r b u n h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r e b e w e e n p r e c e g e n c h r n l g c l g e h e c u r r e n e f h e r r e u l n r p h b n b f n e u n n g l l 5 n v g g f c e 2 3 w e l b u l l l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n b l e 2 w h c h h w l l f c h e v e h e e f h e r p e r f r n c e n r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l l e l l l n l l f n n n e e p l l e l l l c p n n b f g l l n h e u p e r r f l l f c p r e w h l l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l l r e p e c v e l b l e 2 e f g e e n c p r n n r p h 2 4 e h l l 1 1 c p n n 1 1 b f g l l 6 l l v g g f c e 5 l l f v g g f c e u r e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 h e r b u n f g e n e r n e h n c v e r u n b l n c e n r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f r p h c l l e r p h u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n h e b e p e r f r n c e r e p r e n r p h u b g v e n b 2 l l 1 5 e p e n e n l l e h 2 l l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 l e r n l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l l f w h l e x n e f l l w n g h e e x p e r e n e n g u e n 2 l l w e e v l u e u r l l f n h e c p e r n c l u n g b h l l n l l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 l l f h e c p e r r e r n e n h e e e e p f e u r e u e b 2 l l c n b e e e n f r b l e 3 u r l l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 e f g e e n c p r n n b u n r e u n l r b u n r p h u b n h e l b e l r b u n u e n r n n g e r e h e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 h e p r p e e h l l f c h e v e 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 l l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l 2 l l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e 1 x c 3 x 9 7 c x c 3 x 9 7 c n 1 x c 3 x 9 7 c x c 3 x 9 7 c x c 3 x 9 7 c r e p e c v e l f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h e c p l e x f n e r n u p e l e f n e r e n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 1 x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l l f r e e f f c e n n r p h u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n v e e u r e b k l n g e e n n r p h u b w h 6 0 r n n g e r e u r e b e f r f f e r e n p r e e r e n g n h e c n r e e n u b e r f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n e c 2 h e e n e b l e r e g l e r n f r e p r p e n n f 2 0 f f e r e n f r u r h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e w r h e n w e r e p l c e u r e n e b l e r e g n l l f b h e n e u e n n f n n e h e h n f l l h e c r r e p n n g h l l w e l n e b n f l l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e b e r v e f r f g 4 h e p e r f r n c e f l l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n n v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e r e e e p h r e e e p h n h e r p r n p r e e r f r e c n r e e n l l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l u n g u c h l r g e r e p h l e b p e r f r n c e n v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 h e p e r f r n c e c h n g e f g e e n n r p h u b n r n g p r e c n n v e b v r n g r e e n u b e r n b r e e e p h u r p p r c h l l f l l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n f n f l l n f l l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n c k n w l e g e e n h w r k w u p p r e n p r b h e n n l n u r l c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 p r j e c u p p r e b h n g h u n c p l e u c n c n n h n g h e u c n e v e l p e n f u n n n 1 5 c g 4 3 n n p r b n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 n g e n h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 l b e r g e r p e r n v j p e r x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 c r n n j h n e c n f r e f r c p u e r v n n e c l g e n l p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g e e e r n k n w l e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g n p r j c p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k h l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n n p r c 2 0 1 0 9 x g e n g q w n g n x f c l g e e n b p v e l b e l r b u n l e r n n g n p r c c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n x h e p e e n b e n u l v r e l b e l r b u n n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n e e e r n p e r n n l c h n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u f u c r e r n h u n g g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n e e e r n g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g u n c r p p u l n g e e n n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g z h n g h n g n z h u n g e p e n e n l b e l r b u n l e r n n g f r g e e n e e e r n n g e p r c e n g 2 0 1 7 1 6 k h r n e c n f r e n p r c c r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 k h h e r n u b p c e e h f r c n r u c n g e c n f r e e e e r n p e r n n l c h n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 j e h e l h e r j n h u e k r e v j l n g r g r h c k g u r r n r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 j r n z g h h r n j k k l n l k u l n n r u c n v r n l e h f r g r p h c l e l c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r f e r u c r n n r b u l x c 3 x b 2 e e p n e u r l e c n f r e n p r c c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 k r z h e v k u k e v e r n g e h n n g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k n p r c n p p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 l n c r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n e e e r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 p r k h v e l n z e r n e e p f c e r e c g n n n p r c b v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n e f e r p h l n g u n l g e b e f n r l u l g e p r g r e n n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j h n w f z g b b n c k h r p f n c c h r r e k p n n b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g u k n k k u l l b e l c l f c n n v e r v e w n e r n n l j u r n l f w r e h u n g n n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x n g x g e n g n z h u p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n n p r c j c p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 l u l l e n r n g r j n h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n n p r c p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.18\n",
      "XGBoost Accuracy on Test set -> 0.34\n",
      "RandomForest Accuracy on Test set -> 0.32\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING LOW_RSW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l r b u n l e r n n g f r e w e h e n 1 2 k z h 1 l u g u 1 l n u l l e 2 k e l b r r f p e c l f b e r p c n p c l c c e n e w r k h n g h n u e f r v n c e c u n c n n c e n c e c h l f c u n c n n n f r n e n g n e e r n g h n g h u n v e r 2 e p r e n f c p u e r c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c b r c l b e l r b u n l e r n n g l l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r h p p e r p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g h e e f f e c v e n e f h e p r p e l l f v e r f e n e v e r l l l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l l e h 1 n r u c n l b e l r b u n l e r n n g l l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g l l n u l l b e l l e r n n g l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l n h e r e x p l e v e r n g p r e c n 7 n f u v e r e v e w w e b e u c h n e f l x b n u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h n l l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n e h e r l l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l n f r n p r c e n g e n p 2 0 1 7 l n g b e c h c u x 0 c f g u r e 1 h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l l k h w v n g e n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l u r l l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l l e h h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 l l u r n f l b e l r b u n l e r n n g f r e h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k n c e u r l l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e h e n w e c u c u r r e n l l e h e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r h e n e w l p r p e e e p n e u r l e c n f r e n f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n u r e h e x e n n f r e l l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e l h u g h e p z e f r e e u p e f u n c n w g v e n n n f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l l l n n h e r e g e u e n l l f n n f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n f n 3 l l r e e n l l f c n b e l e r n e j n l w h l e r e e n n f w e r e l e r n e l e r n v e l h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n e c 4 4 l l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e u u p w r n f 2 0 h e c n r b u n f l l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e l l w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e c e l l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f l l c l l e b f g l l b u n g q u n e w n p z n l l h e b v e l l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r n h e r w r e h e l l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l v r l l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l l k b v e w e g h e r e g r e r h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h l l l g b h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e l l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l l k n n e r e n g p c b u h e e x n g u c h e h c l l e l l 5 l l f c u e n x u e n r p e l b e l l u r e h l l f e x e n f f e r e n b l e e c n r e e r e l l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l r b u n l e r n n g f r e f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 h e g l f h e l l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 d r e u e f r c n r u c n g r e e e e r n e r n l n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n n l n n r r e p e c v e l h e u p u f h e r e e w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 r e e p z n g v e n r n n g e x n 1 u r g l l e r n e c n r e e e c r b e n e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 l r f r e c h p l e x h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 h e c h u p u u n f g x x c e x 9 8 l e r n n g h e r e e r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 4 x c e x 9 8 q l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 h e n w e f x x c e x 9 8 n p z e q h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g p l n e n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 n l x 1 1 g c x x c e x 9 8 n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 g c x x c e x 9 8 p p w h e r e g c x x c e x 9 8 n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 n g c x x c e x 9 8 n l g c x x c e x 9 8 n r h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r u p p e r b u n f r h e l f u n c n r q x c e x 9 8 c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x a 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b e q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b e q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b e q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b e q x c c x 8 4 c x l g n 1 c 1 x c e x b e q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x a 5 r q x c e x 9 8 n x c f x 8 6 q q r q x c e x 9 8 u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x a 4 r q x c e x 9 8 w h c h p l e r q 1 x c e x 9 8 x e 2 x 8 9 x a 4 r q x c e x 9 8 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 f e r e n u r n g h r q x c e x 9 8 x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b e q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b e q c x p c 1 p n c x c e x b e q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e f r e n e n e b l e f e c n r e e f 1 k n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r k w h e r e r k h e l f u n c n f r r e e k e f n e b e q n 3 l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e b u n e f n b c h e b h e r n n g p r c e u r e f l l f h w n n l g r h 1 l g r h 1 h e r n n g p r c e u r e f l l f r e q u r e r n n g e n b h e n u b e r f n b c h e u p e q n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r u p e x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 k 6 x 0 c 4 e x p e r e n l r e u l u r r e l z n f l l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 d 1 8 u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l l f r n e g r e w h n e e p n e w r k l l f w e e v l u e l l f n f f e r e n l l k n c p r e w h h e r n l n e l l e h l l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l l f n c p u e r v n p p l c n e f c l g e e n h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l l f n l n e l l e h w e c p r e u r h l l w e l l l f w h h e r e f h e r n l n e l l e h f r l l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l l e n 6 v e h u n g e n e n n u r l c e n e 1 h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n x c f x 8 6 r e n e n q u r e x c f x 8 7 2 n w l r e u r e f e l n e r e c n w e e v l u e u r h l l w e l l l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l l e h h e r e u l f l l f n h e c p e r r e u r z e n b l e 1 f r v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 d n e r l e h w r n n g n e n g g e v e c n b e e e n f r b l e 1 l l f p e r f r b e n l l f h e x e u r e b l e 1 c p r n r e u l n h r e e l l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 d n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 d n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l e e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 n e r e c n x e 2 x 8 6 x 9 1 v e l l f u r l l g b 2 7 l l g b 2 7 l v r 7 b f g l l 6 l l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l l f u r l v r 7 b f g l l 6 l l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l l f u r l v r 7 b f g l l 6 l l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l c e n e 4 2 e v l u n f l l f n f c l g e e n n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l l p r b l e w e c n u c f c l g e e n e x p e r e n n r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l l n e x h 7 x 0 c l b e l r b u n h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r e b e w e e n p r e c e g e n c h r n l g c l g e h e c u r r e n e f h e r r e u l n r p h b n b f n e u n n g l l 5 n v g g f c e 2 3 w e l b u l l l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n b l e 2 w h c h h w l l f c h e v e h e e f h e r p e r f r n c e n r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l l e l l l n l l f n n n e e p l l e l l l c p n n b f g l l n h e u p e r r f l l f c p r e w h l l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l l r e p e c v e l b l e 2 e f g e e n c p r n n r p h 2 4 e h l l 1 1 c p n n 1 1 b f g l l 6 l l v g g f c e 5 l l f v g g f c e u r e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 h e r b u n f g e n e r n e h n c v e r u n b l n c e n r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f r p h c l l e r p h u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n h e b e p e r f r n c e r e p r e n r p h u b g v e n b 2 l l 1 5 e p e n e n l l e h 2 l l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 d l e r n l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l l f w h l e x n e f l l w n g h e e x p e r e n e n g u e n 2 l l w e e v l u e u r l l f n h e c p e r n c l u n g b h l l n l l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 l l f h e c p e r r e r n e n h e e e e p f e u r e u e b 2 l l c n b e e e n f r b l e 3 u r l l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 e f g e e n c p r n n b u n r e u n l r b u n r p h u b n h e l b e l r b u n u e n r n n g e r e h e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 h e p r p e e h l l f c h e v e 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 l l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l 2 l l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e 1 x c 3 x 9 7 c x c 3 x 9 7 c n 1 x c 3 x 9 7 c x c 3 x 9 7 c x c 3 x 9 7 c r e p e c v e l f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h e c p l e x f n e r n u p e l e f n e r e n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 1 x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l l f r e e f f c e n n r p h u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n v e e u r e b k l n g e e n n r p h u b w h 6 0 r n n g e r e u r e b e f r f f e r e n p r e e r e n g n h e c n r e e n u b e r f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n e c 2 h e e n e b l e r e g l e r n f r e p r p e n n f 2 0 f f e r e n f r u r h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e w r h e n w e r e p l c e u r e n e b l e r e g n l l f b h e n e u e n n f n n e h e h n f l l h e c r r e p n n g h l l w e l n e b n f l l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e b e r v e f r f g 4 h e p e r f r n c e f l l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n n v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e r e e e p h r e e e p h n h e r p r n p r e e r f r e c n r e e n l l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x a 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l u n g u c h l r g e r e p h l e b p e r f r n c e n v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 h e p e r f r n c e c h n g e f g e e n n r p h u b n r n g p r e c n n v e b v r n g r e e n u b e r n b r e e e p h u r p p r c h l l f l l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n f n f l l n f l l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n c k n w l e g e e n h w r k w u p p r e n p r b h e n n l n u r l c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 d p r j e c u p p r e b h n g h u n c p l e u c n c n n h n g h e u c n e v e l p e n f u n n n 1 5 c g 4 3 n n p r b n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 n g e n h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 l b e r g e r p e r n v j p e r x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 c r n n j h n e c n f r e f r c p u e r v n n e c l g e n l p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g e e e r n k n w l e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g n p r j c p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k h l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n n p r c 2 0 1 0 9 x g e n g q w n g n x f c l g e e n b p v e l b e l r b u n l e r n n g n p r c c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n x h e p e e n b e n u l v r e l b e l r b u n n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n e e e r n p e r n n l c h n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u f u c r e r n h u n g g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n e e e r n g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g u n c r p p u l n g e e n n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g z h n g h n g n z h u n g e p e n e n l b e l r b u n l e r n n g f r g e e n e e e r n n g e p r c e n g 2 0 1 7 1 6 k h r n e c n f r e n p r c c r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 k h h e r n u b p c e e h f r c n r u c n g e c n f r e e e e r n p e r n n l c h n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 j e h e l h e r j n h u e k r e v j l n g r g r h c k g u r r n r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 j r n z g h h r n j k k l n l k u l n n r u c n v r n l e h f r g r p h c l e l c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r f e r u c r n n r b u l x c 3 x b 2 e e p n e u r l e c n f r e n p r c c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 k r z h e v k u k e v e r n g e h n n g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k n p r c n p p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 l n c r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n e e e r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 p r k h v e l n z e r n e e p f c e r e c g n n n p r c b v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n e f e r p h l n g u n l g e b e f n r l u l g e p r g r e n n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j h n w f z g b b n c k h r p f n c c h r r e k p n n b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g u k n k k u l l b e l c l f c n n v e r v e w n e r n n l j u r n l f w r e h u n g n n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x n g x g e n g n z h u p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n n p r c j c p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 l u l l e n r n g r j n h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n n p r c p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.34\n",
      "RandomForest Accuracy on Test set -> 0.3\n",
      "DecisionTree Accuracy on Test set -> 0.18\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_LOW_RSW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l r b u n l e r n n g f r e w e h e n 1 2 k z h 1 l u g u 1 l n u l l e 2 k e l b r r f p e c l f b e r p c n p c l c c e n e w r k h n g h n u e f r v n c e c u n c n n c e n c e c h l f c u n c n n n f r n e n g n e e r n g h n g h u n v e r 2 e p r e n f c p u e r c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c b r c l b e l r b u n l e r n n g l l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r h p p e r p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g h e e f f e c v e n e f h e p r p e l l f v e r f e n e v e r l l l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l l e h 1 n r u c n l b e l r b u n l e r n n g l l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g l l n u l l b e l l e r n n g l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l n h e r e x p l e v e r n g p r e c n 7 n f u v e r e v e w w e b e u c h n e f l x b n u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h n l l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n e h e r l l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l n f r n p r c e n g e n p 2 0 1 7 l n g b e c h c u x 0 c f g u r e 1 h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l l k h w v n g e n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l u r l l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l l e h h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 l l u r n f l b e l r b u n l e r n n g f r e h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k n c e u r l l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e h e n w e c u c u r r e n l l e h e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r h e n e w l p r p e e e p n e u r l e c n f r e n f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n u r e h e x e n n f r e l l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e l h u g h e p z e f r e e u p e f u n c n w g v e n n n f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l l l n n h e r e g e u e n l l f n n f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n f n 3 l l r e e n l l f c n b e l e r n e j n l w h l e r e e n n f w e r e l e r n e l e r n v e l h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n e c 4 4 l l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e u u p w r n f 2 0 h e c n r b u n f l l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e l l w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e c e l l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f l l c l l e b f g l l b u n g q u n e w n p z n l l h e b v e l l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r n h e r w r e h e l l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l v r l l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l l k b v e w e g h e r e g r e r h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h l l l g b h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e l l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l l k n n e r e n g p c b u h e e x n g u c h e h c l l e l l 5 l l f c u e n x u e n r p e l b e l l u r e h l l f e x e n f f e r e n b l e e c n r e e r e l l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l r b u n l e r n n g f r e f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 h e g l f h e l l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 r e u e f r c n r u c n g r e e e e r n e r n l n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n n l n n r r e p e c v e l h e u p u f h e r e e w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 r e e p z n g v e n r n n g e x n 1 u r g l l e r n e c n r e e e c r b e n e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 l r f r e c h p l e x h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 h e c h u p u u n f g x x c e x 9 8 l e r n n g h e r e e r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 4 x c e x 9 8 q l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 h e n w e f x x c e x 9 8 n p z e q h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g p l n e n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 n l x 1 1 g c x x c e x 9 8 n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 g c x x c e x 9 8 p p w h e r e g c x x c e x 9 8 n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 n g c x x c e x 9 8 n l g c x x c e x 9 8 n r h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r u p p e r b u n f r h e l f u n c n r q x c e x 9 8 c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b q x c c x 8 4 c x l g n 1 c 1 x c e x b q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x 5 r q x c e x 9 8 n x c f x 8 6 q q r q x c e x 9 8 u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x 4 r q x c e x 9 8 w h c h p l e r q 1 x c e x 9 8 x e 2 x 8 9 x 4 r q x c e x 9 8 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 f e r e n u r n g h r q x c e x 9 8 x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b q c x p c 1 p n c x c e x b q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e f r e n e n e b l e f e c n r e e f 1 k n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r k w h e r e r k h e l f u n c n f r r e e k e f n e b e q n 3 l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e b u n e f n b c h e b h e r n n g p r c e u r e f l l f h w n n l g r h 1 l g r h 1 h e r n n g p r c e u r e f l l f r e q u r e r n n g e n b h e n u b e r f n b c h e u p e q n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r u p e x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 k 6 x 0 c 4 e x p e r e n l r e u l u r r e l z n f l l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 1 8 u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l l f r n e g r e w h n e e p n e w r k l l f w e e v l u e l l f n f f e r e n l l k n c p r e w h h e r n l n e l l e h l l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l l f n c p u e r v n p p l c n e f c l g e e n h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l l f n l n e l l e h w e c p r e u r h l l w e l l l f w h h e r e f h e r n l n e l l e h f r l l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l l e n 6 v e h u n g e n e n n u r l c e n e 1 h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n x c f x 8 6 r e n e n q u r e x c f x 8 7 2 n w l r e u r e f e l n e r e c n w e e v l u e u r h l l w e l l l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l l e h h e r e u l f l l f n h e c p e r r e u r z e n b l e 1 f r v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 n e r l e h w r n n g n e n g g e v e c n b e e e n f r b l e 1 l l f p e r f r b e n l l f h e x e u r e b l e 1 c p r n r e u l n h r e e l l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l e e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 n e r e c n x e 2 x 8 6 x 9 1 v e l l f u r l l g b 2 7 l l g b 2 7 l v r 7 b f g l l 6 l l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l l f u r l v r 7 b f g l l 6 l l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l l f u r l v r 7 b f g l l 6 l l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l c e n e 4 2 e v l u n f l l f n f c l g e e n n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l l p r b l e w e c n u c f c l g e e n e x p e r e n n r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l l n e x h 7 x 0 c l b e l r b u n h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r e b e w e e n p r e c e g e n c h r n l g c l g e h e c u r r e n e f h e r r e u l n r p h b n b f n e u n n g l l 5 n v g g f c e 2 3 w e l b u l l l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n b l e 2 w h c h h w l l f c h e v e h e e f h e r p e r f r n c e n r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l l e l l l n l l f n n n e e p l l e l l l c p n n b f g l l n h e u p e r r f l l f c p r e w h l l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l l r e p e c v e l b l e 2 e f g e e n c p r n n r p h 2 4 e h l l 1 1 c p n n 1 1 b f g l l 6 l l v g g f c e 5 l l f v g g f c e u r e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 h e r b u n f g e n e r n e h n c v e r u n b l n c e n r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f r p h c l l e r p h u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n h e b e p e r f r n c e r e p r e n r p h u b g v e n b 2 l l 1 5 e p e n e n l l e h 2 l l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 l e r n l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l l f w h l e x n e f l l w n g h e e x p e r e n e n g u e n 2 l l w e e v l u e u r l l f n h e c p e r n c l u n g b h l l n l l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 l l f h e c p e r r e r n e n h e e e e p f e u r e u e b 2 l l c n b e e e n f r b l e 3 u r l l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 e f g e e n c p r n n b u n r e u n l r b u n r p h u b n h e l b e l r b u n u e n r n n g e r e h e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 h e p r p e e h l l f c h e v e 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 l l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l 2 l l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e 1 x c 3 x 9 7 c x c 3 x 9 7 c n 1 x c 3 x 9 7 c x c 3 x 9 7 c x c 3 x 9 7 c r e p e c v e l f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h e c p l e x f n e r n u p e l e f n e r e n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 1 x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l l f r e e f f c e n n r p h u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n v e e u r e b k l n g e e n n r p h u b w h 6 0 r n n g e r e u r e b e f r f f e r e n p r e e r e n g n h e c n r e e n u b e r f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n e c 2 h e e n e b l e r e g l e r n f r e p r p e n n f 2 0 f f e r e n f r u r h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e w r h e n w e r e p l c e u r e n e b l e r e g n l l f b h e n e u e n n f n n e h e h n f l l h e c r r e p n n g h l l w e l n e b n f l l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e b e r v e f r f g 4 h e p e r f r n c e f l l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n n v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e r e e e p h r e e e p h n h e r p r n p r e e r f r e c n r e e n l l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l u n g u c h l r g e r e p h l e b p e r f r n c e n v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 h e p e r f r n c e c h n g e f g e e n n r p h u b n r n g p r e c n n v e b v r n g r e e n u b e r n b r e e e p h u r p p r c h l l f l l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n f n f l l n f l l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n c k n w l e g e e n h w r k w u p p r e n p r b h e n n l n u r l c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 p r j e c u p p r e b h n g h u n c p l e u c n c n n h n g h e u c n e v e l p e n f u n n n 1 5 c g 4 3 n n p r b n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 n g e n h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 l b e r g e r p e r n v j p e r x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 c r n n j h n e c n f r e f r c p u e r v n n e c l g e n l p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g e e e r n k n w l e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g n p r j c p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k h l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n n p r c 2 0 1 0 9 x g e n g q w n g n x f c l g e e n b p v e l b e l r b u n l e r n n g n p r c c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n x h e p e e n b e n u l v r e l b e l r b u n n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n e e e r n p e r n n l c h n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u f u c r e r n h u n g g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n e e e r n g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g u n c r p p u l n g e e n n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g z h n g h n g n z h u n g e p e n e n l b e l r b u n l e r n n g f r g e e n e e e r n n g e p r c e n g 2 0 1 7 1 6 k h r n e c n f r e n p r c c r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 k h h e r n u b p c e e h f r c n r u c n g e c n f r e e e e r n p e r n n l c h n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 j e h e l h e r j n h u e k r e v j l n g r g r h c k g u r r n r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 j r n z g h h r n j k k l n l k u l n n r u c n v r n l e h f r g r p h c l e l c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r f e r u c r n n r b u l x c 3 x b 2 e e p n e u r l e c n f r e n p r c c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 k r z h e v k u k e v e r n g e h n n g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k n p r c n p p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 l n c r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n e e e r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 p r k h v e l n z e r n e e p f c e r e c g n n n p r c b v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n e f e r p h l n g u n l g e b e f n r l u l g e p r g r e n n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j h n w f z g b b n c k h r p f n c c h r r e k p n n b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g u k n k k u l l b e l c l f c n n v e r v e w n e r n n l j u r n l f w r e h u n g n n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x n g x g e n g n z h u p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n n p r c j c p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 l u l l e n r n g r j n h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n n p r c p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.18\n",
      "XGBoost Accuracy on Test set -> 0.34\n",
      "RandomForest Accuracy on Test set -> 0.32\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING STM_RSW_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       w       e       h       e       n       1       2       k       z       h       1       l       u       g       u       1       l       n       u       l       l       e       2       k       e       l       b       r       r       f       p       e       c       l       f       b       e       r       p       c       n       p       c       l       c       c       e       n       e       w       r       k       h       n       g       h       n       u       e       f       r       v       n       c       e       c       u       n       c       n       n       c       e       n       c       e       c       h       l       f       c       u       n       c       n       n       n       f       r       n       e       n       g       n       e       e       r       n       g       h       n       g       h       u       n       v       e       r       2       e       p       r       e       n       f       c       p       u       e       r       c       e       n       c       e       j       h       n       h       p       k       n       u       n       v       e       r       r       x       v       1       7       0       2       0       6       0       8       6       v       4       c       l       g       1       6       c       2       0       1       7       1       h       e       n       w       e       1       2       3       1       z       h       k       1       2       0       6       g       l       l       u       n       0       l       n       l       u       l       l       e       g       l       c       b       r       c       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       l       g       e       n       e       r       l       l       e       r       n       n       g       f       r       e       w       r       k       w       h       c       h       g       n       n       n       n       c       e       r       b       u       n       v       e       r       e       f       l       b       e       l       r       h       e       r       h       n       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       c       u       r       r       e       n       l       l       e       h       h       v       e       e       h       e       r       r       e       r       c       e       u       p       n       n       h       e       e       x       p       r       e       n       f       r       f       h       e       l       b       e       l       r       b       u       n       r       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       h       p       p       e       r       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       b       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       h       c       h       h       v       e       e       v       e       r       l       v       n       g       e       1       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       l       e       f       n       e       p       r       e       c       n       2       h       e       l       e       r       n       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       f       r       e       e       n       b       l       n       g       l       l       h       e       r       e       e       b       e       l       e       r       n       e       j       n       l       n       h       w       h       n       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       p       r       e       c       n       w       h       c       h       g       u       r       n       e       e       r       c       e       c       r       e       e       f       h       e       l       f       u       n       c       n       c       n       b       e       e       r       v       e       b       v       r       n       l       b       u       n       n       g       h       e       e       f       f       e       c       v       e       n       e       f       h       e       p       r       p       e       l       l       f       v       e       r       f       e       n       e       v       e       r       l       l       l       k       n       c       p       u       e       r       v       n       p       p       l       c       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       l       e       h       1       n       r       u       c       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       l       6       1       1       l       e       r       n       n       g       f       r       e       w       r       k       e       l       w       h       p       r       b       l       e       f       l       b       e       l       b       g       u       u       n       l       k       e       n       g       l       e       l       b       e       l       l       e       r       n       n       g       l       l       n       u       l       l       b       e       l       l       e       r       n       n       g       l       l       2       6       w       h       c       h       u       e       n       n       n       c       e       g       n       e       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       l       l       l       e       r       n       n       g       h       e       r       e       l       v       e       p       r       n       c       e       f       e       c       h       l       b       e       l       n       v       l       v       e       n       h       e       e       c       r       p       n       f       n       n       n       c       e       e       r       b       u       n       v       e       r       h       e       e       f       l       b       e       l       u       c       h       l       e       r       n       n       g       r       e       g       u       b       l       e       f       r       n       r       e       l       w       r       l       p       r       b       l       e       w       h       c       h       h       v       e       l       b       e       l       b       g       u       n       e       x       p       l       e       f       c       l       g       e       e       n       8       e       v       e       n       h       u       n       c       n       n       p       r       e       c       h       e       p       r       e       c       e       g       e       f       r       n       g       l       e       f       c       l       g       e       h       e       h       h       e       p       e       r       n       p       r       b       b       l       n       n       e       g       e       g       r       u       p       n       l       e       l       k       e       l       b       e       n       n       h       e       r       h       e       n       c       e       r       e       n       u       r       l       g       n       r       b       u       n       f       g       e       l       b       e       l       e       c       h       f       c       l       g       e       f       g       1       n       e       f       u       n       g       n       g       l       e       g       e       l       b       e       l       n       h       e       r       e       x       p       l       e       v       e       r       n       g       p       r       e       c       n       7       n       f       u       v       e       r       e       v       e       w       w       e       b       e       u       c       h       n       e       f       l       x       b       n       u       b       n       p       r       v       e       c       r       w       p       n       n       f       r       e       c       h       v       e       p       e       c       f       e       b       h       e       r       b       u       n       f       r       n       g       c       l       l       e       c       e       f       r       h       e       r       u       e       r       f       g       1       b       f       e       c       u       l       p       r       e       c       e       l       p       r       e       c       u       c       h       r       n       g       r       b       u       n       f       r       e       v       e       r       v       e       b       e       f       r       e       r       e       l       e       e       v       e       p       r       u       c       e       r       c       n       r       e       u       c       e       h       e       r       n       v       e       e       n       r       k       n       h       e       u       e       n       c       e       c       n       b       e       e       r       c       h       e       w       h       c       h       v       e       w       c       h       n       l       l       e       h       u       e       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       n       l       e       r       n       b       p       z       n       g       n       e       n       e       r       g       f       u       n       c       n       b       e       n       h       e       e       l       8       1       1       2       8       6       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       e       g       h       f       f       c       u       l       n       r       e       p       r       e       e       n       n       g       x       u       r       e       r       b       u       n       e       h       e       r       l       l       e       h       e       x       e       n       h       e       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       g       b       b       n       g       n       u       p       p       r       v       e       c       r       r       e       g       r       e       n       e       l       w       h       l       b       e       l       r       b       u       n       7       2       7       w       h       c       h       v       k       n       g       h       u       p       n       b       u       h       v       e       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       h       e       n       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       1       c       n       f       e       r       e       n       c       e       n       n       e       u       r       l       n       f       r       n       p       r       c       e       n       g       e       n       p       2       0       1       7       l       n       g       b       e       c       h       c       u       x       0       c       f       g       u       r       e       1       h       e       r       e       l       w       r       l       w       h       c       h       r       e       u       b       l       e       b       e       e       l       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       e       e       f       c       l       g       e       u       n       l       r       b       u       n       b       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       u       l       l       r       b       u       n       n       h       p       p       e       r       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       2       0       e       x       e       n       n       g       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       e       l       w       h       h       e       l       l       k       h       w       v       n       g       e       n       e       h       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       h       e       l       e       f       n       e       p       r       e       c       n       w       h       c       h       v       k       n       g       r       n       g       u       p       n       n       h       e       f       r       f       h       e       l       b       e       l       r       b       u       n       h       e       e       c       n       h       h       e       p       l       n       e       p       r       e       e       r       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       w       h       c       h       e       n       b       l       e       c       b       n       n       f       r       e       e       l       e       r       n       n       g       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       r       e       e       b       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       v       e       r       g       e       n       c       e       k       l       b       e       w       e       e       n       h       e       g       r       u       n       r       u       h       l       b       e       l       r       b       u       n       n       h       e       r       b       u       n       p       r       e       c       e       b       h       e       r       e       e       b       f       x       n       g       p       l       n       e       w       e       h       w       h       h       e       p       z       n       f       l       e       f       n       e       p       r       e       c       n       n       z       e       h       e       l       f       u       n       c       n       f       h       e       r       e       e       c       n       b       e       r       e       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       n       w       h       c       h       h       e       r       g       n       l       l       f       u       n       c       n       b       e       n       z       e       g       e       e       r       v       e       l       r       e       p       l       c       e       b       e       c       r       e       n       g       e       q       u       e       n       c       e       f       u       p       p       e       r       b       u       n       f       l       l       w       n       g       h       p       z       n       r       e       g       w       e       e       r       v       e       c       r       e       e       e       r       v       e       f       u       n       c       n       u       p       e       h       e       l       e       f       n       e       p       r       e       c       n       l       e       r       n       f       r       e       w       e       v       e       r       g       e       h       e       l       e       f       l       l       h       e       n       v       u       l       r       e       e       b       e       h       e       l       f       r       h       e       f       r       e       n       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       n       h       w       h       e       p       l       n       e       p       r       e       e       r       f       l       l       h       e       n       v       u       l       r       e       e       c       n       b       e       l       e       r       n       e       j       n       l       u       r       l       l       f       c       n       b       e       u       e       h       l       l       w       n       l       n       e       e       l       n       c       n       l       b       e       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       e       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       c       n       b       e       l       n       e       r       r       n       f       r       n       n       e       e       p       n       e       w       r       k       r       e       p       e       c       v       e       l       f       g       2       l       l       u       r       e       k       e       c       h       c       h       r       f       u       r       l       l       f       w       h       e       r       e       f       r       e       c       n       f       w       r       e       e       h       w       n       w       e       v       e       r       f       h       e       e       f       f       e       c       v       e       n       e       f       u       r       e       l       n       e       v       e       r       l       l       l       k       u       c       h       c       r       w       p       n       n       p       r       e       c       n       n       v       e       n       e       e       p       r       e       c       n       b       e       n       h       u       n       g       e       n       e       w       e       l       l       n       e       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       l       e       h       h       e       l       b       e       l       r       b       u       n       f       r       h       e       e       k       n       c       l       u       e       b       h       u       n       l       r       b       u       n       e       g       h       e       g       e       r       b       u       n       n       f       g       1       n       x       u       r       e       r       b       u       n       h       e       r       n       g       r       b       u       n       n       v       e       n       f       g       1       b       h       e       u       p       e       r       r       f       u       r       e       l       n       b       h       f       h       e       v       e       r       f       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       f       g       u       r       e       2       l       l       u       r       n       f       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       h       e       p       c       r       c       l       e       e       n       e       h       e       u       p       u       u       n       f       h       e       f       u       n       c       n       f       p       r       e       e       r       z       e       b       x       c       e       x       9       8       w       h       c       h       c       n       b       e       f       e       u       r       e       v       e       c       r       r       f       u       l       l       c       n       n       e       c       e       l       e       r       f       e       e       p       n       e       w       r       k       h       e       b       l       u       e       n       g       r       e       e       n       c       r       c       l       e       r       e       p       l       n       e       n       l       e       f       n       e       r       e       p       e       c       v       e       l       w       n       e       x       f       u       n       c       n       x       c       f       x       9       5       1       n       x       c       f       x       9       5       2       r       e       g       n       e       h       e       e       w       r       e       e       r       e       p       e       c       v       e       l       h       e       b       l       c       k       h       r       r       w       n       c       e       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       f       h       e       e       w       r       e       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       e       h       n       e       u       p       u       u       n       c       r       r       e       p       n       h       e       p       l       n       e       b       e       l       n       g       n       g       f       f       e       r       e       n       r       e       e       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       e       n       e       b       h       g       r       n       l       e       f       n       e       h       e       u       p       u       f       h       e       f       r       e       x       u       r       e       f       h       e       r       e       e       p       r       e       c       n       f       x       c       2       x       b       7       x       c       e       x       9       8       n       q       r       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       2       x       0       c       2       r       e       l       e       w       r       k       n       c       e       u       r       l       l       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       n       e       c       e       r       f       r       r       e       v       e       w       e       p       c       l       e       c       h       n       q       u       e       f       e       c       n       r       e       e       h       e       n       w       e       c       u       c       u       r       r       e       n       l       l       e       h       e       c       n       r       e       e       r       n       f       r       e       r       r       n       z       e       e       c       n       r       e       e       1       6       1       3       4       r       e       p       p       u       l       r       e       n       e       b       l       e       p       r       e       c       v       e       e       l       u       b       l       e       f       r       n       c       h       n       e       l       e       r       n       n       g       k       n       h       e       p       l       e       r       n       n       g       f       e       c       n       r       e       e       w       b       e       n       h       e       u       r       c       u       c       h       g       r       e       e       l       g       r       h       w       h       e       r       e       l       c       l       l       p       l       h       r       e       c       n       r       e       e       e       c       h       p       l       n       e       1       n       h       u       c       n       n       b       e       n       e       g       r       e       n       n       e       e       p       l       e       r       n       n       g       f       r       e       w       r       k       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       h       e       n       e       w       l       p       r       p       e       e       e       p       n       e       u       r       l       e       c       n       f       r       e       n       f       2       0       v       e       r       c       e       h       p       r       b       l       e       b       n       r       u       c       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       h       e       p       l       n       e       n       g       l       b       l       l       f       u       n       c       n       e       f       n       e       n       r       e       e       h       e       n       u       r       e       h       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       n       l       e       f       n       e       p       r       e       c       n       c       n       b       e       u       p       e       b       c       r       e       e       e       r       v       e       f       u       n       c       n       u       r       e       h       e       x       e       n       n       f       r       e       l       l       p       r       b       l       e       b       u       h       e       x       e       n       n       n       n       r       v       l       b       e       c       u       e       l       e       r       n       n       g       l       e       f       n       e       p       r       e       c       n       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       l       h       u       g       h       e       p       z       e       f       r       e       e       u       p       e       f       u       n       c       n       w       g       v       e       n       n       n       f       u       p       e       l       e       f       n       e       p       r       e       c       n       w       n       l       p       r       v       e       c       n       v       e       r       g       e       f       r       c       l       f       c       n       l       c       n       e       q       u       e       n       l       w       u       n       c       l       e       r       h       w       b       n       u       c       h       n       u       p       e       f       u       n       c       n       f       r       h       e       r       l       e       w       e       b       e       r       v       e       h       w       e       v       e       r       h       h       e       u       p       e       f       u       n       c       n       n       n       f       c       n       b       e       e       r       v       e       f       r       v       r       n       l       b       u       n       n       g       w       h       c       h       l       l       w       u       e       x       e       n       u       r       l       l       l       n       n       h       e       r       e       g       e       u       e       n       l       l       f       n       n       f       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       f       r       e       r       e       f       f       e       r       e       n       1       w       e       e       x       p       l       c       l       e       f       n       e       l       f       u       n       c       n       f       r       f       r       e       w       h       l       e       n       l       h       e       l       f       u       n       c       n       f       r       n       g       l       e       r       e       e       w       e       f       n       e       n       n       f       2       w       e       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       w       h       l       e       n       f       n       3       l       l       r       e       e       n       l       l       f       c       n       b       e       l       e       r       n       e       j       n       l       w       h       l       e       r       e       e       n       n       f       w       e       r       e       l       e       r       n       e       l       e       r       n       v       e       l       h       e       e       c       h       n       g       e       n       h       e       e       n       e       b       l       e       l       e       r       n       n       g       r       e       p       r       n       b       e       c       u       e       h       w       n       n       u       r       e       x       p       e       r       e       n       e       c       4       4       l       l       f       c       n       g       e       b       e       e       r       r       e       u       l       b       u       n       g       r       e       r       e       e       b       u       b       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       f       h       e       r       e       u       l       f       f       r       e       r       e       e       v       e       n       w       r       e       h       n       h       e       f       r       n       g       l       e       r       e       e       u       u       p       w       r       n       f       2       0       h       e       c       n       r       b       u       n       f       l       l       f       r       e       f       r       w       e       e       x       e       n       f       r       c       l       f       c       n       2       0       r       b       u       n       l       e       r       n       n       g       b       p       r       p       n       g       r       b       u       n       b       e       l       f       r       h       e       f       r       e       n       e       r       v       e       h       e       g       r       e       n       l       e       r       n       p       l       n       e       w       r       h       l       e       c       n       w       e       e       r       v       e       h       e       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       b       v       r       n       l       b       u       n       n       g       h       v       n       g       b       e       r       v       e       h       h       e       u       p       e       f       u       n       c       n       n       2       0       w       p       e       c       l       c       e       f       v       r       n       l       b       u       n       n       g       l       b       u       n       h       e       l       e       w       e       p       r       p       e       b       v       e       h       r       e       e       r       e       g       e       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       w       h       c       h       r       e       f       f       e       r       e       n       f       r       2       0       b       u       w       e       h       w       r       e       e       f       f       e       c       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       n       u       b       e       r       f       p       e       c       l       z       e       l       g       r       h       h       v       e       b       e       e       n       p       r       p       e       r       e       h       e       l       l       k       n       h       v       e       h       w       n       h       e       r       e       f       f       e       c       v       e       n       e       n       n       c       p       u       e       r       v       n       p       p       l       c       n       u       c       h       f       c       l       g       e       e       n       8       1       1       2       8       e       x       p       r       e       n       r       e       c       g       n       n       3       0       n       h       n       r       e       n       n       e       n       1       0       g       e       n       g       e       l       8       e       f       n       e       h       e       l       b       e       l       r       b       u       n       f       r       n       n       n       c       e       v       e       c       r       c       n       n       n       g       h       e       p       r       b       b       l       e       f       h       e       n       n       c       e       h       v       n       g       e       c       h       l       b       e       l       h       e       l       g       v       e       r       e       g       g       n       p       r       p       e       r       l       b       e       l       r       b       u       n       n       n       n       c       e       w       h       n       g       l       e       l       b       e       l       e       g       n       n       g       g       u       n       r       r       n       g       l       e       r       b       u       n       w       h       e       p       e       k       h       e       n       g       l       e       l       b       e       l       n       p       r       p       e       n       l       g       r       h       c       l       l       e       l       l       w       h       c       h       n       e       r       v       e       p       z       n       p       r       c       e       b       e       n       w       l       e       r       e       n       e       r       g       b       e       e       l       n       g       e       l       2       8       h       e       n       e       f       n       e       h       r       e       e       l       e       r       e       n       e       r       g       b       e       e       l       c       l       l       e       c       e       l       l       n       w       h       c       h       h       e       b       l       p       e       r       f       r       f       e       u       r       e       l       e       r       n       n       g       p       r       v       e       b       n       g       h       e       e       x       r       h       e       n       l       e       r       n       p       r       c       n       r       n       r       e       l       n       c       r       p       r       e       e       l       r       e       h       e       e       l       g       e       n       g       6       e       v       e       l       p       e       n       c       c       e       l       e       r       e       v       e       r       n       f       l       l       c       l       l       e       b       f       g       l       l       b       u       n       g       q       u       n       e       w       n       p       z       n       l       l       h       e       b       v       e       l       l       e       h       u       e       h       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       n       h       e       r       w       r       e       h       e       l       l       k       e       x       e       n       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       l       w       h       l       b       e       l       r       b       u       n       g       e       n       g       n       h       u       7       p       r       p       e       l       v       r       l       l       e       h       b       e       x       e       n       n       g       u       p       p       r       v       e       c       r       r       e       g       r       e       r       w       h       c       h       f       g       f       u       n       c       n       e       c       h       c       p       n       e       n       f       h       e       r       b       u       n       u       l       n       e       u       l       b       u       p       p       r       v       e       c       r       c       h       n       e       x       n       g       e       l       2       7       h       e       n       e       x       e       n       e       b       n       g       r       e       h       e       l       l       k       b       v       e       w       e       g       h       e       r       e       g       r       e       r       h       e       h       w       e       h       u       n       g       h       e       v       e       c       r       r       e       e       e       l       h       e       w       e       k       r       e       g       r       e       r       c       n       l       e       b       e       e       r       p       e       r       f       r       n       c       e       n       n       e       h       e       h       l       l       l       g       b       h       e       l       e       r       n       n       g       f       h       r       e       e       e       l       b       e       n       l       c       l       l       p       l       h       r       p       r       n       f       u       n       c       n       e       c       h       p       l       n       e       l       l       l       g       b       u       n       b       l       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       x       e       n       n       g       c       u       r       r       e       n       e       e       p       l       e       r       n       n       g       l       g       r       h       3       x       0       c       r       e       h       e       l       l       k       n       n       e       r       e       n       g       p       c       b       u       h       e       e       x       n       g       u       c       h       e       h       c       l       l       e       l       l       5       l       l       f       c       u       e       n       x       u       e       n       r       p       e       l       b       e       l       l       u       r       e       h       l       l       f       e       x       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       r       e       l       l       k       n       w       h       c       h       h       e       p       r       e       c       e       l       b       e       l       r       b       u       n       f       r       p       l       e       c       n       b       e       e       x       p       r       e       e       b       l       n       e       r       c       b       n       n       f       h       e       l       b       e       l       r       b       u       n       f       h       e       r       n       n       g       n       h       u       h       v       e       n       r       e       r       c       n       n       h       e       r       b       u       n       e       g       n       r       e       q       u       r       e       e       n       f       h       e       x       u       e       n       r       p       e       l       n       n       h       n       k       h       e       n       r       u       c       n       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       l       l       f       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       w       e       f       r       n       r       u       c       e       h       w       l       e       r       n       n       g       l       e       e       c       n       r       e       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       h       e       n       e       c       r       b       e       h       e       l       e       r       n       n       g       f       f       r       e       3       1       p       r       b       l       e       f       r       u       l       n       l       e       x       r       e       n       e       h       e       n       p       u       p       c       e       n       1       2       c       e       n       e       h       e       c       p       l       e       e       e       f       l       b       e       l       w       h       e       r       e       c       h       e       n       u       b       e       r       f       p       b       l       e       l       b       e       l       v       l       u       e       w       e       c       n       e       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       l       p       r       b       l       e       w       h       e       r       e       f       r       e       c       h       n       p       u       p       l       e       x       x       e       2       x       8       8       x       8       8       x       h       e       r       e       l       b       e       l       r       b       u       n       x       1       x       2       x       c       x       e       2       x       8       8       x       8       8       r       c       h       e       r       e       x       c       e       x       p       r       e       e       h       e       p       r       b       b       l       f       h       e       p       l       e       x       h       v       n       g       h       e       c       h       l       b       e       l       c       n       h       u       h       h       e       p       c       c       n       r       n       h       x       c       x       e       2       x       8       8       x       8       8       0       1       n       c       1       x       c       1       h       e       g       l       f       h       e       l       l       p       r       b       l       e       l       e       r       n       p       p       n       g       f       u       n       c       n       g       x       x       e       2       x       8       6       x       9       2       b       e       w       e       e       n       n       n       p       u       p       l       e       x       n       c       r       r       e       p       n       n       g       l       b       e       l       r       b       u       n       h       e       r       e       w       e       w       n       l       e       r       n       h       e       p       p       n       g       f       u       n       c       n       g       x       b       e       c       n       r       e       e       b       e       e       l       e       c       n       r       e       e       c       n       f       e       f       p       l       n       e       n       n       e       f       l       e       f       n       e       l       e       c       h       p       l       n       e       n       x       e       2       x       8       8       x       8       8       n       e       f       n       e       p       l       f       u       n       c       n       n       x       c       2       x       b       7       x       c       e       x       9       8       x       x       e       2       x       8       6       x       9       2       0       1       p       r       e       e       r       z       e       b       x       c       e       x       9       8       e       e       r       n       e       w       h       e       h       e       r       p       l       e       e       n       h       e       l       e       f       r       r       g       h       u       b       r       e       e       e       c       h       l       e       f       n       e       x       e       2       x       8       8       x       8       8       l       h       l       r       b       u       n       q       q       1       q       2       q       c       p       c       v       e       r       e       q       c       x       e       2       x       8       8       x       8       8       0       1       n       c       1       q       c       1       b       u       l       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       f       l       l       w       n       g       2       0       w       e       u       e       p       r       b       b       l       c       p       l       f       u       n       c       n       n       x       x       c       e       x       9       8       x       c       f       x       8       3       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       w       h       e       r       e       x       c       f       x       8       3       x       c       2       x       b       7       g       f       u       n       c       n       x       c       f       x       9       5       x       c       2       x       b       7       n       n       e       x       f       u       n       c       n       b       r       n       g       h       e       x       c       f       x       9       5       n       h       u       p       u       f       f       u       n       c       n       f       x       x       c       e       x       9       8       n       c       r       r       e       p       n       e       n       c       e       w       h       p       l       n       e       n       n       f       x       x       e       2       x       8       6       x       9       2       r       r       e       l       v       l       u       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       e       p       e       n       n       g       n       h       e       p       l       e       x       n       h       e       p       r       e       e       r       x       c       e       x       9       8       n       c       n       k       e       n       f       r       f       r       p       l       e       f       r       c       n       b       e       l       n       e       r       r       n       f       r       n       f       x       w       h       e       r       e       x       c       e       x       9       8       h       e       r       n       f       r       n       r       x       f       r       c       p       l       e       x       f       r       c       n       b       e       e       e       p       n       e       w       r       k       p       e       r       f       r       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       h       e       n       x       c       e       x       9       8       h       e       n       e       w       r       k       p       r       e       e       r       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       c       e       b       x       c       f       x       9       5       x       c       2       x       b       7       h       r       n       l       g       e       n       e       r       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       e       w       h       c       h       u       p       u       u       n       f       r       x       e       2       x       8       0       x       9       c       f       x       e       2       x       8       0       x       9       r       e       u       e       f       r       c       n       r       u       c       n       g       r       e       e       e       e       r       n       e       r       n       l       n       e       x       p       l       e       e       n       r       e       x       c       f       x       9       5       x       c       2       x       b       7       h       w       n       n       f       g       2       h       e       n       h       e       p       r       b       b       l       f       h       e       p       l       e       x       f       l       l       n       g       n       l       e       f       n       e       g       v       e       n       b       l       r       p       x       x       c       e       x       9       8       n       x       x       c       e       x       9       8       1       x       e       2       x       8       8       x       8       8       l       n       1       x       e       2       x       8       8       x       9       2       n       x       x       c       e       x       9       8       1       x       e       2       x       8       8       x       8       8       l       n       1       n       x       e       2       x       8       8       x       8       8       n       w       h       e       r       e       1       x       c       2       x       b       7       n       n       c       r       f       u       n       c       n       n       l       l       n       n       l       r       n       e       n       e       h       e       e       f       l       e       f       n       e       h       e       l       b       h       e       l       e       f       n       r       g       h       u       b       r       e       e       f       n       e       n       n       l       n       n       r       r       e       p       e       c       v       e       l       h       e       u       p       u       f       h       e       r       e       e       w       r       x       e       h       e       p       p       n       g       f       u       n       c       n       g       e       f       n       e       b       x       g       x       x       c       e       x       9       8       p       x       x       c       e       x       9       8       q       2       x       e       2       x       8       8       x       8       8       l       3       2       r       e       e       p       z       n       g       v       e       n       r       n       n       g       e       x       n       1       u       r       g       l       l       e       r       n       e       c       n       r       e       e       e       c       r       b       e       n       e       c       3       1       w       h       c       h       c       n       u       p       u       r       b       u       n       g       x       x       c       e       x       9       8       l       r       f       r       e       c       h       p       l       e       x       h       e       n       r       g       h       f       r       w       r       w       n       z       e       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       k       l       v       e       r       g       e       n       c       e       b       e       w       e       e       n       e       c       h       g       x       x       c       e       x       9       8       n       r       e       q       u       v       l       e       n       l       n       z       e       h       e       f       l       l       w       n       g       c       r       e       n       r       p       l       r       q       x       c       e       x       9       8       x       e       2       x       8       8       x       9       2       n       c       n       c       x       1       0       x       x       1       1       1       x       x       c       1       x       x       c       x       l       g       g       c       x       x       c       e       x       9       8       x       e       2       x       8       8       x       9       2       x       l       g       p       x       x       c       e       x       9       8       q       c       3       n       1       c       1       n       1       c       1       x       e       2       x       8       8       x       8       8       l       4       x       0       c       w       h       e       r       e       q       e       n       e       h       e       r       b       u       n       h       e       l       b       l       l       h       e       l       e       f       n       e       l       n       g       c       x       x       c       e       x       9       8       h       e       c       h       u       p       u       u       n       f       g       x       x       c       e       x       9       8       l       e       r       n       n       g       h       e       r       e       e       r       e       q       u       r       e       h       e       e       n       f       w       p       r       e       e       r       1       h       e       p       l       n       e       p       r       e       e       r       x       c       e       x       9       8       n       2       h       e       r       b       u       n       q       h       e       l       b       h       e       l       e       f       n       e       h       e       b       e       p       r       e       e       r       x       c       e       x       9       8       x       e       2       x       8       8       x       9       7       q       x       e       2       x       8       8       x       9       7       r       e       e       e       r       n       e       b       x       c       e       x       9       8       x       e       2       x       8       8       x       9       7       q       x       e       2       x       8       8       x       9       7       r       g       n       r       q       x       c       e       x       9       8       4       x       c       e       x       9       8       q       l       v       e       e       q       n       4       w       e       c       n       e       r       n       l       e       r       n       n       g       p       z       n       r       e       g       f       r       w       e       f       x       q       n       p       z       e       x       c       e       x       9       8       h       e       n       w       e       f       x       x       c       e       x       9       8       n       p       z       e       q       h       e       e       w       l       e       r       n       n       g       e       p       r       e       l       e       r       n       v       e       l       p       e       r       f       r       e       u       n       l       c       n       v       e       r       g       e       n       c       e       r       x       u       n       u       b       e       r       f       e       r       n       r       e       c       h       e       e       f       n       e       n       h       e       e       x       p       e       r       e       n       3       2       1       l       e       r       n       n       g       p       l       n       e       n       h       e       c       n       w       e       e       c       r       b       e       h       w       l       e       r       n       h       e       p       r       e       e       r       x       c       e       x       9       8       f       r       p       l       n       e       w       h       e       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       r       e       f       x       e       w       e       c       p       u       e       h       e       g       r       e       n       f       h       e       l       r       q       x       c       e       x       9       8       w       r       x       c       e       x       9       8       b       h       e       c       h       n       r       u       l       e       n       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       x       x       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       5       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       1       n       x       e       2       x       8       8       x       8       8       n       w       h       e       r       e       n       l       h       e       f       r       e       r       e       p       e       n       n       h       e       r       e       e       n       h       e       e       c       n       e       r       e       p       e       n       n       h       e       p       e       c       f       c       p       e       f       h       e       f       u       n       c       n       f       x       c       f       x       9       5       n       h       e       f       r       e       r       g       v       e       n       b       c       x       0       1       g       c       x       x       c       e       x       9       8       n       l       x       1       1       g       c       x       x       c       e       x       9       8       n       r       1       x       c       x       1       0       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       x       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       n       x       x       c       e       x       9       8       6       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       n       c       1       g       c       x       x       c       e       x       9       8       g       c       x       x       c       e       x       9       8       p       p       w       h       e       r       e       g       c       x       x       c       e       x       9       8       n       l       x       e       2       x       8       8       x       8       8       l       l       n       p       x       x       c       e       x       9       8       q       c       n       g       c       x       x       c       e       x       9       8       n       r       x       e       2       x       8       8       x       8       8       l       r       n       p       x       x       c       e       x       9       8       q       c       n       e       h       l       e       n       b       e       h       e       r       e       e       r       e       h       e       n       e       n       h       e       n       w       e       h       v       e       g       c       x       x       c       e       x       9       8       n       g       c       x       x       c       e       x       9       8       n       l       g       c       x       x       c       e       x       9       8       n       r       h       e       n       h       e       g       r       e       n       c       p       u       n       n       e       q       n       6       c       n       b       e       r       e       h       e       l       e       f       n       e       n       c       r       r       e       u       n       b       u       p       n       n       e       r       h       u       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       n       r       b       c       k       p       r       p       g       n       3       2       2       l       e       r       n       n       g       l       e       f       n       e       n       w       f       x       n       g       h       e       p       r       e       e       r       x       c       e       x       9       8       w       e       h       w       h       w       l       e       r       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       w       h       c       h       c       n       r       n       e       p       z       n       p       r       b       l       e       n       r       q       x       c       e       x       9       8       x       e       2       x       8       8       x       8       0       q       c       x       q       c       1       7       c       1       h       e       r       e       w       e       p       r       p       e       r       e       h       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       w       h       c       h       l       e       e       p       z       e       f       r       e       e       n       f       c       n       v       e       r       g       e       u       p       e       r       u       l       e       f       r       q       n       v       r       n       l       b       u       n       n       g       n       r       g       n       l       b       j       e       c       v       e       f       u       n       c       n       b       e       n       z       e       g       e       r       e       p       l       c       e       b       b       u       n       n       n       e       r       v       e       n       n       e       r       u       p       p       e       r       b       u       n       f       r       h       e       l       f       u       n       c       n       r       q       x       c       e       x       9       8       c       n       b       e       b       n       e       b       j       e       n       e       n       x       e       2       x       8       0       x       9       9       n       e       q       u       l       r       q       x       c       e       x       9       8       x       e       2       x       8       8       x       9       2       n       c       x       1       0       x       x       1       1       1       x       x       c       x       l       g       p       x       x       c       e       x       9       8       q       c       n       1       c       1       x       e       2       x       8       8       x       8       8       l       x       e       2       x       8       9       x       4       x       e       2       x       8       8       x       9       2       w       h       e       r       e       x       c       e       x       b       q       c       x       1       n       n       x       c       x       1       c       1       p       x       x       c       e       x       9       8       q       c       g       c       x       x       c       e       x       9       8       x       c       f       x       8       6       q       q       x       c       c       x       8       4       x       e       2       x       8       8       x       9       2       x       c       x       x       c       e       x       b       q       x       c       c       x       8       4       c       x       l       g       x       e       2       x       8       8       x       8       8       l       x       1       0       p       x       x       c       e       x       9       8       q       x       1       1       c       x       c       e       x       b       q       x       c       c       x       8       4       c       x       8       w       e       e       f       n       e       n       c       x       1       0       p       x       x       c       e       x       9       8       q       x       1       1       1       x       x       c       x       c       x       x       c       e       x       b       q       x       c       c       x       8       4       c       x       l       g       n       1       c       1       x       c       e       x       b       q       x       c       c       x       8       4       c       x       9       x       e       2       x       8       8       x       8       8       l       h       e       n       x       c       f       x       8       6       q       q       x       c       c       x       8       4       n       u       p       p       e       r       b       u       n       f       r       r       q       x       c       e       x       9       8       w       h       c       h       h       h       e       p       r       p       e       r       h       f       r       n       q       n       q       x       c       c       x       8       4       x       c       f       x       8       6       q       q       x       c       c       x       8       4       x       e       2       x       8       9       x       5       r       q       x       c       e       x       9       8       n       x       c       f       x       8       6       q       q       r       q       x       c       e       x       9       8       u       e       h       w       e       r       e       p       n       q       c       r       r       e       p       n       n       g       h       e       h       e       r       n       h       e       n       x       c       f       x       8       6       q       q       n       u       p       p       e       r       b       u       n       f       r       r       q       x       c       e       x       9       8       n       h       e       n       e       x       e       r       n       q       1       c       h       e       n       u       c       h       h       x       c       f       x       8       6       q       1       q       x       e       2       x       8       9       x       4       r       q       x       c       e       x       9       8       w       h       c       h       p       l       e       r       q       1       x       c       e       x       9       8       x       e       2       x       8       9       x       4       r       q       x       c       e       x       9       8       5       x       0       c       c       n       e       q       u       e       n       l       w       e       c       n       n       z       e       x       c       f       x       8       6       q       q       x       c       c       x       8       4       n       e       f       r       q       x       c       e       x       9       8       f       e       r       e       n       u       r       n       g       h       r       q       x       c       e       x       9       8       x       c       f       x       8       6       q       q       x       c       c       x       8       4       e       q       x       c       c       x       8       4       q       w       e       h       v       e       q       1       r       g       n       x       c       f       x       8       6       q       q       x       e       2       x       8       8       x       8       0       q       c       x       q       c       1       1       0       c       1       w       h       c       h       l       e       n       z       n       g       h       e       l       g       r       n       g       n       e       f       n       e       b       x       c       f       x       9       5       q       q       x       c       f       x       8       6       q       q       x       x       c       e       x       b       b       x       e       2       x       8       8       x       8       8       l       w       h       e       r       e       x       c       e       x       b       b       h       e       l       g       r       n       g       e       u       l       p       l       e       r       b       e       n       g       x       c       e       x       b       b       1       n       e       h       q       c       1       x       e       2       x       8       8       x       8       8       0       1       n       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       h       e       r       n       g       0       r       b       u       n       q       c       c       1       3       3       q       c       x       e       2       x       8       8       x       9       2       1       1       1       c       1       x       e       2       x       8       8       x       8       2       x       c       f       x       9       5       q       q       x       e       2       x       8       8       x       8       2       q       c       n       c       1       x       x       c       1       x       c       e       x       b       q       c       x       n       q       c       n       1       c       1       x       f       e       h       q       c       c       x       0       w       e       h       v       e       p       n       c       x       x       c       e       x       b       q       c       x       p       c       1       p       n       c       x       c       e       x       b       q       x       x       c       1       1       c       1       2       1       1       e       q       n       1       2       h       e       u       p       e       c       h       e       e       f       r       c       1       q       c       0       p       n       q       c       n       b       e       p       l       n       l       z       e       b       h       e       u       n       f       r       p       c       l       e       r       n       n       g       f       r       e       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       f       1       k       n       h       e       r       n       n       g       g       e       l       l       r       e       e       n       h       e       f       r       e       f       u       e       h       e       e       p       r       e       e       r       x       c       e       x       9       8       f       r       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x       c       2       x       b       7       x       c       e       x       9       8       b       u       c       r       r       e       p       n       f       f       e       r       e       n       u       p       u       u       n       f       f       g       n       e       b       x       c       f       x       9       5       e       e       f       g       2       b       u       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       h       e       l       f       u       n       c       n       f       r       f       r       e       g       v       e       n       b       v       e       r       g       n       g       h       e       l       f       u       n       c       n       f       r       l       l       n       v       u       l       r       e       e       p       k       1       r       f       k       k       1       r       k       w       h       e       r       e       r       k       h       e       l       f       u       n       c       n       f       r       r       e       e       k       e       f       n       e       b       e       q       n       3       l       e       r       n       x       c       e       x       9       8       b       f       x       n       g       h       e       l       e       f       n       e       p       r       e       c       n       q       f       l       l       h       e       r       e       e       n       h       e       f       r       e       f       b       e       n       h       e       e       r       v       n       n       e       c       3       2       n       r       e       f       e       r       r       n       g       f       g       2       w       e       h       v       e       n       k       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       k       n       x       x       c       e       x       9       8       1       x       x       x       x       e       2       x       8       8       x       8       2       r       f       x       e       2       x       8       8       x       8       2       r       k       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       k       1       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       k       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       1       3       k       1       n       x       e       2       x       8       8       x       8       8       n       k       w       h       e       r       e       n       k       n       x       c       f       x       9       5       k       x       c       2       x       b       7       r       e       h       e       p       l       n       e       e       n       h       e       n       e       x       f       u       n       c       n       f       k       r       e       p       e       c       v       e       l       n       e       h       h       e       n       e       x       f       u       n       c       n       x       c       f       x       9       5       k       x       c       2       x       b       7       f       r       e       c       h       r       e       e       r       n       l       g       n       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       n       h       u       p       l       n       e       c       r       r       e       p       n       u       b       e       f       u       p       u       u       n       f       f       h       r       e       g       l       r       h       e       r       n       u       b       p       c       e       e       h       1       7       w       h       c       h       n       c       r       e       e       h       e       r       n       n       e       n       r       n       n       g       r       e       u       c       e       h       e       r       k       f       v       e       r       f       n       g       f       r       q       n       c       e       e       c       h       r       e       e       n       h       e       f       r       e       f       h       w       n       l       e       f       n       e       p       r       e       c       n       q       w       e       c       n       u       p       e       h       e       n       e       p       e       n       e       n       l       b       e       q       n       1       2       g       v       e       n       b       x       c       e       x       9       8       f       r       p       l       e       e       n       n       l       c       n       v       e       n       e       n       c       e       w       e       n       c       n       u       c       h       u       p       e       c       h       e       e       n       h       e       w       h       l       e       e       b       u       n       e       f       n       b       c       h       e       b       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       l       f       h       w       n       n       l       g       r       h       1       l       g       r       h       1       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       l       f       r       e       q       u       r       e       r       n       n       g       e       n       b       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       q       n       l       z       e       x       c       e       x       9       8       r       n       l       n       q       u       n       f       r       l       e       b       x       e       2       x       8       8       x       8       5       w       h       l       e       n       c       n       v       e       r       g       e       w       h       l       e       b       n       b       r       n       l       e       l       e       c       n       b       c       h       b       f       r       u       p       e       x       c       e       x       9       8       b       c       p       u       n       g       g       r       e       n       e       q       n       1       3       n       b       b       b       b       e       n       w       h       l       e       u       p       e       q       b       e       r       n       g       e       q       n       1       2       n       b       b       x       e       2       x       8       8       x       8       5       e       n       w       h       l       e       n       h       e       e       n       g       g       e       h       e       u       p       u       f       h       e       f       r       e       f       g       v       e       n       b       v       e       r       g       n       g       h       e       p       r       e       c       n       f       r       l       l       h       e       p       k       1       n       v       u       l       r       e       e       g       x       x       c       e       x       9       8       f       k       k       1       g       x       x       c       e       x       9       8       k       6       x       0       c       4       e       x       p       e       r       e       n       l       r       e       u       l       u       r       r       e       l       z       n       f       l       l       f       b       e       n       x       e       2       x       8       0       x       9       c       c       f       f       e       x       e       2       x       8       0       x       9       1       8       u       l       r       n       p       l       e       e       n       e       n       r       n       e       u       r       l       n       e       w       r       k       l       e       r       w       e       c       n       e       h       e       r       u       e       h       l       l       w       n       l       n       e       e       l       l       l       f       r       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       l       l       f       w       e       e       v       l       u       e       l       l       f       n       f       f       e       r       e       n       l       l       k       n       c       p       r       e       w       h       h       e       r       n       l       n       e       l       l       e       h       l       l       f       c       n       b       e       l       e       r       n       e       f       r       r       w       g       e       n       n       e       n       e       n       n       n       e       r       w       e       v       e       r       f       l       l       f       n       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       h       e       e       f       u       l       e       n       g       f       r       h       e       p       r       e       e       r       f       u       r       f       r       e       r       e       r       e       e       n       u       b       e       r       5       r       e       e       e       p       h       7       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       6       4       e       r       n       e       u       p       e       l       e       f       n       e       p       r       e       c       n       2       0       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       l       e       f       n       e       p       r       e       c       n       1       0       0       x       u       e       r       n       2       5       0       0       0       4       1       c       p       r       n       f       l       l       f       n       l       n       e       l       l       e       h       w       e       c       p       r       e       u       r       h       l       l       w       e       l       l       l       f       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       l       e       h       f       r       l       l       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x       x       c       e       x       9       8       l       n       e       r       r       n       f       r       n       f       x       e       h       e       h       u       p       u       u       n       f       x       x       c       e       x       b       8       x       c       e       x       b       8       x       w       h       e       r       e       x       c       e       x       b       8       h       e       h       c       l       u       n       f       h       e       r       n       f       r       n       r       x       x       c       e       x       9       8       w       e       u       e       3       p       p       u       l       r       l       l       e       n       6       v       e       h       u       n       g       e       n       e       n       n       u       r       l       c       e       n       e       1       h       e       p       l       e       n       h       e       e       3       e       r       e       r       e       p       r       e       e       n       e       b       n       u       e       r       c       l       e       c       r       p       r       n       h       e       g       r       u       n       r       u       h       f       r       h       e       r       e       h       e       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       h       e       e       e       r       b       u       n       r       e       l       e       h       u       n       g       e       n       e       n       l       b       e       l       r       b       u       n       n       c       e       n       e       u       c       h       p       l       n       k       n       c       l       u       r       e       p       e       c       v       e       l       h       e       l       b       e       l       r       b       u       n       f       h       e       e       3       e       r       e       x       u       r       e       r       b       u       n       u       c       h       h       e       r       n       g       r       b       u       n       h       w       n       n       f       g       1       b       f       l       l       w       n       g       7       2       7       w       e       u       e       6       e       u       r       e       e       v       l       u       e       h       e       p       e       r       f       r       n       c       e       f       l       l       e       h       w       h       c       h       c       p       u       e       h       e       v       e       r       g       e       l       r       n       c       e       b       e       w       e       e       n       h       e       p       r       e       c       e       r       n       g       r       b       u       n       n       h       e       r       e       l       r       n       g       r       b       u       n       n       c       l       u       n       g       4       n       c       e       e       u       r       e       k       l       e       u       c       l       e       n       x       c       f       x       8       6       r       e       n       e       n       q       u       r       e       x       c       f       x       8       7       2       n       w       l       r       e       u       r       e       f       e       l       n       e       r       e       c       n       w       e       e       v       l       u       e       u       r       h       l       l       w       e       l       l       l       f       n       h       e       e       3       e       n       c       p       r       e       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       l       e       h       h       e       r       e       u       l       f       l       l       f       n       h       e       c       p       e       r       r       e       u       r       z       e       n       b       l       e       1       f       r       v       e       w       e       q       u       e       h       e       r       e       u       l       r       e       p       r       e       n       2       7       h       e       c       e       f       2       7       n       p       u       b       l       c       l       v       l       b       l       e       f       r       h       e       r       e       u       l       f       h       e       h       e       r       w       w       e       r       u       n       c       e       h       h       e       u       h       r       h       e       v       l       b       l       e       n       l       l       c       e       f       l       l       w       n       g       2       7       6       w       e       p       l       e       c       h       e       n       1       0       f       x       e       f       l       n       n       r       e       n       f       l       c       r       v       l       n       w       h       c       h       r       e       p       r       e       e       n       h       e       r       e       u       l       b       x       e       2       x       8       0       x       9       c       e       n       x       c       2       x       b       1       n       r       e       v       n       x       e       2       x       8       0       x       9       n       e       r       l       e       h       w       r       n       n       g       n       e       n       g       g       e       v       e       c       n       b       e       e       e       n       f       r       b       l       e       1       l       l       f       p       e       r       f       r       b       e       n       l       l       f       h       e       x       e       u       r       e       b       l       e       1       c       p       r       n       r       e       u       l       n       h       r       e       e       l       l       e       6       x       e       2       x       8       0       x       9       c       x       e       2       x       8       6       x       9       1       x       e       2       x       8       0       x       9       n       x       e       2       x       8       0       x       9       c       x       e       2       x       8       6       x       9       3       x       e       2       x       8       0       x       9       n       c       e       h       e       l       r       g       e       r       n       h       e       l       l       e       r       h       e       b       e       e       r       r       e       p       e       c       v       e       l       e       e       h       k       l       x       e       2       x       8       6       x       9       3       e       u       c       l       e       n       x       e       2       x       8       6       x       9       3       x       c       f       x       8       6       r       e       n       e       n       x       e       2       x       8       6       x       9       3       q       u       r       e       x       c       f       x       8       7       2       x       e       2       x       8       6       x       9       3       f       e       l       x       e       2       x       8       6       x       9       1       n       e       r       e       c       n       x       e       2       x       8       6       x       9       1       v       e       l       l       f       u       r       l       l       g       b       2       7       l       l       g       b       2       7       l       v       r       7       b       f       g       l       l       6       l       l       1       1       0       0       7       3       x       c       2       x       b       1       0       0       0       5       0       0       8       6       x       c       2       x       b       1       0       0       0       4       0       0       9       0       x       c       2       x       b       1       0       0       0       4       0       0       9       2       x       c       2       x       b       1       0       0       0       5       0       0       9       9       x       c       2       x       b       1       0       0       0       4       0       1       2       9       x       c       2       x       b       1       0       0       0       7       0       1       3       3       x       c       2       x       b       1       0       0       0       3       0       1       5       5       x       c       2       x       b       1       0       0       0       3       0       1       5       9       x       c       2       x       b       1       0       0       0       3       0       1       5       8       x       c       2       x       b       1       0       0       0       4       0       1       6       7       x       c       2       x       b       1       0       0       0       4       0       1       8       7       x       c       2       x       b       1       0       0       0       4       0       1       3       0       x       c       2       x       b       1       0       0       0       3       0       1       5       2       x       c       2       x       b       1       0       0       0       3       0       1       5       5       x       c       2       x       b       1       0       0       0       3       0       1       5       6       x       c       2       x       b       1       0       0       0       4       0       1       6       4       x       c       2       x       b       1       0       0       0       3       0       1       8       3       x       c       2       x       b       1       0       0       0       4       0       0       7       0       x       c       2       x       b       1       0       0       0       4       0       0       8       4       x       c       2       x       b       1       0       0       0       3       0       0       8       8       x       c       2       x       b       1       0       0       0       3       0       0       8       8       x       c       2       x       b       1       0       0       0       4       0       0       9       6       x       c       2       x       b       1       0       0       0       4       0       1       2       0       x       c       2       x       b       1       0       0       0       5       0       9       8       1       x       c       2       x       b       1       0       0       0       1       0       9       7       8       x       c       2       x       b       1       0       0       0       1       0       9       7       7       x       c       2       x       b       1       0       0       0       1       0       9       7       7       x       c       2       x       b       1       0       0       0       1       0       9       7       4       x       c       2       x       b       1       0       0       0       1       0       9       6       7       x       c       2       x       b       1       0       0       0       1       0       8       7       0       x       c       2       x       b       1       0       0       0       3       0       8       4       8       x       c       2       x       b       1       0       0       0       3       0       8       4       5       x       c       2       x       b       1       0       0       0       3       0       8       4       4       x       c       2       x       b       1       0       0       0       4       0       8       3       6       x       c       2       x       b       1       0       0       0       3       0       8       1       7       x       c       2       x       b       1       0       0       0       4       l       l       f       u       r       l       v       r       7       b       f       g       l       l       6       l       l       1       1       0       2       2       8       x       c       2       x       b       1       0       0       0       6       0       2       4       5       x       c       2       x       b       1       0       0       1       9       0       2       3       1       x       c       2       x       b       1       0       0       2       1       0       2       3       9       x       c       2       x       b       1       0       0       1       8       0       0       8       5       x       c       2       x       b       1       0       0       0       2       0       0       9       9       x       c       2       x       b       1       0       0       0       5       0       0       7       6       x       c       2       x       b       1       0       0       0       6       0       0       8       9       x       c       2       x       b       1       0       0       0       6       0       2       1       2       x       c       2       x       b       1       0       0       0       2       0       2       2       9       x       c       2       x       b       1       0       0       1       5       0       2       3       1       x       c       2       x       b       1       0       0       1       2       0       2       5       3       x       c       2       x       b       1       0       0       0       9       0       1       7       9       x       c       2       x       b       1       0       0       0       4       0       1       8       9       x       c       2       x       b       1       0       0       2       1       0       2       1       1       x       c       2       x       b       1       0       0       1       8       0       2       0       5       x       c       2       x       b       1       0       0       1       2       0       9       4       8       x       c       2       x       b       1       0       0       0       1       0       9       4       0       x       c       2       x       b       1       0       0       0       6       0       9       3       8       x       c       2       x       b       1       0       0       0       8       0       9       4       4       x       c       2       x       b       1       0       0       0       3       0       7       8       8       x       c       2       x       b       1       0       0       0       2       0       7       7       1       x       c       2       x       b       1       0       0       1       5       0       7       6       9       x       c       2       x       b       1       0       0       1       2       0       7       4       7       x       c       2       x       b       1       0       0       0       9       l       l       f       u       r       l       v       r       7       b       f       g       l       l       6       l       l       1       1       0       5       3       4       x       c       2       x       b       1       0       0       1       3       0       8       5       2       x       c       2       x       b       1       0       0       2       3       0       8       5       6       x       c       2       x       b       1       0       0       6       1       0       8       7       9       x       c       2       x       b       1       0       0       2       3       0       3       1       7       x       c       2       x       b       1       0       0       1       4       0       5       1       1       x       c       2       x       b       1       0       0       2       1       0       4       7       5       x       c       2       x       b       1       0       0       2       9       0       4       5       8       x       c       2       x       b       1       0       0       1       4       0       3       3       6       x       c       2       x       b       1       0       0       1       0       0       4       9       2       x       c       2       x       b       1       0       0       1       6       0       5       0       8       x       c       2       x       b       1       0       0       2       6       0       5       3       9       x       c       2       x       b       1       0       0       1       1       0       4       4       8       x       c       2       x       b       1       0       0       1       7       0       5       9       5       x       c       2       x       b       1       0       0       2       6       0       7       1       6       x       c       2       x       b       1       0       0       4       1       0       7       9       2       x       c       2       x       b       1       0       0       1       9       0       8       2       4       x       c       2       x       b       1       0       0       0       8       0       8       1       3       x       c       2       x       b       1       0       0       0       8       0       7       2       2       x       c       2       x       b       1       0       0       2       1       0       6       8       6       x       c       2       x       b       1       0       0       0       9       0       6       6       4       x       c       2       x       b       1       0       0       1       0       0       5       0       9       x       c       2       x       b       1       0       0       1       6       0       4       9       2       x       c       2       x       b       1       0       0       2       6       0       4       6       1       x       c       2       x       b       1       0       0       1       1       h       u       n       g       e       n       e       n       u       r       l       c       e       n       e       4       2       e       v       l       u       n       f       l       l       f       n       f       c       l       g       e       e       n       n       e       l       e       r       u       r       e       8       1       1       2       8       1       5       5       g       e       e       n       f       r       u       l       e       l       l       p       r       b       l       e       w       e       c       n       u       c       f       c       l       g       e       e       n       e       x       p       e       r       e       n       n       r       p       h       2       4       w       h       c       h       c       n       n       r       e       h       n       5       0       0       0       0       f       c       l       g       e       f       r       b       u       1       3       0       0       0       p       e       p       l       e       f       f       f       e       r       e       n       r       c       e       e       c       h       f       c       l       g       e       n       n       e       w       h       c       h       r       n       l       g       c       l       g       e       g       e       n       e       r       e       n       g       e       r       b       u       n       f       r       e       c       h       f       c       e       g       e       w       e       f       l       l       w       h       e       e       r       e       g       u       e       n       8       2       8       5       w       h       c       h       u       e       g       u       n       r       b       u       n       w       h       e       e       n       h       e       c       h       r       n       l       g       c       l       g       e       f       h       e       f       c       e       g       e       f       g       1       h       e       p       r       e       c       e       g       e       f       r       f       c       e       g       e       p       l       h       e       g       e       h       v       n       g       h       e       h       g       h       e       p       r       b       b       l       n       h       e       p       r       e       c       e       1       w       e       w       n       l       h       e       e       e       f       r       h       p       c       e       e       u       e       u       c       n       p       e       p       l       e       x       g       e       n       g       l       l       n       e       x       h       7       x       0       c       l       b       e       l       r       b       u       n       h       e       p       e       r       f       r       n       c       e       f       g       e       e       n       e       v       l       u       e       b       h       e       e       n       b       l       u       e       e       r       r       r       e       b       e       w       e       e       n       p       r       e       c       e       g       e       n       c       h       r       n       l       g       c       l       g       e       h       e       c       u       r       r       e       n       e       f       h       e       r       r       e       u       l       n       r       p       h       b       n       b       f       n       e       u       n       n       g       l       l       5       n       v       g       g       f       c       e       2       3       w       e       l       b       u       l       l       l       f       n       v       g       g       f       c       e       b       r       e       p       l       c       n       g       h       e       f       x       l       e       r       n       v       g       g       n       e       b       l       l       f       f       l       l       w       n       g       5       w       e       n       r       1       0       e       n       f       l       c       r       v       l       n       n       h       e       r       e       u       l       r       e       u       r       z       e       n       b       l       e       2       w       h       c       h       h       w       l       l       f       c       h       e       v       e       h       e       e       f       h       e       r       p       e       r       f       r       n       c       e       n       r       p       h       n       e       h       h       e       g       n       f       c       n       p       e       r       f       r       n       c       e       g       n       b       e       w       e       e       n       e       e       p       l       l       e       l       l       l       n       l       l       f       n       n       n       e       e       p       l       l       e       l       l       l       c       p       n       n       b       f       g       l       l       n       h       e       u       p       e       r       r       f       l       l       f       c       p       r       e       w       h       l       l       v       e       r       f       e       h       e       e       f       f       e       c       v       e       n       e       f       e       n       e       n       l       e       r       n       n       g       n       u       r       r       e       e       b       e       e       l       f       r       l       l       r       e       p       e       c       v       e       l       b       l       e       2       e       f       g       e       e       n       c       p       r       n       n       r       p       h       2       4       e       h       l       l       1       1       c       p       n       n       1       1       b       f       g       l       l       6       l       l       v       g       g       f       c       e       5       l       l       f       v       g       g       f       c       e       u       r       e       5       6       7       x       c       2       x       b       1       0       1       5       4       8       7       x       c       2       x       b       1       0       3       1       3       9       4       x       c       2       x       b       1       0       0       5       2       4       2       x       c       2       x       b       1       0       0       1       2       2       4       x       c       2       x       b       1       0       0       2       h       e       r       b       u       n       f       g       e       n       e       r       n       e       h       n       c       v       e       r       u       n       b       l       n       c       e       n       r       p       h       n       g       e       e       n       e       h       1       3       1       4       1       5       r       e       e       v       l       u       e       n       u       b       e       f       r       p       h       c       l       l       e       r       p       h       u       b       f       r       h       r       w       h       c       h       c       n       f       2       0       1       6       0       e       l       e       c       e       f       c       l       g       e       v       h       e       n       f       l       u       e       n       c       e       f       u       n       b       l       n       c       e       r       b       u       n       h       e       b       e       p       e       r       f       r       n       c       e       r       e       p       r       e       n       r       p       h       u       b       g       v       e       n       b       2       l       l       1       5       e       p       e       n       e       n       l       l       e       h       2       l       l       u       e       h       e       u       p       u       f       h       e       x       e       2       x       8       0       x       9       c       f       c       7       x       e       2       x       8       0       x       9       l       e       r       n       l       e       x       n       e       2       1       h       e       f       c       e       g       e       f       e       u       r       e       h       e       r       e       w       e       n       e       g       r       e       l       l       f       w       h       l       e       x       n       e       f       l       l       w       n       g       h       e       e       x       p       e       r       e       n       e       n       g       u       e       n       2       l       l       w       e       e       v       l       u       e       u       r       l       l       f       n       h       e       c       p       e       r       n       c       l       u       n       g       b       h       l       l       n       l       l       b       e       e       h       u       n       e       r       x       f       f       e       r       e       n       r       n       n       g       e       r       1       0       6       0       l       l       f       h       e       c       p       e       r       r       e       r       n       e       n       h       e       e       e       e       p       f       e       u       r       e       u       e       b       2       l       l       c       n       b       e       e       e       n       f       r       b       l       e       3       u       r       l       l       f       g       n       f       c       n       l       u       p       e       r       f       r       h       e       r       f       r       l       l       r       n       n       g       e       r       n       e       h       h       e       g       e       n       e       r       e       g       e       r       f       g       u       r       e       3       e       f       g       e       e       n       c       p       r       n       n       b       u       n       r       e       u       n       l       r       b       u       n       r       p       h       u       b       n       h       e       l       b       e       l       r       b       u       n       u       e       n       r       n       n       g       e       r       e       h       e       c       4       1       r       e       x       u       r       e       r       b       u       n       1       0       2       0       3       0       4       0       5       0       6       0       h       e       p       r       p       e       e       h       l       l       f       c       h       e       v       e       2       2       4       9       0       8       1       4       7       6       1       6       4       6       5       0       7       4       5       5       5       3       4       4       6       9       0       4       4       0       6       1       h       e       e       f       h       e       r       r       e       u       l       n       b       h       f       l       r       r       1       2       4       7       5       0       1       4       6       1       1       2       4       5       1       3       1       4       4       2       7       3       4       3       5       0       0       4       2       9       4       9       l       l       9       4       1       7       9       1       4       1       6       8       3       4       1       2       2       8       4       1       1       0       7       4       1       0       2       4       4       0       9       0       2       h       e       w       h       c       h       v       e       r       f       e       h       u       r       e       l       2       l       l       1       5       4       1       0       8       0       3       9       8       5       7       3       9       2       0       4       3       8       7       1       2       3       8       5       6       0       3       8       3       8       5       h       h       e       b       l       e       l       n       g       e       n       e       r       l       l       l       f       u       r       3       8       4       9       5       3       6       2       2       0       3       3       9       9       1       3       2       4       0       1       3       1       9       1       7       3       1       2       2       4       f       r       f       l       b       e       l       r       b       u       n       4       3       e       c       p       l       e       x       l       e       h       n       b       b       e       h       e       r       e       e       e       p       h       n       h       e       b       c       h       z       e       r       e       p       e       c       v       e       l       e       c       h       r       e       e       h       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       p       l       n       e       n       2       h       x       e       2       x       8       8       x       9       2       1       l       e       f       n       e       l       e       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       f       r       n       e       r       e       e       n       n       e       p       l       e       h       e       c       p       l       e       x       f       f       r       w       r       p       n       b       c       k       w       r       p       r       e       1       x       c       3       x       9       7       c       x       c       3       x       9       7       c       n       1       x       c       3       x       9       7       c       x       c       3       x       9       7       c       x       c       3       x       9       7       c       r       e       p       e       c       v       e       l       f       r       k       r       e       e       n       n       b       b       c       h       e       h       e       c       p       l       e       x       f       f       r       w       r       n       b       c       k       w       r       p       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       h       e       c       p       l       e       x       f       n       e       r       n       u       p       e       l       e       f       n       e       r       e       n       b       x       c       3       x       9       7       b       x       c       3       x       9       7       k       x       c       3       x       9       7       c       x       c       3       x       9       7       1       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       h       u       h       e       c       p       l       e       x       f       r       h       e       r       n       n       g       p       r       c       e       u       r       e       n       e       e       p       c       h       n       b       b       c       h       e       n       h       e       e       n       g       p       r       c       e       u       r       e       n       e       p       l       e       r       e       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       n       x       c       3       x       9       7       c       x       c       3       x       9       7       k       r       e       p       e       c       v       e       l       l       l       f       r       e       e       f       f       c       e       n       n       r       p       h       u       b       1       2       6       3       6       r       n       n       g       g       e       8       4       2       4       e       n       g       g       e       u       r       e       l       n       l       k       e       5       2       5       0       f       r       r       n       n       g       2       5       0       0       0       e       r       n       n       8       f       r       e       n       g       l       l       8       4       2       4       g       e       4       4       p       r       e       e       r       c       u       n       n       w       w       e       c       u       h       e       n       f       l       u       e       n       c       e       f       p       r       e       e       r       e       n       g       n       p       e       r       f       r       n       c       e       w       e       r       e       p       r       h       e       r       e       u       l       f       r       n       g       p       r       e       c       n       n       v       e       e       u       r       e       b       k       l       n       g       e       e       n       n       r       p       h       u       b       w       h       6       0       r       n       n       g       e       r       e       u       r       e       b       e       f       r       f       f       e       r       e       n       p       r       e       e       r       e       n       g       n       h       e       c       n       r       e       e       n       u       b       e       r       f       r       e       n       e       n       e       b       l       e       e       l       n       e       c       e       r       n       v       e       g       e       h       w       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       h       e       r       e       e       n       u       b       e       r       u       e       n       f       r       e       n       e       h       w       e       c       u       e       n       e       c       2       h       e       e       n       e       b       l       e       r       e       g       l       e       r       n       f       r       e       p       r       p       e       n       n       f       2       0       f       f       e       r       e       n       f       r       u       r       h       e       r       e       f       r       e       n       e       c       e       r       e       e       w       h       c       h       e       n       e       b       l       e       r       e       g       b       e       e       r       l       e       r       n       f       r       e       w       r       h       e       n       w       e       r       e       p       l       c       e       u       r       e       n       e       b       l       e       r       e       g       n       l       l       f       b       h       e       n       e       u       e       n       n       f       n       n       e       h       e       h       n       f       l       l       h       e       c       r       r       e       p       n       n       g       h       l       l       w       e       l       n       e       b       n       f       l       l       w       e       f       x       h       e       r       p       r       e       e       r       e       r       e       e       e       p       h       n       8       x       0       c       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       h       e       e       f       u       l       e       n       g       h       w       n       n       f       g       4       u       r       e       n       e       b       l       e       r       e       g       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       h       e       n       e       u       e       n       n       f       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       b       e       r       v       e       f       r       f       g       4       h       e       p       e       r       f       r       n       c       e       f       l       l       f       c       n       b       e       p       r       v       e       b       u       n       g       r       e       r       e       e       b       u       h       e       p       r       v       e       e       n       b       e       c       e       n       c       r       e       n       g       l       l       l       e       r       n       l       l       e       r       h       e       r       e       f       r       e       u       n       g       u       c       h       l       r       g       e       r       e       n       e       b       l       e       e       n       e       l       b       g       p       r       v       e       e       n       n       v       e       h       e       n       u       b       e       r       f       r       e       e       k       1       0       0       k       l       0       0       7       0       v       k       2       0       k       l       0       0       7       1       n       e       h       n       l       l       r       n       f       r       e       b       e       e       h       u       e       l       r       g       e       n       u       b       e       r       f       r       e       e       e       g       h       n       e       l       2       5       b       n       e       v       e       r       g       p       e       e       n       r       e       u       l       f       r       e       p       h       g       e       b       n       l       3       e       c       n       r       e       e       r       e       e       e       p       h       r       e       e       e       p       h       n       h       e       r       p       r       n       p       r       e       e       r       f       r       e       c       n       r       e       e       n       l       l       f       h       e       r       e       n       p       l       c       c       n       r       n       b       e       w       e       e       n       r       e       e       e       p       h       h       n       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       x       c       f       x       8       4       x       c       f       x       8       4       x       e       2       x       8       9       x       5       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       c       u       h       e       n       f       l       u       e       n       c       e       f       r       e       e       e       p       h       h       e       p       e       r       f       r       n       c       e       f       l       l       f       w       e       e       x       c       f       x       8       4       2       h       x       e       2       x       8       8       x       9       2       1       n       f       x       r       e       e       n       u       b       e       r       k       1       n       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       r       e       e       e       p       h       h       w       n       n       f       g       4       b       w       e       e       e       h       h       e       p       e       r       f       r       n       c       e       f       r       p       r       v       e       h       e       n       e       c       r       e       e       w       h       h       e       n       c       r       e       e       f       h       e       r       e       e       e       p       h       h       e       r       e       n       h       e       r       e       e       e       p       h       n       c       r       e       e       h       e       e       n       n       f       l       e       r       n       e       f       e       u       r       e       n       c       r       e       e       e       x       p       n       e       n       l       l       w       h       c       h       g       r       e       l       n       c       r       e       e       h       e       r       n       n       g       f       f       c       u       l       u       n       g       u       c       h       l       r       g       e       r       e       p       h       l       e       b       p       e       r       f       r       n       c       e       n       v       e       r       e       e       e       p       h       h       1       8       k       l       0       1       1       6       2       v       h       9       k       l       0       0       8       3       1       f       g       u       r       e       4       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       f       g       e       e       n       n       r       p       h       u       b       n       r       n       g       p       r       e       c       n       n       v       e       b       v       r       n       g       r       e       e       n       u       b       e       r       n       b       r       e       e       e       p       h       u       r       p       p       r       c       h       l       l       f       l       l       f       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       f       n       f       l       l       n       f       l       l       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       5       c       n       c       l       u       n       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       h       e       f       r       e       n       f       u       n       h       h       e       l       e       f       n       e       p       r       e       c       n       c       n       b       e       p       z       e       v       v       r       n       l       b       u       n       n       g       w       h       c       h       e       n       b       l       e       l       l       h       e       r       e       e       n       h       e       f       e       u       r       e       h       e       u       e       b       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       e       x       p       e       r       e       n       l       r       e       u       l       h       w       e       h       e       u       p       e       r       r       f       u       r       l       g       r       h       f       r       e       v       e       r       l       l       l       k       n       r       e       l       e       c       p       u       e       r       v       n       p       p       l       c       n       n       v       e       r       f       e       u       r       e       l       h       h       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       c       k       n       w       l       e       g       e       e       n       h       w       r       k       w       u       p       p       r       e       n       p       r       b       h       e       n       n       l       n       u       r       l       c       e       n       c       e       f       u       n       n       f       c       h       n       n       6       1       6       7       2       3       3       6       n       p       r       b       x       e       2       x       8       0       x       9       c       c       h       e       n       g       u       n       g       x       e       2       x       8       0       x       9       p       r       j       e       c       u       p       p       r       e       b       h       n       g       h       u       n       c       p       l       e       u       c       n       c       n       n       h       n       g       h       e       u       c       n       e       v       e       l       p       e       n       f       u       n       n       n       1       5       c       g       4       3       n       n       p       r       b       n       r       n       0       0       0       1       4       1       5       1       2       3       5       6       r       e       f       e       r       e       n       c       e       1       n       g       e       n       h       p       e       q       u       n       z       n       n       r       e       c       g       n       n       w       h       r       n       z       e       r       e       e       n       e       u       r       l       c       p       u       n       9       7       1       5       4       5       x       e       2       x       8       0       x       9       3       1       5       8       8       1       9       9       7       2       l       b       e       r       g       e       r       p       e       r       n       v       j       p       e       r       x       u       e       n       r       p       p       p       r       c       h       n       u       r       l       l       n       g       u       g       e       p       r       c       e       n       g       c       p       u       n       l       l       n       g       u       c       2       2       1       3       9       x       e       2       x       8       0       x       9       3       7       1       1       9       9       6       3       l       b       r       e       n       r       n       f       r       e       c       h       n       e       l       e       r       n       n       g       4       5       1       5       x       e       2       x       8       0       x       9       3       3       2       2       0       0       1       4       c       r       n       n       j       h       n       e       c       n       f       r       e       f       r       c       p       u       e       r       v       n       n       e       c       l       g       e       n       l       p       r       n       g       e       r       2       0       1       3       5       b       b       g       c       x       n       g       c       w       x       e       j       w       u       n       x       g       e       n       g       e       e       p       l       b       e       l       r       b       u       n       l       e       r       n       n       g       w       h       l       b       e       l       b       g       u       r       x       v       1       6       1       1       0       1       7       3       1       2       0       1       7       6       x       g       e       n       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       e       e       e       r       n       k       n       w       l       e       n       g       2       8       7       1       7       3       4       x       e       2       x       8       0       x       9       3       1       7       4       8       2       0       1       6       9       x       0       c       7       x       g       e       n       g       n       p       h       u       p       r       e       r       e       l       e       e       p       r       e       c       n       f       c       r       w       p       n       n       n       v       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       n       p       r       j       c       p       g       e       3       5       1       1       x       e       2       x       8       0       x       9       3       3       5       1       7       2       0       1       5       8       x       g       e       n       g       k       h       l       e       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       n       p       r       c       2       0       1       0       9       x       g       e       n       g       q       w       n       g       n       x       f       c       l       g       e       e       n       b       p       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       n       p       r       c       c       p       r       p       g       e       4       4       6       5       x       e       2       x       8       0       x       9       3       4       4       7       0       2       0       1       4       1       0       x       g       e       n       g       n       x       h       e       p       e       e       n       b       e       n       u       l       v       r       e       l       b       e       l       r       b       u       n       n       p       r       c       c       v       p       r       p       g       e       1       8       3       7       x       e       2       x       8       0       x       9       3       1       8       4       2       2       0       1       4       1       1       x       g       e       n       g       c       n       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       e       e       e       r       n       p       e       r       n       n       l       c       h       n       e       l       l       3       5       1       0       2       4       0       1       x       e       2       x       8       0       x       9       3       2       4       1       2       2       0       1       3       1       2       g       g       u       f       u       c       r       e       r       n       h       u       n       g       g       e       b       e       h       u       n       g       e       e       n       b       n       f       l       l       e       r       n       n       g       n       l       c       l       l       j       u       e       r       b       u       r       e       g       r       e       n       e       e       e       r       n       g       e       p       r       c       e       n       g       1       7       7       1       1       7       8       x       e       2       x       8       0       x       9       3       1       1       8       8       2       0       0       8       1       3       g       g       u       n       g       u       h       u       n       g       e       e       n       w       h       h       e       n       f       l       u       e       n       c       e       c       r       r       c       e       n       g       e       n       e       r       n       c       v       p       r       w       r       k       h       p       p       g       e       7       1       x       e       2       x       8       0       x       9       3       7       8       2       0       1       0       1       4       g       g       u       n       c       z       h       n       g       u       n       c       r       p       p       u       l       n       g       e       e       n       n       p       r       c       c       v       p       r       p       g       e       4       2       5       7       x       e       2       x       8       0       x       9       3       4       2       6       3       2       0       1       4       1       5       z       h       e       x       l       z       z       h       n       g       f       w       u       x       g       e       n       g       z       h       n       g       h       n       g       n       z       h       u       n       g       e       p       e       n       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       e       e       e       r       n       n       g       e       p       r       c       e       n       g       2       0       1       7       1       6       k       h       r       n       e       c       n       f       r       e       n       p       r       c       c       r       p       g       e       2       7       8       x       e       2       x       8       0       x       9       3       2       8       2       1       9       9       5       1       7       k       h       h       e       r       n       u       b       p       c       e       e       h       f       r       c       n       r       u       c       n       g       e       c       n       f       r       e       e       e       e       r       n       p       e       r       n       n       l       c       h       n       e       l       l       2       0       8       8       3       2       x       e       2       x       8       0       x       9       3       8       4       4       1       9       9       8       1       8       j       e       h       e       l       h       e       r       j       n       h       u       e       k       r       e       v       j       l       n       g       r       g       r       h       c       k       g       u       r       r       n       r       r       e       l       l       c       f       f       e       c       n       v       l       u       n       l       r       c       h       e       c       u       r       e       f       r       f       f       e       u       r       e       e       b       e       n       g       r       x       v       p       r       e       p       r       n       r       x       v       1       4       0       8       5       0       9       3       2       0       1       4       1       9       j       r       n       z       g       h       h       r       n       j       k       k       l       n       l       k       u       l       n       n       r       u       c       n       v       r       n       l       e       h       f       r       g       r       p       h       c       l       e       l       c       h       n       e       l       e       r       n       n       g       3       7       2       1       8       3       x       e       2       x       8       0       x       9       3       2       3       3       1       9       9       9       2       0       p       k       n       c       h       e       e       r       f       e       r       u       c       r       n       n       r       b       u       l       x       c       3       x       b       2       e       e       p       n       e       u       r       l       e       c       n       f       r       e       n       p       r       c       c       c       v       p       g       e       1       4       6       7       x       e       2       x       8       0       x       9       3       1       4       7       5       2       0       1       5       2       1       k       r       z       h       e       v       k       u       k       e       v       e       r       n       g       e       h       n       n       g       e       n       e       c       l       f       c       n       w       h       e       e       p       c       n       v       l       u       n       l       n       e       u       r       l       n       e       w       r       k       n       p       r       c       n       p       p       g       e       1       1       0       6       x       e       2       x       8       0       x       9       3       1       1       1       4       2       0       1       2       2       2       l       n       c       r       g       n       v       n       c       c       h       r       u       l       u       c       p       r       n       g       f       f       e       r       e       n       c       l       f       e       r       f       r       u       c       g       e       e       n       e       e       e       r       n       n       c       b       e       r       n       e       c       3       4       1       6       2       1       x       e       2       x       8       0       x       9       3       6       2       8       2       0       0       4       2       3       p       r       k       h       v       e       l       n       z       e       r       n       e       e       p       f       c       e       r       e       c       g       n       n       n       p       r       c       b       v       c       p       g       e       4       1       1       x       e       2       x       8       0       x       9       3       4       1       1       2       2       0       1       5       2       4       k       r       c       n       e       k       n       e       f       e       r       p       h       l       n       g       u       n       l       g       e       b       e       f       n       r       l       u       l       g       e       p       r       g       r       e       n       n       p       r       c       f       g       p       g       e       3       4       1       x       e       2       x       8       0       x       9       3       3       4       5       2       0       0       6       2       5       j       h       n       w       f       z       g       b       b       n       c       k       h       r       p       f       n       c       c       h       r       r       e       k       p       n       n       b       l       k       e       r       e       l       e       h       u       n       p       e       r       e       c       g       n       n       n       p       r       f       r       n       g       l       e       e       p       h       g       e       n       p       r       c       c       v       p       r       p       g       e       1       2       9       7       x       e       2       x       8       0       x       9       3       1       3       0       4       2       0       1       1       2       6       g       u       k       n       k       k       u       l       l       b       e       l       c       l       f       c       n       n       v       e       r       v       e       w       n       e       r       n       n       l       j       u       r       n       l       f       w       r       e       h       u       n       g       n       n       n       g       3       3       1       x       e       2       x       8       0       x       9       3       1       3       2       0       0       7       2       7       c       x       n       g       x       g       e       n       g       n       h       x       u       e       l       g       c       b       n       g       r       e       g       r       e       n       f       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       n       p       r       c       c       v       p       r       p       g       e       4       4       8       9       x       e       2       x       8       0       x       9       3       4       4       9       7       2       0       1       6       2       8       x       n       g       x       g       e       n       g       n       z       h       u       p       r       c       n       n       l       e       n       e       r       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       n       p       r       c       j       c       p       g       e       2       2       5       9       x       e       2       x       8       0       x       9       3       2       2       6       5       2       0       1       6       2       9       l       u       l       l       e       n       r       n       g       r       j       n       h       e       c       n       c       v       e       c       n       v       e       x       p       r       c       e       u       r       e       n       e       u       r       l       c       p       u       n       1       5       4       9       1       5       x       e       2       x       8       0       x       9       3       9       3       6       2       0       0       3       3       0       z       h       u       h       x       u       e       n       x       g       e       n       g       e       n       r       b       u       n       r       e       c       g       n       n       f       r       f       c       l       e       x       p       r       e       n       n       p       r       c       p       g       e       1       2       4       7       x       e       2       x       8       0       x       9       3       1       2       5       0       2       0       1       5       1       0       x       0       c   ']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.18\n",
      "XGBoost Accuracy on Test set -> 0.34\n",
      "RandomForest Accuracy on Test set -> 0.32\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_LOW_STM AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'l b e l d r b u n l e r n n g f r e w e s h e n 1 2 k z h 1 y l u g u 1 a l n y u l l e 2 k e l b r r f s p e c l f b e r o p c n o p c l a c c e n e w r k s h n g h i n u e f r a v n c e c u n c n n d s c e n c e s c h l f c u n c n n i n f r n e n g n e e r n g s h n g h u n v e r 2 d e p r e n f c p u e r s c e n c e j h n h p k n u n v e r r x v 1 7 0 2 0 6 0 8 6 v 4 c l g 1 6 o c 2 0 1 7 1 h e n w e 1 2 3 1 z h k 1 2 0 6 g l l u n 0 l n l u l l e g l c a b r c l b e l r b u n l e r n n g l d l g e n e r l l e r n n g f r e w r k w h c h g n n n n c e r b u n v e r e f l b e l r h e r h n n g l e l b e l r u l p l e l b e l c u r r e n l d l e h h v e e h e r r e r c e u p n n h e e x p r e n f r f h e l b e l r b u n r l n n r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r t h p p e r p r e e n l b e l r b u n l e r n n g f r e l d l f n v e l l b e l r b u n l e r n n g l g r h b e n f f e r e n b l e e c n r e e w h c h h v e e v e r l v n g e 1 d e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f l e f n e p r e c n 2 t h e l e r n n g f f f e r e n b l e e c n r e e c n b e c b n e w h r e p r e e n n l e r n n g w e e f n e r b u n b e l f u n c n f r f r e e n b l n g l l h e r e e b e l e r n e j n l n h w h n u p e f u n c n f r l e f n e p r e c n w h c h g u r n e e r c e c r e e f h e l f u n c n c n b e e r v e b v r n l b u n n g t h e e f f e c v e n e f h e p r p e l d l f v e r f e n e v e r l l d l k n c p u e r v n p p l c n h w n g g n f c n p r v e e n h e e f h e r l d l e h 1 i n r u c n l b e l r b u n l e r n n g l d l 6 1 1 l e r n n g f r e w r k e l w h p r b l e f l b e l b g u u n l k e n g l e l b e l l e r n n g s l l n u l l b e l l e r n n g m l l 2 6 w h c h u e n n n c e g n e n g l e l b e l r u l p l e l b e l l d l l e r n n g h e r e l v e p r n c e f e c h l b e l n v l v e n h e e c r p n f n n n c e e r b u n v e r h e e f l b e l s u c h l e r n n g r e g u b l e f r n r e l w r l p r b l e w h c h h v e l b e l b g u a n e x p l e f c l g e e n 8 e v e n h u n c n n p r e c h e p r e c e g e f r n g l e f c l g e t h e h h e p e r n p r b b l n n e g e g r u p n l e l k e l b e n n h e r h e n c e r e n u r l g n r b u n f g e l b e l e c h f c l g e f g 1 n e f u n g n g l e g e l b e l a n h e r e x p l e v e r n g p r e c n 7 m n f u v e r e v e w w e b e u c h n e f l x i m d b n d u b n p r v e c r w p n n f r e c h v e p e c f e b h e r b u n f r n g c l l e c e f r h e r u e r f g 1 b i f e c u l p r e c e l p r e c u c h r n g r b u n f r e v e r v e b e f r e r e l e e v e p r u c e r c n r e u c e h e r n v e e n r k n h e u e n c e c n b e e r c h e w h c h v e w c h m n l d l e h u e h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 n l e r n b p z n g n e n e r g f u n c n b e n h e e l 8 1 1 2 8 6 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r e g h f f c u l n r e p r e e n n g x u r e r b u n s e h e r l d l e h e x e n h e e x n g l e r n n g l g r h e g b b n g n u p p r v e c r r e g r e n e l w h l b e l r b u n 7 2 7 w h c h v k n g h u p n b u h v e l n n r e p r e e n n l e r n n g e g h e n l e r n e e p f e u r e n n e n e n n n e r 3 1 c n f e r e n c e n n e u r l i n f r n p r c e n g s e n i p s 2 0 1 7 l n g b e c h c a u s a x 0 c f g u r e 1 t h e r e l w r l w h c h r e u b l e b e e l e b l b e l r b u n l e r n n g e e f c l g e u n l r b u n b r n g r b u n f c r w p n n n v e u l l r b u n i n h p p e r w e p r e e n l b e l r b u n l e r n n g f r e l d l f n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e 2 0 e x e n n g f f e r e n b l e e c n r e e e l w h h e l d l k h w v n g e o n e h e c n r e e h v e h e p e n l e l n g e n e r l f r f l b e l r b u n b x u r e f h e l e f n e p r e c n w h c h v k n g r n g u p n n h e f r f h e l b e l r b u n t h e e c n h h e p l n e p r e e r n f f e r e n b l e e c n r e e c n b e l e r n e b b c k p r p g n w h c h e n b l e c b n n f r e e l e r n n g n r e p r e e n n l e r n n g n n e n e n n n e r w e e f n e r b u n b e l f u n c n f r r e e b h e k u l l b c k l e b l e r v e r g e n c e k l b e w e e n h e g r u n r u h l b e l r b u n n h e r b u n p r e c e b h e r e e b f x n g p l n e w e h w h h e p z n f l e f n e p r e c n n z e h e l f u n c n f h e r e e c n b e r e e b v r n l b u n n g 1 9 2 9 n w h c h h e r g n l l f u n c n b e n z e g e e r v e l r e p l c e b e c r e n g e q u e n c e f u p p e r b u n f l l w n g h p z n r e g w e e r v e c r e e e r v e f u n c n u p e h e l e f n e p r e c n t l e r n f r e w e v e r g e h e l e f l l h e n v u l r e e b e h e l f r h e f r e n l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n i n h w h e p l n e p r e e r f l l h e n v u l r e e c n b e l e r n e j n l o u r l d l f c n b e u e h l l w n l n e e l n c n l b e n e g r e w h n e e p n e w r k e h e f e u r e l e r n n g f u n c n c n b e l n e r r n f r n n e e p n e w r k r e p e c v e l f g 2 l l u r e k e c h c h r f u r l d l f w h e r e f r e c n f w r e e h w n w e v e r f h e e f f e c v e n e f u r e l n e v e r l l d l k u c h c r w p n n p r e c n n v e n e e p r e c n b e n h u n g e n e w e l l n e c p u e r v n p p l c n e f c l g e e n h w n g g n f c n p r v e e n h e e f h e r l d l e h t h e l b e l r b u n f r h e e k n c l u e b h u n l r b u n e g h e g e r b u n n f g 1 n x u r e r b u n h e r n g r b u n n v e n f g 1 b t h e u p e r r f u r e l n b h f h e v e r f e b l e l n g e n e r l f r f l b e l r b u n f g u r e 2 i l l u r n f l b e l r b u n l e r n n g f r e t h e p c r c l e e n e h e u p u u n f h e f u n c n f p r e e r z e b x c e x 9 8 w h c h c n b e f e u r e v e c r r f u l l c n n e c e l e r f e e p n e w r k t h e b l u e n g r e e n c r c l e r e p l n e n l e f n e r e p e c v e l t w n e x f u n c n x c f x 9 5 1 n x c f x 9 5 2 r e g n e h e e w r e e r e p e c v e l t h e b l c k h r r w n c e h e c r r e p n e n c e b e w e e n h e p l n e f h e e w r e e n h e u p u u n f f u n c n f n e h n e u p u u n c r r e p n h e p l n e b e l n g n g f f e r e n r e e e c h r e e h n e p e n e n l e f n e p r e c n q e n e b h g r n l e f n e t h e u p u f h e f r e x u r e f h e r e e p r e c n f x c 2 x b 7 x c e x 9 8 n q r e l e r n e j n l n n e n e n n n e r 2 x 0 c 2 r e l e w r k s n c e u r l d l l g r h n p r e b f f e r e n b l e e c n r e e n e c e r f r r e v e w e p c l e c h n q u e f e c n r e e t h e n w e c u c u r r e n l d l e h d e c n r e e r n f r e r r n z e e c n r e e 1 6 1 3 4 r e p p u l r e n e b l e p r e c v e e l u b l e f r n c h n e l e r n n g k i n h e p l e r n n g f e c n r e e w b e n h e u r c u c h g r e e l g r h w h e r e l c l l p l h r e c n r e e e c h p l n e 1 n h u c n n b e n e g r e n n e e p l e r n n g f r e w r k e b e c b n e w h r e p r e e n n l e r n n g n n e n e n n n e r t h e n e w l p r p e e e p n e u r l e c n f r e n d f 2 0 v e r c e h p r b l e b n r u c n g f f f e r e n b l e e c n f u n c n h e p l n e n g l b l l f u n c n e f n e n r e e t h e n u r e h h e p l n e p r e e r c n b e l e r n e b b c k p r p g n n l e f n e p r e c n c n b e u p e b c r e e e r v e f u n c n o u r e h e x e n n d f r e l d l p r b l e b u h e x e n n n n r v l b e c u e l e r n n g l e f n e p r e c n c n r n e c n v e x p z n p r b l e a l h u g h e p z e f r e e u p e f u n c n w g v e n n n d f u p e l e f n e p r e c n w n l p r v e c n v e r g e f r c l f c n l c n e q u e n l w u n c l e r h w b n u c h n u p e f u n c n f r h e r l e w e b e r v e h w e v e r h h e u p e f u n c n n n d f c n b e e r v e f r v r n l b u n n g w h c h l l w u e x e n u r l d l l i n n h e r e g e u e n l d l f n n d f l e r n n g h e e n e b l e f u l p l e r e e f r e r e f f e r e n 1 w e e x p l c l e f n e l f u n c n f r f r e w h l e n l h e l f u n c n f r n g l e r e e w e f n e n n d f 2 w e l l w h e p l n e f r f f e r e n r e e b e c n n e c e h e e u p u u n f h e f e u r e l e r n n g f u n c n w h l e n d f n 3 l l r e e n l d l f c n b e l e r n e j n l w h l e r e e n n d f w e r e l e r n e l e r n v e l t h e e c h n g e n h e e n e b l e l e r n n g r e p r n b e c u e h w n n u r e x p e r e n s e c 4 4 l d l f c n g e b e e r r e u l b u n g r e r e e b u b u n g h e e n e b l e r e g p r p e n n d f h e r e u l f f r e r e e v e n w r e h n h e f r n g l e r e e t u u p w r n d f 2 0 h e c n r b u n f l d l f r e f r w e e x e n f r c l f c n 2 0 r b u n l e r n n g b p r p n g r b u n b e l f r h e f r e n e r v e h e g r e n l e r n p l n e w r h l e c n w e e r v e h e u p e f u n c n f r l e f n e b v r n l b u n n g h v n g b e r v e h h e u p e f u n c n n 2 0 w p e c l c e f v r n l b u n n g l b u n h e l e w e p r p e b v e h r e e r e g e l e r n n g h e e n e b l e f u l p l e r e e w h c h r e f f e r e n f r 2 0 b u w e h w r e e f f e c v e l b e l r b u n l e r n n g a n u b e r f p e c l z e l g r h h v e b e e n p r p e r e h e l d l k n h v e h w n h e r e f f e c v e n e n n c p u e r v n p p l c n u c h f c l g e e n 8 1 1 2 8 e x p r e n r e c g n n 3 0 n h n r e n n e n 1 0 g e n g e l 8 e f n e h e l b e l r b u n f r n n n c e v e c r c n n n g h e p r b b l e f h e n n c e h v n g e c h l b e l t h e l g v e r e g g n p r p e r l b e l r b u n n n n c e w h n g l e l b e l e g n n g g u n r t r n g l e r b u n w h e p e k h e n g l e l b e l n p r p e n l g r h c l l e i i s l l d w h c h n e r v e p z n p r c e b e n w l e r e n e r g b e e l y n g e l 2 8 h e n e f n e h r e e l e r e n e r g b e e l c l l e s c e l d l n w h c h h e b l p e r f r f e u r e l e r n n g p r v e b n g h e e x r h e n l e r n p r c n r n r e l n c r p r e e l r e h e e l g e n g 6 e v e l p e n c c e l e r e v e r n f i i s l l d c l l e b f g s l d l b u n g q u n e w n p z n a l l h e b v e l d l e h u e h h e l b e l r b u n c n b e r e p r e e n e b x u e n r p e l 2 b u h e e x p n e n l p r f h e l r e r c h e g e n e r l f h e r b u n f r a n h e r w r e h e l d l k e x e n e x n g l e r n n g l g r h e l w h l b e l r b u n g e n g n h u 7 p r p e l d s v r l d l e h b e x e n n g u p p r v e c r r e g r e r w h c h f g f u n c n e c h c p n e n f h e r b u n u l n e u l b u p p r v e c r c h n e x n g e l 2 7 h e n e x e n e b n g r e h e l d l k b v e w e g h e r e g r e r t h e h w e h u n g h e v e c r r e e e l h e w e k r e g r e r c n l e b e e r p e r f r n c e n n e h e h a o s o l d l l g b a h e l e r n n g f h r e e e l b e n l c l l p l h r p r n f u n c n e c h p l n e a o s o l d l l g b u n b l e b e c b n e w h r e p r e e n n l e r n n g e x e n n g c u r r e n e e p l e r n n g l g r h 3 x 0 c r e h e l d l k n n e r e n g p c b u h e e x n g u c h e h c l l e d l d l 5 l l f c u e n x u e n r p e l b e l d l o u r e h l d l f e x e n f f e r e n b l e e c n r e e r e l d l k n w h c h h e p r e c e l b e l r b u n f r p l e c n b e e x p r e e b l n e r c b n n f h e l b e l r b u n f h e r n n g n h u h v e n r e r c n n h e r b u n e g n r e q u r e e n f h e x u e n r p e l i n n h n k h e n r u c n f f f e r e n b l e e c n f u n c n l d l f c n b e c b n e w h r e p r e e n n l e r n n g e g l e r n e e p f e u r e n n e n e n n n e r 3 l b e l d r b u n l e r n n g f r e a f r e n e n e b l e f e c n r e e w e f r n r u c e h w l e r n n g l e e c n r e e b l b e l r b u n l e r n n g h e n e c r b e h e l e r n n g f f r e 3 1 p r b l e f r u l n l e x r e n e h e n p u p c e n y 1 2 c e n e h e c p l e e e f l b e l w h e r e c h e n u b e r f p b l e l b e l v l u e w e c n e r l b e l r b u n l e r n n g l d l p r b l e w h e r e f r e c h n p u p l e x x e 2 x 8 8 x 8 8 x h e r e l b e l r b u n x 1 x 2 x c x e 2 x 8 8 x 8 8 r c h e r e x c e x p r e e h e p r b b l f h e p l e x h v n g h e c h l b e l c n h u h h e p c c n r n h x c x e 2 x 8 8 x 8 8 0 1 n c 1 x c 1 t h e g l f h e l d l p r b l e l e r n p p n g f u n c n g x x e 2 x 8 6 x 9 2 b e w e e n n n p u p l e x n c r r e p n n g l b e l r b u n h e r e w e w n l e r n h e p p n g f u n c n g x b e c n r e e b e e l t a e c n r e e c n f e f p l n e n n e f l e f n e l e c h p l n e n x e 2 x 8 8 x 8 8 n e f n e p l f u n c n n x c 2 x b 7 x c e x 9 8 x x e 2 x 8 6 x 9 2 0 1 p r e e r z e b x c e x 9 8 e e r n e w h e h e r p l e e n h e l e f r r g h u b r e e e c h l e f n e x e 2 x 8 8 x 8 8 l h l r b u n q q 1 q 2 q c p c v e r y e q c x e 2 x 8 8 x 8 8 0 1 n c 1 q c 1 t b u l f f e r e n b l e e c n r e e f l l w n g 2 0 w e u e p r b b l c p l f u n c n n x x c e x 9 8 x c f x 8 3 f x c f x 9 5 n x x c e x 9 8 w h e r e x c f x 8 3 x c 2 x b 7 g f u n c n x c f x 9 5 x c 2 x b 7 n n e x f u n c n b r n g h e x c f x 9 5 n h u p u f f u n c n f x x c e x 9 8 n c r r e p n e n c e w h p l n e n n f x x e 2 x 8 6 x 9 2 r m r e l v l u e f e u r e l e r n n g f u n c n e p e n n g n h e p l e x n h e p r e e r x c e x 9 8 n c n k e n f r f r p l e f r c n b e l n e r r n f r n f x w h e r e x c e x 9 8 h e r n f r n r x f r c p l e x f r c n b e e e p n e w r k p e r f r r e p r e e n n l e r n n g n n e n e n n n e r h e n x c e x 9 8 h e n e w r k p r e e r t h e c r r e p n e n c e b e w e e n h e p l n e n h e u p u u n f f u n c n f n c e b x c f x 9 5 x c 2 x b 7 h r n l g e n e r e b e f r e r e e l e r n n g e w h c h u p u u n f r x e 2 x 8 0 x 9 c f x e 2 x 8 0 x 9 d r e u e f r c n r u c n g r e e e e r n e r n l a n e x p l e e n r e x c f x 9 5 x c 2 x b 7 h w n n f g 2 t h e n h e p r b b l f h e p l e x f l l n g n l e f n e g v e n b y l r p x x c e x 9 8 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 1 x e 2 x 8 8 x 8 8 l n 1 n x e 2 x 8 8 x 8 8 n w h e r e 1 x c 2 x b 7 n n c r f u n c n n l l n n l r n e n e h e e f l e f n e h e l b h e l e f n r g h u b r e e f n e n t n l n t n r r e p e c v e l t h e u p u f h e r e e t w r x e h e p p n g f u n c n g e f n e b x g x x c e x 9 8 t p x x c e x 9 8 q 2 x e 2 x 8 8 x 8 8 l 3 2 t r e e o p z n g v e n r n n g e s x n 1 u r g l l e r n e c n r e e t e c r b e n s e c 3 1 w h c h c n u p u r b u n g x x c e x 9 8 t l r f r e c h p l e x t h e n r g h f r w r w n z e h e k u l l b c k l e b l e r k l v e r g e n c e b e w e e n e c h g x x c e x 9 8 t n r e q u v l e n l n z e h e f l l w n g c r e n r p l r q x c e x 9 8 s x e 2 x 8 8 x 9 2 n c n c x 1 0 x x 1 1 1 x x c 1 x x c x l g g c x x c e x 9 8 t x e 2 x 8 8 x 9 2 x l g p x x c e x 9 8 q c 3 n 1 c 1 n 1 c 1 x e 2 x 8 8 x 8 8 l 4 x 0 c w h e r e q e n e h e r b u n h e l b l l h e l e f n e l n g c x x c e x 9 8 t h e c h u p u u n f g x x c e x 9 8 t l e r n n g h e r e e t r e q u r e h e e n f w p r e e r 1 h e p l n e p r e e r x c e x 9 8 n 2 h e r b u n q h e l b h e l e f n e t h e b e p r e e r x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r e e e r n e b x c e x 9 8 x e 2 x 8 8 x 9 7 q x e 2 x 8 8 x 9 7 r g n r q x c e x 9 8 s 4 x c e x 9 8 q t l v e e q n 4 w e c n e r n l e r n n g p z n r e g f r w e f x q n p z e x c e x 9 8 t h e n w e f x x c e x 9 8 n p z e q t h e e w l e r n n g e p r e l e r n v e l p e r f r e u n l c n v e r g e n c e r x u n u b e r f e r n r e c h e e f n e n h e e x p e r e n 3 2 1 l e r n n g s p l n e i n h e c n w e e c r b e h w l e r n h e p r e e r x c e x 9 8 f r p l n e w h e n h e r b u n h e l b h e l e f n e q r e f x e w e c p u e h e g r e n f h e l r q x c e x 9 8 s w r x c e x 9 8 b h e c h n r u l e n x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x x x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 5 x e 2 x 8 8 x 8 2 x c e x 9 8 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 n x e 2 x 8 8 x 8 8 n w h e r e n l h e f r e r e p e n n h e r e e n h e e c n e r e p e n n h e p e c f c p e f h e f u n c n f x c f x 9 5 n t h e f r e r g v e n b c x 0 1 g c x x c e x 9 8 t n l x 1 1 g c x x c e x 9 8 t n r 1 x c x 1 0 x e 2 x 8 8 x 8 2 r q x c e x 9 8 s x n x x c e x 9 8 x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 n x x c e x 9 8 6 x e 2 x 8 8 x 8 2 f x c f x 9 5 n x x c e x 9 8 n c 1 g c x x c e x 9 8 t g c x x c e x 9 8 t p p w h e r e g c x x c e x 9 8 t n l x e 2 x 8 8 x 8 8 l l n p x x c e x 9 8 q c n g c x x c e x 9 8 t n r x e 2 x 8 8 x 8 8 l r n p x x c e x 9 8 q c n e h l e t n b e h e r e e r e h e n e n h e n w e h v e g c x x c e x 9 8 t n g c x x c e x 9 8 t n l g c x x c e x 9 8 t n r t h e n h e g r e n c p u n n e q n 6 c n b e r e h e l e f n e n c r r e u n b u p n n e r t h u h e p l n e p r e e r c n b e l e r n e b n r b c k p r p g n 3 2 2 l e r n n g l e f n e n w f x n g h e p r e e r x c e x 9 8 w e h w h w l e r n h e r b u n h e l b h e l e f n e q w h c h c n r n e p z n p r b l e n r q x c e x 9 8 s x e 2 x 8 8 x 8 0 q c x q c 1 7 c 1 h e r e w e p r p e r e h c n r n e c n v e x p z n p r b l e b v r n l b u n n g 1 9 2 9 w h c h l e e p z e f r e e n f c n v e r g e u p e r u l e f r q i n v r n l b u n n g n r g n l b j e c v e f u n c n b e n z e g e r e p l c e b b u n n n e r v e n n e r a u p p e r b u n f r h e l f u n c n r q x c e x 9 8 s c n b e b n e b j e n e n x e 2 x 8 0 x 9 9 n e q u l r q x c e x 9 8 s x e 2 x 8 8 x 9 2 n c x 1 0 x x 1 1 1 x x c x l g p x x c e x 9 8 q c n 1 c 1 x e 2 x 8 8 x 8 8 l x e 2 x 8 9 x a 4 x e 2 x 8 8 x 9 2 w h e r e x c e x b e q c x 1 n n x c x 1 c 1 p x x c e x 9 8 q c g c x x c e x 9 8 t x c f x 8 6 q q x c c x 8 4 x e 2 x 8 8 x 9 2 x c x x c e x b e q x c c x 8 4 c x l g x e 2 x 8 8 x 8 8 l x 1 0 p x x c e x 9 8 q x 1 1 c x c e x b e q x c c x 8 4 c x 8 w e e f n e n c x 1 0 p x x c e x 9 8 q x 1 1 1 x x c x c x x c e x b e q x c c x 8 4 c x l g n 1 c 1 x c e x b e q x c c x 8 4 c x 9 x e 2 x 8 8 x 8 8 l t h e n x c f x 8 6 q q x c c x 8 4 n u p p e r b u n f r r q x c e x 9 8 s w h c h h h e p r p e r h f r n q n q x c c x 8 4 x c f x 8 6 q q x c c x 8 4 x e 2 x 8 9 x a 5 r q x c e x 9 8 s n x c f x 8 6 q q r q x c e x 9 8 s a u e h w e r e p n q c r r e p n n g h e h e r n h e n x c f x 8 6 q q n u p p e r b u n f r r q x c e x 9 8 s i n h e n e x e r n q 1 c h e n u c h h x c f x 8 6 q 1 q x e 2 x 8 9 x a 4 r q x c e x 9 8 s w h c h p l e r q 1 x c e x 9 8 s x e 2 x 8 9 x a 4 r q x c e x 9 8 s 5 x 0 c c n e q u e n l w e c n n z e x c f x 8 6 q q x c c x 8 4 n e f r q x c e x 9 8 s f e r e n u r n g h r q x c e x 9 8 s x c f x 8 6 q q x c c x 8 4 e q x c c x 8 4 q s w e h v e q 1 r g n x c f x 8 6 q q x e 2 x 8 8 x 8 0 q c x q c 1 1 0 c 1 w h c h l e n z n g h e l g r n g n e f n e b x c f x 9 5 q q x c f x 8 6 q q x x c e x b b x e 2 x 8 8 x 8 8 l w h e r e x c e x b b h e l g r n g e u l p l e r b e n g x c e x b b 1 n e h q c 1 x e 2 x 8 8 x 8 8 0 1 n r b u n h e l b h e l e f n e t h e r n g 0 r b u n q c c 1 3 3 q c x e 2 x 8 8 x 9 2 1 1 1 c 1 x e 2 x 8 8 x 8 2 x c f x 9 5 q q x e 2 x 8 8 x 8 2 q c n c 1 x x c 1 x c e x b e q c x n q c n 1 c 1 x f e h q c c x 0 w e h v e p n c x x c e x b e q c x p c 1 p n c x c e x b e q x x c 1 1 c 1 2 1 1 e q n 1 2 h e u p e c h e e f r c 1 q c 0 p n q c n b e p l n l z e b h e u n f r p c l e r n n g f r e a f r e n e n e b l e f e c n r e e f t 1 t k i n h e r n n g g e l l r e e n h e f r e f u e h e e p r e e r x c e x 9 8 f r f e u r e l e r n n g f u n c n f x c 2 x b 7 x c e x 9 8 b u c r r e p n f f e r e n u p u u n f f g n e b x c f x 9 5 e e f g 2 b u e c h r e e h n e p e n e n l e f n e p r e c n q t h e l f u n c n f r f r e g v e n b v e r g n g h e l f u n c n f r l l n v u l r e e p k 1 r f k k 1 r t k w h e r e r t k h e l f u n c n f r r e e t k e f n e b e q n 3 t l e r n x c e x 9 8 b f x n g h e l e f n e p r e c n q f l l h e r e e n h e f r e f b e n h e e r v n n s e c 3 2 n r e f e r r n g f g 2 w e h v e n k x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 1 x x x x e 2 x 8 8 x 8 2 r f x e 2 x 8 8 x 8 2 r t k x e 2 x 8 8 x 8 2 x c e x 9 8 k 1 x e 2 x 8 8 x 8 2 f x c f x 9 5 k n x x c e x 9 8 x e 2 x 8 8 x 8 2 x c e x 9 8 1 3 k 1 n x e 2 x 8 8 x 8 8 n k w h e r e n k n x c f x 9 5 k x c 2 x b 7 r e h e p l n e e n h e n e x f u n c n f t k r e p e c v e l n e h h e n e x f u n c n x c f x 9 5 k x c 2 x b 7 f r e c h r e e r n l g n e b e f r e r e e l e r n n g n h u p l n e c r r e p n u b e f u p u u n f f t h r e g l r h e r n u b p c e e h 1 7 w h c h n c r e e h e r n n e n r n n g r e u c e h e r k f v e r f n g a f r q n c e e c h r e e n h e f r e f h w n l e f n e p r e c n q w e c n u p e h e n e p e n e n l b e q n 1 2 g v e n b x c e x 9 8 f r p l e e n n l c n v e n e n c e w e n c n u c h u p e c h e e n h e w h l e e s b u n e f n b c h e b t h e r n n g p r c e u r e f l d l f h w n n a l g r h 1 a l g r h 1 t h e r n n g p r c e u r e f l d l f r e q u r e s r n n g e n b h e n u b e r f n b c h e u p e q i n l z e x c e x 9 8 r n l n q u n f r l e b x e 2 x 8 8 x 8 5 w h l e n c n v e r g e w h l e b n b r n l e l e c n b c h b f r s u p e s x c e x 9 8 b c p u n g g r e n e q n 1 3 n b b b b e n w h l e u p e q b e r n g e q n 1 2 n b b x e 2 x 8 8 x 8 5 e n w h l e i n h e e n g g e h e u p u f h e f r e f g v e n b v e r g n g h e p r e c n f r l l h e p k 1 n v u l r e e g x x c e x 9 8 f k k 1 g x x c e x 9 8 t k 6 x 0 c 4 e x p e r e n l r e u l o u r r e l z n f l d l f b e n x e 2 x 8 0 x 9 c c f f e x e 2 x 8 0 x 9 d 1 8 i u l r n p l e e n e n r n e u r l n e w r k l e r w e c n e h e r u e h l l w n l n e e l l d l f r n e g r e w h n e e p n e w r k l d l f w e e v l u e l d l f n f f e r e n l d l k n c p r e w h h e r n l n e l d l e h a l d l f c n b e l e r n e f r r w g e n n e n e n n n e r w e v e r f l d l f n c p u e r v n p p l c n e f c l g e e n t h e e f u l e n g f r h e p r e e r f u r f r e r e r e e n u b e r 5 r e e e p h 7 u p u u n n u b e r f h e f e u r e l e r n n g f u n c n 6 4 e r n e u p e l e f n e p r e c n 2 0 h e n u b e r f n b c h e u p e l e f n e p r e c n 1 0 0 x u e r n 2 5 0 0 0 4 1 c p r n f l d l f s n l n e l d l m e h w e c p r e u r h l l w e l l d l f w h h e r e f h e r n l n e l d l e h f r l d l f h e f e u r e l e r n n g f u n c n f x x c e x 9 8 l n e r r n f r n f x e h e h u p u u n f x x c e x b 8 x c e x b 8 x w h e r e x c e x b 8 h e h c l u n f h e r n f r n r x x c e x 9 8 w e u e 3 p p u l r l d l e n 6 m v e h u n g e n e n n u r l s c e n e 1 t h e p l e n h e e 3 e r e r e p r e e n e b n u e r c l e c r p r n h e g r u n r u h f r h e r e h e r n g r b u n f c r w p n n n v e h e e e r b u n r e l e h u n g e n e n l b e l r b u n n c e n e u c h p l n k n c l u r e p e c v e l t h e l b e l r b u n f h e e 3 e r e x u r e r b u n u c h h e r n g r b u n h w n n f g 1 b f l l w n g 7 2 7 w e u e 6 e u r e e v l u e h e p e r f r n c e f l d l e h w h c h c p u e h e v e r g e l r n c e b e w e e n h e p r e c e r n g r b u n n h e r e l r n g r b u n n c l u n g 4 n c e e u r e k l e u c l e n s x c f x 8 6 r e n e n s q u r e x c f x 8 7 2 n w l r e u r e f e l i n e r e c n w e e v l u e u r h l l w e l l d l f n h e e 3 e n c p r e w h h e r e f h e r n l n e l d l e h t h e r e u l f l d l f n h e c p e r r e u r z e n t b l e 1 f r m v e w e q u e h e r e u l r e p r e n 2 7 h e c e f 2 7 n p u b l c l v l b l e f r h e r e u l f h e h e r w w e r u n c e h h e u h r h e v l b l e i n l l c e f l l w n g 2 7 6 w e p l e c h e n 1 0 f x e f l n n r e n f l c r v l n w h c h r e p r e e n h e r e u l b x e 2 x 8 0 x 9 c e n x c 2 x b 1 n r e v n x e 2 x 8 0 x 9 d n e r l e h w r n n g n e n g g e v e a c n b e e e n f r t b l e 1 l d l f p e r f r b e n l l f h e x e u r e t b l e 1 c p r n r e u l n h r e e l d l e 6 x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 1 x e 2 x 8 0 x 9 d n x e 2 x 8 0 x 9 c x e 2 x 8 6 x 9 3 x e 2 x 8 0 x 9 d n c e h e l r g e r n h e l l e r h e b e e r r e p e c v e l d e m e h k l x e 2 x 8 6 x 9 3 e u c l e n x e 2 x 8 6 x 9 3 s x c f x 8 6 r e n e n x e 2 x 8 6 x 9 3 s q u r e x c f x 8 7 2 x e 2 x 8 6 x 9 3 f e l x e 2 x 8 6 x 9 1 i n e r e c n x e 2 x 8 6 x 9 1 m v e l d l f u r a o s o l d l g b 2 7 l d l g b 2 7 l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 0 7 3 x c 2 x b 1 0 0 0 5 0 0 8 6 x c 2 x b 1 0 0 0 4 0 0 9 0 x c 2 x b 1 0 0 0 4 0 0 9 2 x c 2 x b 1 0 0 0 5 0 0 9 9 x c 2 x b 1 0 0 0 4 0 1 2 9 x c 2 x b 1 0 0 0 7 0 1 3 3 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 9 x c 2 x b 1 0 0 0 3 0 1 5 8 x c 2 x b 1 0 0 0 4 0 1 6 7 x c 2 x b 1 0 0 0 4 0 1 8 7 x c 2 x b 1 0 0 0 4 0 1 3 0 x c 2 x b 1 0 0 0 3 0 1 5 2 x c 2 x b 1 0 0 0 3 0 1 5 5 x c 2 x b 1 0 0 0 3 0 1 5 6 x c 2 x b 1 0 0 0 4 0 1 6 4 x c 2 x b 1 0 0 0 3 0 1 8 3 x c 2 x b 1 0 0 0 4 0 0 7 0 x c 2 x b 1 0 0 0 4 0 0 8 4 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 3 0 0 8 8 x c 2 x b 1 0 0 0 4 0 0 9 6 x c 2 x b 1 0 0 0 4 0 1 2 0 x c 2 x b 1 0 0 0 5 0 9 8 1 x c 2 x b 1 0 0 0 1 0 9 7 8 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 7 x c 2 x b 1 0 0 0 1 0 9 7 4 x c 2 x b 1 0 0 0 1 0 9 6 7 x c 2 x b 1 0 0 0 1 0 8 7 0 x c 2 x b 1 0 0 0 3 0 8 4 8 x c 2 x b 1 0 0 0 3 0 8 4 5 x c 2 x b 1 0 0 0 3 0 8 4 4 x c 2 x b 1 0 0 0 4 0 8 3 6 x c 2 x b 1 0 0 0 3 0 8 1 7 x c 2 x b 1 0 0 0 4 l d l f u r l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 2 2 8 x c 2 x b 1 0 0 0 6 0 2 4 5 x c 2 x b 1 0 0 1 9 0 2 3 1 x c 2 x b 1 0 0 2 1 0 2 3 9 x c 2 x b 1 0 0 1 8 0 0 8 5 x c 2 x b 1 0 0 0 2 0 0 9 9 x c 2 x b 1 0 0 0 5 0 0 7 6 x c 2 x b 1 0 0 0 6 0 0 8 9 x c 2 x b 1 0 0 0 6 0 2 1 2 x c 2 x b 1 0 0 0 2 0 2 2 9 x c 2 x b 1 0 0 1 5 0 2 3 1 x c 2 x b 1 0 0 1 2 0 2 5 3 x c 2 x b 1 0 0 0 9 0 1 7 9 x c 2 x b 1 0 0 0 4 0 1 8 9 x c 2 x b 1 0 0 2 1 0 2 1 1 x c 2 x b 1 0 0 1 8 0 2 0 5 x c 2 x b 1 0 0 1 2 0 9 4 8 x c 2 x b 1 0 0 0 1 0 9 4 0 x c 2 x b 1 0 0 0 6 0 9 3 8 x c 2 x b 1 0 0 0 8 0 9 4 4 x c 2 x b 1 0 0 0 3 0 7 8 8 x c 2 x b 1 0 0 0 2 0 7 7 1 x c 2 x b 1 0 0 1 5 0 7 6 9 x c 2 x b 1 0 0 1 2 0 7 4 7 x c 2 x b 1 0 0 0 9 l d l f u r l d s v r 7 b f g s l d l 6 i i s l d l 1 1 0 5 3 4 x c 2 x b 1 0 0 1 3 0 8 5 2 x c 2 x b 1 0 0 2 3 0 8 5 6 x c 2 x b 1 0 0 6 1 0 8 7 9 x c 2 x b 1 0 0 2 3 0 3 1 7 x c 2 x b 1 0 0 1 4 0 5 1 1 x c 2 x b 1 0 0 2 1 0 4 7 5 x c 2 x b 1 0 0 2 9 0 4 5 8 x c 2 x b 1 0 0 1 4 0 3 3 6 x c 2 x b 1 0 0 1 0 0 4 9 2 x c 2 x b 1 0 0 1 6 0 5 0 8 x c 2 x b 1 0 0 2 6 0 5 3 9 x c 2 x b 1 0 0 1 1 0 4 4 8 x c 2 x b 1 0 0 1 7 0 5 9 5 x c 2 x b 1 0 0 2 6 0 7 1 6 x c 2 x b 1 0 0 4 1 0 7 9 2 x c 2 x b 1 0 0 1 9 0 8 2 4 x c 2 x b 1 0 0 0 8 0 8 1 3 x c 2 x b 1 0 0 0 8 0 7 2 2 x c 2 x b 1 0 0 2 1 0 6 8 6 x c 2 x b 1 0 0 0 9 0 6 6 4 x c 2 x b 1 0 0 1 0 0 5 0 9 x c 2 x b 1 0 0 1 6 0 4 9 2 x c 2 x b 1 0 0 2 6 0 4 6 1 x c 2 x b 1 0 0 1 1 h u n g e n e n u r l s c e n e 4 2 e v l u n f l d l f n f c l a g e e n i n e l e r u r e 8 1 1 2 8 1 5 5 g e e n f r u l e l d l p r b l e w e c n u c f c l g e e n e x p e r e n n m r p h 2 4 w h c h c n n r e h n 5 0 0 0 0 f c l g e f r b u 1 3 0 0 0 p e p l e f f f e r e n r c e e c h f c l g e n n e w h c h r n l g c l g e t g e n e r e n g e r b u n f r e c h f c e g e w e f l l w h e e r e g u e n 8 2 8 5 w h c h u e g u n r b u n w h e e n h e c h r n l g c l g e f h e f c e g e f g 1 t h e p r e c e g e f r f c e g e p l h e g e h v n g h e h g h e p r b b l n h e p r e c e 1 w e w n l h e e e f r h p c e e u e u c n p e p l e x g e n g l d l n e x h 7 x 0 c l b e l r b u n t h e p e r f r n c e f g e e n e v l u e b h e e n b l u e e r r r m a e b e w e e n p r e c e g e n c h r n l g c l g e a h e c u r r e n e f h e r r e u l n m r p h b n b f n e u n n g d l d l 5 n v g g f c e 2 3 w e l b u l l d l f n v g g f c e b r e p l c n g h e f x l e r n v g g n e b l d l f f l l w n g 5 w e n r 1 0 e n f l c r v l n n h e r e u l r e u r z e n t b l e 2 w h c h h w l d l f c h e v e h e e f h e r p e r f r n c e n m r p h n e h h e g n f c n p e r f r n c e g n b e w e e n e e p l d l e l d l d l n l d l f n n n e e p l d l e l i i s l d l c p n n b f g s l d l n h e u p e r r f l d l f c p r e w h d l d l v e r f e h e e f f e c v e n e f e n e n l e r n n g n u r r e e b e e l f r l d l r e p e c v e l t b l e 2 m a e f g e e n c p r n n m r p h 2 4 m e h i i s l d l 1 1 c p n n 1 1 b f g s l d l 6 d l d l v g g f c e 5 l d l f v g g f c e u r m a e 5 6 7 x c 2 x b 1 0 1 5 4 8 7 x c 2 x b 1 0 3 1 3 9 4 x c 2 x b 1 0 0 5 2 4 2 x c 2 x b 1 0 0 1 2 2 4 x c 2 x b 1 0 0 2 a h e r b u n f g e n e r n e h n c v e r u n b l n c e n m r p h n g e e n e h 1 3 1 4 1 5 r e e v l u e n u b e f m r p h c l l e m r p h s u b f r h r w h c h c n f 2 0 1 6 0 e l e c e f c l g e v h e n f l u e n c e f u n b l n c e r b u n t h e b e p e r f r n c e r e p r e n m r p h s u b g v e n b d 2 l d l 1 5 e p e n e n l d l e h a d 2 l d l u e h e u p u f h e x e 2 x 8 0 x 9 c f c 7 x e 2 x 8 0 x 9 d l e r n a l e x n e 2 1 h e f c e g e f e u r e h e r e w e n e g r e l d l f w h a l e x n e f l l w n g h e e x p e r e n e n g u e n d 2 l d l w e e v l u e u r l d l f n h e c p e r n c l u n g b h s l l n l d l b e e h u n e r x f f e r e n r n n g e r 1 0 6 0 a l l f h e c p e r r e r n e n h e e e e p f e u r e u e b d 2 l d l a c n b e e e n f r t b l e 3 u r l d l f g n f c n l u p e r f r h e r f r l l r n n g e r n e h h e g e n e r e g e r f g u r e 3 m a e f g e e n c p r n n b u n r e u n l r b u n m r p h s u b n h e l b e l r b u n u e n t r n n g e r m e h s e c 4 1 r e x u r e r b u n 1 0 2 0 3 0 4 0 5 0 6 0 t h e p r p e e h l d l f c h e v e a a s 2 2 4 9 0 8 1 4 7 6 1 6 4 6 5 0 7 4 5 5 5 3 4 4 6 9 0 4 4 0 6 1 h e e f h e r r e u l n b h f l a r r 1 2 4 7 5 0 1 4 6 1 1 2 4 5 1 3 1 4 4 2 7 3 4 3 5 0 0 4 2 9 4 9 i i s a l d l 9 4 1 7 9 1 4 1 6 8 3 4 1 2 2 8 4 1 1 0 7 4 1 0 2 4 4 0 9 0 2 h e w h c h v e r f e h u r e l d 2 l d l 1 5 4 1 0 8 0 3 9 8 5 7 3 9 2 0 4 3 8 7 1 2 3 8 5 6 0 3 8 3 8 5 h h e b l e l n g e n e r l l d l f u r 3 8 4 9 5 3 6 2 2 0 3 3 9 9 1 3 2 4 0 1 3 1 9 1 7 3 1 2 2 4 f r f l b e l r b u n 4 3 t e c p l e x l e h n b b e h e r e e e p h n h e b c h z e r e p e c v e l e c h r e e h 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 p l n e n 2 h x e 2 x 8 8 x 9 2 1 l e f n e l e d 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 f r n e r e e n n e p l e h e c p l e x f f r w r p n b c k w r p r e o d d 1 x c 3 x 9 7 c o d x c 3 x 9 7 c n o d 1 x c 3 x 9 7 c d x c 3 x 9 7 c o d x c 3 x 9 7 c r e p e c v e l s f r k r e e n n b b c h e h e c p l e x f f r w r n b c k w r p o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b t h e c p l e x f n e r n u p e l e f n e r e o n b x c 3 x 9 7 b x c 3 x 9 7 k x c 3 x 9 7 c x c 3 x 9 7 d 1 o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b t h u h e c p l e x f r h e r n n g p r c e u r e n e e p c h n b b c h e n h e e n g p r c e u r e n e p l e r e o d x c 3 x 9 7 c x c 3 x 9 7 k x c 3 x 9 7 n b x c 3 x 9 7 b n o d x c 3 x 9 7 c x c 3 x 9 7 k r e p e c v e l l d l f r e e f f c e n o n m r p h s u b 1 2 6 3 6 r n n g g e 8 4 2 4 e n g g e u r e l n l k e 5 2 5 0 f r r n n g 2 5 0 0 0 e r n n 8 f r e n g l l 8 4 2 4 g e 4 4 p r e e r d c u n n w w e c u h e n f l u e n c e f p r e e r e n g n p e r f r n c e w e r e p r h e r e u l f r n g p r e c n n m v e e u r e b k l n g e e n n m r p h s u b w h 6 0 r n n g e r e u r e b m a e f r f f e r e n p r e e r e n g n h e c n t r e e n u b e r a f r e n e n e b l e e l n e c e r n v e g e h w p e r f r n c e c h n g e b v r n g h e r e e n u b e r u e n f r e n e h w e c u e n s e c 2 h e e n e b l e r e g l e r n f r e p r p e n n d f 2 0 f f e r e n f r u r t h e r e f r e n e c e r e e w h c h e n e b l e r e g b e e r l e r n f r e t w r h e n w e r e p l c e u r e n e b l e r e g n l d l f b h e n e u e n n d f n n e h e h n d f l d l t h e c r r e p n n g h l l w e l n e b n d f l d l w e f x h e r p r e e r e r e e e p h n 8 x 0 c u p u u n n u b e r f h e f e u r e l e r n n g f u n c n h e e f u l e n g a h w n n f g 4 u r e n e b l e r e g c n p r v e h e p e r f r n c e b u n g r e r e e w h l e h e n e u e n n d f e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e o b e r v e f r f g 4 h e p e r f r n c e f l d l f c n b e p r v e b u n g r e r e e b u h e p r v e e n b e c e n c r e n g l l l e r n l l e r t h e r e f r e u n g u c h l r g e r e n e b l e e n e l b g p r v e e n o n m v e h e n u b e r f r e e k 1 0 0 k l 0 0 7 0 v k 2 0 k l 0 0 7 1 n e h n l l r n f r e b e e h u e l r g e n u b e r f r e e e g s h n e l 2 5 b n e v e r g p e e n r e u l f r e p h g e b n l 3 e c n r e e t r e e e p h t r e e e p h n h e r p r n p r e e r f r e c n r e e i n l d l f h e r e n p l c c n r n b e w e e n r e e e p h h n u p u u n n u b e r f h e f e u r e l e r n n g f u n c n x c f x 8 4 x c f x 8 4 x e 2 x 8 9 x a 5 2 h x e 2 x 8 8 x 9 2 1 x e 2 x 8 8 x 9 2 1 t c u h e n f l u e n c e f r e e e p h h e p e r f r n c e f l d l f w e e x c f x 8 4 2 h x e 2 x 8 8 x 9 2 1 n f x r e e n u b e r k 1 n h e p e r f r n c e c h n g e b v r n g r e e e p h h w n n f g 4 b w e e e h h e p e r f r n c e f r p r v e h e n e c r e e w h h e n c r e e f h e r e e e p h t h e r e n h e r e e e p h n c r e e h e e n n f l e r n e f e u r e n c r e e e x p n e n l l w h c h g r e l n c r e e h e r n n g f f c u l s u n g u c h l r g e r e p h l e b p e r f r n c e o n m v e r e e e p h h 1 8 k l 0 1 1 6 2 v h 9 k l 0 0 8 3 1 f g u r e 4 t h e p e r f r n c e c h n g e f g e e n n m r p h s u b n r n g p r e c n n m v e b v r n g r e e n u b e r n b r e e e p h o u r p p r c h l d l f l d l f c n p r v e h e p e r f r n c e b u n g r e r e e w h l e u n g h e e n e b l e r e g p r p e n n d f n d f l d l n d f l d l e v e n l e w r e p e r f r n c e h n n e f r n g l e r e e 5 c n c l u n w e p r e e n l b e l r b u n l e r n n g f r e n v e l l b e l r b u n l e r n n g l g r h n p r e b f f e r e n b l e e c n r e e w e e f n e r b u n b e l f u n c n f r h e f r e n f u n h h e l e f n e p r e c n c n b e p z e v v r n l b u n n g w h c h e n b l e l l h e r e e n h e f e u r e h e u e b e l e r n e j n l n n e n e n n n e r e x p e r e n l r e u l h w e h e u p e r r f u r l g r h f r e v e r l l d l k n r e l e c p u e r v n p p l c n n v e r f e u r e l h h e b l e l n g e n e r l f r f l b e l r b u n a c k n w l e g e e n t h w r k w u p p r e n p r b h e n n l n u r l s c e n c e f u n n f c h n n 6 1 6 7 2 3 3 6 n p r b x e 2 x 8 0 x 9 c c h e n g u n g x e 2 x 8 0 x 9 d p r j e c u p p r e b s h n g h m u n c p l e u c n c n n s h n g h e u c n d e v e l p e n f u n n n 1 5 c g 4 3 n n p r b o n r n 0 0 0 1 4 1 5 1 2 3 5 6 r e f e r e n c e 1 y a n d g e n s h p e q u n z n n r e c g n n w h r n z e r e e n e u r l c p u n 9 7 1 5 4 5 x e 2 x 8 0 x 9 3 1 5 8 8 1 9 9 7 2 a l b e r g e r s d p e r n v j d p e r a x u e n r p p p r c h n u r l l n g u g e p r c e n g c p u n l l n g u c 2 2 1 3 9 x e 2 x 8 0 x 9 3 7 1 1 9 9 6 3 l b r e n r n f r e m c h n e l e r n n g 4 5 1 5 x e 2 x 8 0 x 9 3 3 2 2 0 0 1 4 a c r n n j s h n d e c n f r e f r c p u e r v n n m e c l i g e a n l s p r n g e r 2 0 1 3 5 b b g c x n g c w x e j w u n x g e n g d e e p l b e l r b u n l e r n n g w h l b e l b g u r x v 1 6 1 1 0 1 7 3 1 2 0 1 7 6 x g e n g l b e l r b u n l e r n n g i e e e t r n k n w l d e n g 2 8 7 1 7 3 4 x e 2 x 8 0 x 9 3 1 7 4 8 2 0 1 6 9 x 0 c 7 x g e n g n p h u p r e r e l e e p r e c n f c r w p n n n v e b l b e l r b u n l e r n n g i n p r i j c a i p g e 3 5 1 1 x e 2 x 8 0 x 9 3 3 5 1 7 2 0 1 5 8 x g e n g k s h m l e n z z h u f c l g e e n b l e r n n g f r l b e l r b u n i n p r c a a a i 2 0 1 0 9 x g e n g q w n g n y x f c l g e e n b p v e l b e l r b u n l e r n n g i n p r c i c p r p g e 4 4 6 5 x e 2 x 8 0 x 9 3 4 4 7 0 2 0 1 4 1 0 x g e n g n y x h e p e e n b e n u l v r e l b e l r b u n i n p r c c v p r p g e 1 8 3 7 x e 2 x 8 0 x 9 3 1 8 4 2 2 0 1 4 1 1 x g e n g c y n n z z h u f c l g e e n b l e r n n g f r l b e l r b u n i e e e t r n p e r n a n l m c h i n e l l 3 5 1 0 2 4 0 1 x e 2 x 8 0 x 9 3 2 4 1 2 2 0 1 3 1 2 g g u y f u c r d e r n t s h u n g i g e b e h u n g e e n b n f l l e r n n g n l c l l j u e r b u r e g r e n i e e e t r n i g e p r c e n g 1 7 7 1 1 7 8 x e 2 x 8 0 x 9 3 1 1 8 8 2 0 0 8 1 3 g g u n g m u h u n g e e n w h h e n f l u e n c e c r r c e n g e n e r i n c v p r w r k h p p g e 7 1 x e 2 x 8 0 x 9 3 7 8 2 0 1 0 1 4 g g u n c z h n g a u n c r p p u l n g e e n i n p r c c v p r p g e 4 2 5 7 x e 2 x 8 0 x 9 3 4 2 6 3 2 0 1 4 1 5 z h e x l z z h n g f w u x g e n g y z h n g m h y n g n y z h u n g d e p e n e n l b e l r b u n l e r n n g f r g e e n i e e e t r n n i g e p r c e n g 2 0 1 7 1 6 t k h r n e c n f r e i n p r c i c d a r p g e 2 7 8 x e 2 x 8 0 x 9 3 2 8 2 1 9 9 5 1 7 t k h t h e r n u b p c e e h f r c n r u c n g e c n f r e i e e e t r n p e r n a n l m c h i n e l l 2 0 8 8 3 2 x e 2 x 8 0 x 9 3 8 4 4 1 9 9 8 1 8 y j e s h e l h e r j d n h u e s k r e v j l n g r g r h c k s g u r r n t d r r e l l c f f e c n v l u n l r c h e c u r e f r f f e u r e e b e n g r x v p r e p r n r x v 1 4 0 8 5 0 9 3 2 0 1 4 1 9 m i j r n z g h h r n t s j k k l n l k s u l a n n r u c n v r n l e h f r g r p h c l e l m c h n e l e r n n g 3 7 2 1 8 3 x e 2 x 8 0 x 9 3 2 3 3 1 9 9 9 2 0 p k n c h e e r m f e r u a c r n n s r b u l x c 3 x b 2 d e e p n e u r l e c n f r e i n p r c i c c v p g e 1 4 6 7 x e 2 x 8 0 x 9 3 1 4 7 5 2 0 1 5 2 1 a k r z h e v k i s u k e v e r n g e h n n i g e n e c l f c n w h e e p c n v l u n l n e u r l n e w r k i n p r c n i p s p g e 1 1 0 6 x e 2 x 8 0 x 9 3 1 1 1 4 2 0 1 2 2 2 a l n c d r g n v n c c h r u l u c p r n g f f e r e n c l f e r f r u c g e e n i e e e t r n n c b e r n e c 3 4 1 6 2 1 x e 2 x 8 0 x 9 3 6 2 8 2 0 0 4 2 3 o m p r k h a v e l n a z e r n d e e p f c e r e c g n n i n p r c b m v c p g e 4 1 1 x e 2 x 8 0 x 9 3 4 1 1 2 2 0 1 5 2 4 k r c n e k n t t e f e m o r p h a l n g u n l g e b e f n r l u l g e p r g r e n i n p r c f g p g e 3 4 1 x e 2 x 8 0 x 9 3 3 4 5 2 0 0 6 2 5 j s h n a w f z g b b n m c k t s h r p m f n c c h r m r e a k p n n a b l k e r e l e h u n p e r e c g n n n p r f r n g l e e p h g e i n p r c c v p r p g e 1 2 9 7 x e 2 x 8 0 x 9 3 1 3 0 4 2 0 1 1 2 6 g t u k n i k k m u l l b e l c l f c n a n v e r v e w i n e r n n l j u r n l f d w r e h u n g n m n n g 3 3 1 x e 2 x 8 0 x 9 3 1 3 2 0 0 7 2 7 c x n g x g e n g n h x u e l g c b n g r e g r e n f r l b e l r b u n l e r n n g i n p r c c v p r p g e 4 4 8 9 x e 2 x 8 0 x 9 3 4 4 9 7 2 0 1 6 2 8 x y n g x g e n g n d z h u s p r c n n l e n e r g l b e l r b u n l e r n n g f r g e e n i n p r c i j c a i p g e 2 2 5 9 x e 2 x 8 0 x 9 3 2 2 6 5 2 0 1 6 2 9 a l y u l l e n a r n g r j n t h e c n c v e c n v e x p r c e u r e n e u r l c p u n 1 5 4 9 1 5 x e 2 x 8 0 x 9 3 9 3 6 2 0 0 3 3 0 y z h u h x u e n x g e n g e n r b u n r e c g n n f r f c l e x p r e n i n p r c m m p g e 1 2 4 7 x e 2 x 8 0 x 9 3 1 2 5 0 2 0 1 5 1 0 x 0 c']], shape=(1, 1), dtype=string)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.32\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.22\n",
      "\n",
      "\n",
      "* * * * EVALUATION USING RSW_STM_LOW AS PREPROCESSING FUNCTION * * * *\n",
      "Length of the longest sample is: 15000\n",
      "\n",
      "\n",
      "***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\n",
      "Sample considered is:  tf.Tensor([[b'Label Distribution Learning Forests\\n\\nWei Shen1,2 , Kai Zhao1 , Yilu Guo1 , Alan Yuille2\\nKey Laboratory of Specialty Fiber Optics and Optical Access Networks,\\nShanghai Institute for Advanced Communication and Data Science,\\nSchool of Communication and Information Engineering, Shanghai University\\n2\\nDepartment of Computer Science, Johns Hopkins University\\n\\narXiv:1702.06086v4 [cs.LG] 16 Oct 2017\\n\\n1\\n\\n{shenwei1231,zhaok1206,gyl.luan0,alan.l.yuille}@gmail.com\\n\\nAbstract\\nLabel distribution learning (LDL) is a general learning framework, which assigns\\nto an instance a distribution over a set of labels rather than a single label or multiple\\nlabels. Current LDL methods have either restricted assumptions on the expression\\nform of the label distribution or limitations in representation learning, e.g., to\\nlearn deep features in an end-to-end manner. This paper presents label distribution\\nlearning forests (LDLFs) - a novel label distribution learning algorithm based on\\ndifferentiable decision trees, which have several advantages: 1) Decision trees\\nhave the potential to model any general form of label distributions by a mixture\\nof leaf node predictions. 2) The learning of differentiable decision trees can be\\ncombined with representation learning. We define a distribution-based loss function\\nfor a forest, enabling all the trees to be learned jointly, and show that an update\\nfunction for leaf node predictions, which guarantees a strict decrease of the loss\\nfunction, can be derived by variational bounding. The effectiveness of the proposed\\nLDLFs is verified on several LDL tasks and a computer vision application, showing\\nsignificant improvements to the state-of-the-art LDL methods.\\n\\n1\\n\\nIntroduction\\n\\nLabel distribution learning (LDL) [6, 11] is a learning framework to deal with problems of label\\nambiguity. Unlike single-label learning (SLL) and multi-label learning (MLL) [26], which assume an\\ninstance is assigned to a single label or multiple labels, LDL aims at learning the relative importance\\nof each label involved in the description of an instance, i.e., a distribution over the set of labels. Such\\na learning strategy is suitable for many real-world problems, which have label ambiguity. An example\\nis facial age estimation [8]. Even humans cannot predict the precise age from a single facial image.\\nThey may say that the person is probably in one age group and less likely to be in another. Hence it is\\nmore natural to assign a distribution of age labels to each facial image (Fig. 1(a)) instead of using a\\nsingle age label. Another example is movie rating prediction [7]. Many famous movie review web\\nsites, such as Netflix, IMDb and Douban, provide a crowd opinion for each movie specified by the\\ndistribution of ratings collected from their users (Fig. 1(b)). If a system could precisely predict such a\\nrating distribution for every movie before it is released, movie producers can reduce their investment\\nrisk and the audience can better choose which movies to watch.\\nMany LDL methods assume the label distribution can be represented by a maximum entropy model [2]\\nand learn it by optimizing an energy function based on the model [8, 11, 28, 6]. But, the exponential\\npart of this model restricts the generality of the distribution form, e.g., it has difficulty in representing\\nmixture distributions. Some other LDL methods extend the existing learning algorithms, e.g, by\\nboosting and support vector regression, to deal with label distributions [7, 27], which avoid making\\nthis assumption, but have limitations in representation learning, e.g., they do not learn deep features\\nin an end-to-end manner.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\n\\n\\x0cFigure 1: The real-world data which are suitable to be modeled by label distribution learning. (a)\\nEstimated facial ages (a unimodal distribution). (b) Rating distribution of crowd opinion on a movie\\n(a multimodal distribution).\\n\\nIn this paper, we present label distribution learning forests (LDLFs) - a novel label distribution\\nlearning algorithm inspired by differentiable decision trees [20]. Extending differentiable decision\\ntrees to deal with the LDL task has two advantages. One is that decision trees have the potential\\nto model any general form of label distributions by mixture of the leaf node predictions, which\\navoid making strong assumption on the form of the label distributions. The second is that the split\\nnode parameters in differentiable decision trees can be learned by back-propagation, which enables\\na combination of tree learning and representation learning in an end-to-end manner. We define a\\ndistribution-based loss function for a tree by the Kullback-Leibler divergence (K-L) between the\\nground truth label distribution and the distribution predicted by the tree. By fixing split nodes, we\\nshow that the optimization of leaf node predictions to minimize the loss function of the tree can\\nbe addressed by variational bounding [19, 29], in which the original loss function to be minimized\\ngets iteratively replaced by a decreasing sequence of upper bounds. Following this optimization\\nstrategy, we derive a discrete iterative function to update the leaf node predictions. To learn a forest,\\nwe average the losses of all the individual trees to be the loss for the forest and allow the split nodes\\nfrom different trees to be connected to the same output unit of the feature learning function. In this\\nway, the split node parameters of all the individual trees can be learned jointly. Our LDLFs can be\\nused as a (shallow) stand-alone model, and can also be integrated with any deep networks, i.e., the\\nfeature learning function can be a linear transformation and a deep network, respectively. Fig. 2\\nillustrates a sketch chart of our LDLFs, where a forest consists of two trees is shown.\\nWe verify the effectiveness of our model on several LDL tasks, such as crowd opinion prediction on\\nmovies and disease prediction based on human genes, as well as one computer vision application, i.e.,\\nfacial age estimation, showing significant improvements to the state-of-the-art LDL methods. The\\nlabel distributions for these tasks include both unimodal distributions (e.g., the age distribution in\\nFig. 1(a)) and mixture distributions (the rating distribution on a movie in Fig. 1(b)). The superiority\\nof our model on both of them verifies its ability to model any general form of label distributions\\n\\nFigure 2: Illustration of a label distribution learning forest. The top circles denote the output units\\nof the function f parameterized by \\xce\\x98, which can be a feature vector or a fully-connected layer of\\na deep network. The blue and green circles are split nodes and leaf nodes, respectively. Two index\\nfunction \\xcf\\x951 and \\xcf\\x952 are assigned to these two trees respectively. The black dash arrows indicate the\\ncorrespondence between the split nodes of these two trees and the output units of function f . Note\\nthat, one output unit may correspond to the split nodes belonging to different trees. Each tree has\\nindependent leaf node predictions q (denoted by histograms in leaf nodes). The output of the forest\\nis a mixture of the tree predictions. f (\\xc2\\xb7; \\xce\\x98) and q are learned jointly in an end-to-end manner.\\n\\n2\\n\\n\\x0c2\\n\\nRelated Work\\n\\nSince our LDL algorithm is inspired by differentiable decision trees, it is necessary to first review\\nsome typical techniques of decision trees. Then, we discuss current LDL methods.\\nDecision trees. Random forests or randomized decision trees [16, 1, 3, 4], are a popular ensemble\\npredictive model suitable for many machine learning tasks. In the past, learning of a decision tree was\\nbased on heuristics such as a greedy algorithm where locally-optimal hard decisions are made at each\\nsplit node [1], and thus, cannot be integrated into in a deep learning framework, i.e., be combined\\nwith representation learning in an end-to-end manner.\\nThe newly proposed deep neural decision forests (dNDFs) [20] overcomes this problem by introducing\\na soft differentiable decision function at the split nodes and a global loss function defined on a tree.\\nThis ensures that the split node parameters can be learned by back-propagation and leaf node\\npredictions can be updated by a discrete iterative function.\\nOur method extends dNDFs to address LDL problems, but this extension is non-trivial, because\\nlearning leaf node predictions is a constrained convex optimization problem. Although a step-size\\nfree update function was given in dNDFs to update leaf node predictions, it was only proved to\\nconverge for a classification loss. Consequently, it was unclear how to obtain such an update function\\nfor other losses. We observed, however, that the update function in dNDFs can be derived from\\nvariational bounding, which allows us to extend it to our LDL loss. In addition, the strategies used in\\nLDLFs and dNDFs to learning the ensemble of multiple trees (forests) are different: 1) we explicitly\\ndefine a loss function for forests, while only the loss function for a single tree was defined in dNDFs;\\n2) we allow the split nodes from different trees to be connected to the same output unit of the feature\\nlearning function, while dNDFs did not; 3) all trees in LDLFs can be learned jointly, while trees in\\ndNDFs were learned alternatively. These changes in the ensemble learning are important, because as\\nshown in our experiments (Sec. 4.4), LDLFs can get better results by using more trees, but by using\\nthe ensemble strategy proposed in dNDFs, the results of forests are even worse than those for a single\\ntree.\\nTo sum up, w.r.t. dNDFs [20], the contributions of LDLFs are: first, we extend from classification [20]\\nto distribution learning by proposing a distribution-based loss for the forests and derive the gradient to\\nlearn splits nodes w.r.t. this loss; second, we derived the update function for leaf nodes by variational\\nbounding (having observed that the update function in [20] was a special case of variational\\nbounding); last but not the least, we propose above three strategies to learning the ensemble of\\nmultiple trees, which are different from [20], but we show are effective.\\nLabel distribution learning. A number of specialized algorithms have been proposed to address the\\nLDL task, and have shown their effectiveness in many computer vision applications, such as facial\\nage estimation [8, 11, 28], expression recognition [30] and hand orientation estimation [10].\\nGeng et al. [8] defined the label distribution for an instance as a vector containing the probabilities\\nof the instance having each label. They also gave a strategy to assign a proper label distribution\\nto an instance with a single label, i.e., assigning a Gaussian or Triangle distribution whose peak\\nis the single label, and proposed an algorithm called IIS-LLD, which is an iterative optimization\\nprocess based on a two-layer energy based model. Yang et al. [28] then defined a three-layer energy\\nbased model, called SCE-LDL, in which the ability to perform feature learning is improved by\\nadding the extra hidden layer and sparsity constraints are also incorporated to ameliorate the model.\\nGeng [6] developed an accelerated version of IIS-LLD, called BFGS-LDL, by using quasi-Newton\\noptimization. All the above LDL methods assume that the label distribution can be represented by a\\nmaximum entropy model [2], but the exponential part of this model restricts the generality of the\\ndistribution form.\\nAnother way to address the LDL task, is to extend existing learning algorithms to deal with label\\ndistributions. Geng and Hou [7] proposed LDSVR, a LDL method by extending support vector\\nregressor, which fit a sigmoid function to each component of the distribution simultaneously by a\\nsupport vector machine. Xing et al. [27] then extended boosting to address the LDL task by additive\\nweighted regressors. They showed that using the vector tree model as the weak regressor can lead to\\nbetter performance and named this method AOSO-LDLLogitBoost. As the learning of this tree model\\nis based on locally-optimal hard data partition functions at each split node, AOSO-LDLLogitBoost is\\nunable to be combined with representation learning. Extending current deep learning algorithms to\\n3\\n\\n\\x0caddress the LDL task is an interesting topic. But, the existing such a method, called DLDL [5], still\\nfocuses on maximum entropy model based LDL.\\nOur method, LDLFs, extends differentiable decision trees to address LDL tasks, in which the predicted\\nlabel distribution for a sample can be expressed by a linear combination of the label distributions\\nof the training data, and thus have no restrictions on the distributions (e.g., no requirement of the\\nmaximum entropy model). In addition, thanks to the introduction of differentiable decision functions,\\nLDLFs can be combined with representation learning, e.g., to learn deep features in an end-to-end\\nmanner.\\n\\n3\\n\\nLabel Distribution Learning Forests\\n\\nA forest is an ensemble of decision trees. We first introduce how to learn a single decision tree by\\nlabel distribution learning, then describe the learning of a forest.\\n3.1\\n\\nProblem Formulation\\n\\nLet X = Rm denote the input space and Y = {y1 , y2 , . . . , yC } denote the complete set of labels,\\nwhere C is the number of possible label values. We consider a label distribution learning (LDL)\\nproblem, where for each input sample x \\xe2\\x88\\x88 X , there is a label distribution d = (dxy1 , dyx2 , . . . , dyxC )> \\xe2\\x88\\x88\\nRC . Here dyxc expresses the probability of the sample x having the c-th label yc and thus has the\\nPC\\nconstraints that dyxc \\xe2\\x88\\x88 [0, 1] and c=1 dyxc = 1. The goal of the LDL problem is to learn a mapping\\nfunction g : x \\xe2\\x86\\x92 d between an input sample x and its corresponding label distribution d.\\nHere, we want to learn the mapping function g(x) by a decision tree based model T . A decision\\ntree consists of a set of split nodes N and a set of leaf nodes L. Each split node n \\xe2\\x88\\x88 N defines\\na split function sn (\\xc2\\xb7; \\xce\\x98) : X \\xe2\\x86\\x92 [0, 1] parameterized by \\xce\\x98 to determine whether a sample is sent\\nto the left or right subtree. Each leaf node ` \\xe2\\x88\\x88 L holds a distribution q` = (q`1 , q`2 , . . . , q`C )>\\nPC\\nover Y, i.e, q`c \\xe2\\x88\\x88 [0, 1] and c=1 q`c = 1. To build a differentiable decision tree, following [20],\\nwe use a probabilistic split function sn (x; \\xce\\x98) = \\xcf\\x83(f\\xcf\\x95(n) (x; \\xce\\x98)), where \\xcf\\x83(\\xc2\\xb7) is a sigmoid function,\\n\\xcf\\x95(\\xc2\\xb7) is an index function to bring the \\xcf\\x95(n)-th output of function f (x; \\xce\\x98) in correspondence with\\nsplit node n, and f : x \\xe2\\x86\\x92 RM is a real-valued feature learning function depending on the sample x\\nand the parameter \\xce\\x98, and can take any form. For a simple form, it can be a linear transformation\\nof x, where \\xce\\x98 is the transformation matrix; For a complex form, it can be a deep network to\\nperform representation learning in an end-to-end manner, then \\xce\\x98 is the network parameter. The\\ncorrespondence between the split nodes and the output units of function f , indicated by \\xcf\\x95(\\xc2\\xb7) that is\\nrandomly generated before tree learning, i.e., which output units from \\xe2\\x80\\x9cf \\xe2\\x80\\x9d are used for constructing a\\ntree is determined randomly. An example to demonstrate \\xcf\\x95(\\xc2\\xb7) is shown in Fig. 2. Then, the probability\\nof the sample x falling into leaf node ` is given by\\nY\\nl\\nr\\np(`|x; \\xce\\x98) =\\nsn (x; \\xce\\x98)1(`\\xe2\\x88\\x88Ln ) (1 \\xe2\\x88\\x92 sn (x; \\xce\\x98))1(`\\xe2\\x88\\x88Ln ) ,\\n(1)\\nn\\xe2\\x88\\x88N\\n\\nwhere 1(\\xc2\\xb7) is an indicator function and Lln and Lrn denote the sets of leaf nodes held by the left and\\nright subtrees of node n, Tnl and Tnr , respectively. The output of the tree T w.r.t. x, i.e., the mapping\\nfunction g, is defined by\\nX\\ng(x; \\xce\\x98, T ) =\\np(`|x; \\xce\\x98)q` .\\n(2)\\n`\\xe2\\x88\\x88L\\n\\n3.2\\n\\nTree Optimization\\n\\nGiven a training set S = {(xi , di )}N\\ni=1 , our goal is to learn a decision tree T described in Sec. 3.1\\nwhich can output a distribution g(xi ; \\xce\\x98, T ) similar to di for each sample xi . To this end, a\\nstraightforward way is to minimize the Kullback-Leibler (K-L) divergence between each g(xi ; \\xce\\x98, T )\\nand di , or equivalently to minimize the following cross-entropy loss:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\n1 X X yc\\ndxi log(gc (xi ; \\xce\\x98, T )) = \\xe2\\x88\\x92\\ndxi log\\np(`|xi ; \\xce\\x98)q`c , (3)\\nN i=1 c=1\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n4\\n\\n\\x0cwhere q denote the distributions held by all the leaf nodes L and gc (xi ; \\xce\\x98, T ) is the c-th output unit\\nof g(xi ; \\xce\\x98, T ). Learning the tree T requires the estimation of two parameters: 1) the split node\\nparameter \\xce\\x98 and 2) the distributions q held by the leaf nodes. The best parameters (\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) are\\ndetermined by\\n(\\xce\\x98\\xe2\\x88\\x97 , q\\xe2\\x88\\x97 ) = arg min R(q, \\xce\\x98; S).\\n(4)\\n\\xce\\x98,q\\n\\nTo solve Eqn. 4, we consider an alternating optimization strategy: First, we fix q and optimize\\n\\xce\\x98; Then, we fix \\xce\\x98 and optimize q. These two learning steps are alternatively performed, until\\nconvergence or a maximum number of iterations is reached (defined in the experiments).\\n3.2.1\\n\\nLearning Split Nodes\\n\\nIn this section, we describe how to learn the parameter \\xce\\x98 for split nodes, when the distributions held\\nby the leaf nodes q are fixed. We compute the gradient of the loss R(q, \\xce\\x98; S) w.r.t. \\xce\\x98 by the chain\\nrule:\\nN\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S) X X \\xe2\\x88\\x82R(q, \\xce\\x98; S) \\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n=\\n,\\n(5)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\ni=1\\nn\\xe2\\x88\\x88N\\n\\nwhere only the first term depends on the tree and the second term depends on the specific type of the\\nfunction f\\xcf\\x95(n) . The first term is given by\\nC\\n\\x01 gc (xi ; \\xce\\x98, Tnl ) \\x11\\ngc (xi ; \\xce\\x98, Tnr )\\n1 X yc \\x10\\n\\xe2\\x88\\x82R(q, \\xce\\x98; S)\\n=\\ndxi sn (xi ; \\xce\\x98)\\n\\xe2\\x88\\x92 1 \\xe2\\x88\\x92 sn (xi ; \\xce\\x98)\\n, (6)\\n\\xe2\\x88\\x82f\\xcf\\x95(n) (xi ; \\xce\\x98)\\nN c=1\\ngc (xi ; \\xce\\x98, T )\\ngc (xi ; \\xce\\x98, T )\\nP\\nP\\nwhere gc (xi ; \\xce\\x98, Tnl ) = `\\xe2\\x88\\x88Lln p(`|xi ; \\xce\\x98)q`c and g c (xi ; \\xce\\x98, Tnr ) = `\\xe2\\x88\\x88Lrn p(`|xi ; \\xce\\x98)q`c . Note that,\\nlet Tn be the tree rooted at the node n, then we have gc (xi ; \\xce\\x98, Tn ) = gc (xi ; \\xce\\x98, Tnl ) + gc (xi ; \\xce\\x98, Tnr ).\\nThis means the gradient computation in Eqn. 6 can be started at the leaf nodes and carried out in a\\nbottom up manner. Thus, the split node parameters can be learned by standard back-propagation.\\n\\n3.2.2\\n\\nLearning Leaf Nodes\\n\\nNow, fixing the parameter \\xce\\x98, we show how to learn the distributions held by the leaf nodes q, which\\nis a constrained optimization problem:\\nmin R(q, \\xce\\x98; S), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1.\\n\\n(7)\\n\\nc=1\\n\\nHere, we propose to address this constrained convex optimization problem by variational bounding [19, 29], which leads to a step-size free and fast-converged update rule for q. In variational\\nbounding, an original objective function to be minimized gets replaced by its bound in an iterative\\nmanner. A upper bound for the loss function R(q, \\xce\\x98; S) can be obtained by Jensen\\xe2\\x80\\x99s inequality:\\nR(q, \\xce\\x98; S) = \\xe2\\x88\\x92\\n\\nN C\\n\\x10X\\n\\x11\\n1 X X yc\\ndxi log\\np(`|xi ; \\xce\\x98)q`c\\nN i=1 c=1\\n`\\xe2\\x88\\x88L\\n\\n\\xe2\\x89\\xa4\\xe2\\x88\\x92\\nwhere \\xce\\xbe` (q`c , xi ) =\\n\\n1\\nN\\n\\nN X\\nC\\nX\\ni=1 c=1\\n\\np(`|xi ;\\xce\\x98)q`c\\ngc (xi ;\\xce\\x98,T )\\n\\n\\xcf\\x86(q, q\\xcc\\x84) = \\xe2\\x88\\x92\\n\\ndyxci\\n\\nX\\n\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n\\n`\\xe2\\x88\\x88L\\n\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\ni\\n`c\\n,\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(8)\\n\\n. We define\\n\\nN C\\n\\x10 p(`|x ; \\xce\\x98)q \\x11\\n1 X X yc X\\ni\\n`c\\ndxi\\n\\xce\\xbe` (q\\xcc\\x84`c , xi ) log\\n.\\nN i=1 c=1\\n\\xce\\xbe` (q\\xcc\\x84`c , xi )\\n\\n(9)\\n\\n`\\xe2\\x88\\x88L\\n\\nThen \\xcf\\x86(q, q\\xcc\\x84) is an upper bound for R(q, \\xce\\x98; S), which has the property that for any q and q\\xcc\\x84,\\n\\xcf\\x86(q, q\\xcc\\x84) \\xe2\\x89\\xa5 R(q, \\xce\\x98; S), and \\xcf\\x86(q, q) = R(q, \\xce\\x98; S). Assume that we are at a point q(t) corresponding\\nto the t-th iteration, then \\xcf\\x86(q, q(t) ) is an upper bound for R(q, \\xce\\x98; S). In the next iteration, q(t+1)\\nis chosen such that \\xcf\\x86(q(t+1) , q) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S), which implies R(q(t+1) , \\xce\\x98; S) \\xe2\\x89\\xa4 R(q(t) , \\xce\\x98; S).\\n5\\n\\n\\x0cConsequently, we can minimize \\xcf\\x86(q, q\\xcc\\x84) instead of R(q, \\xce\\x98; S) after ensuring that R(q(t) , \\xce\\x98; S) =\\n\\xcf\\x86(q(t) , q\\xcc\\x84), i.e., q\\xcc\\x84 = q(t) . So we have\\nq(t+1) = arg min \\xcf\\x86(q, q(t) ), s.t., \\xe2\\x88\\x80`,\\nq\\n\\nC\\nX\\n\\nq`c = 1,\\n\\n(10)\\n\\nc=1\\n\\nwhich leads to minimizing the Lagrangian defined by\\n\\xcf\\x95(q, q(t) ) = \\xcf\\x86(q, q(t) ) +\\n\\nX\\n\\n\\xce\\xbb` (\\n\\n`\\xe2\\x88\\x88L\\n\\nwhere \\xce\\xbb` is the Lagrange multiplier. By setting\\n\\xce\\xbb` =\\n(t+1)\\n\\nNote that, q`c\\n\\n(t+1)\\n\\n\\xe2\\x88\\x88 [0, 1] and\\n\\ndistributions held by the leaf nodes. The starting\\n(0)\\ndistribution: q`c = C1 .\\n3.3\\n\\nq`c \\xe2\\x88\\x92 1),\\n\\n(11)\\n\\nc=1\\n\\n\\xe2\\x88\\x82\\xcf\\x95(q,q(t) )\\n\\xe2\\x88\\x82q`c\\n\\nN C\\n1 X X yc\\n(t)\\n(t+1)\\nd \\xce\\xbe` (q`c , xi ) and q`c\\nN i=1 c=1 xi\\n\\nsatisfies that q`c\\n\\nC\\nX\\n\\n= 0, we have\\nPN yc\\n(t)\\ndxi \\xce\\xbe` (q`c , xi )\\n.\\n= PC i=1\\nPN yc\\n(t)\\n\\xce\\xbe\\n(q\\n,\\nx\\n)\\nd\\nx\\n`\\ni\\ni\\nc=1\\ni=1\\n`c\\n\\n(12)\\n\\n(t+1)\\n= 1. Eqn. 12 is the update scheme for\\nc=1 q`c\\n(0)\\npoint q` can be simply initialized by the uniform\\n\\nPC\\n\\nLearning a Forest\\n\\nA forest is an ensemble of decision trees F = {T1 , . . . , TK }. In the training stage, all trees in the\\nforest F use the same parameters \\xce\\x98 for feature learning function f (\\xc2\\xb7; \\xce\\x98) (but correspond to different\\noutput units of f assigned by \\xcf\\x95, see Fig. 2), but each tree has independent leaf node predictions\\nq. The loss function for a forest is given by averaging the loss functions for all individual trees:\\nPK\\n1\\nRF = K\\nk=1 RTk , where RTk is the loss function for tree Tk defined by Eqn. 3. To learn \\xce\\x98 by\\nfixing the leaf node predictions q of all the trees in the forest F, based on the derivation in Sec. 3.2\\nand referring to Fig. 2, we have\\nN K\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n1 XX X\\n\\xe2\\x88\\x82RF\\n\\xe2\\x88\\x82RTk\\n=\\n,\\n\\xe2\\x88\\x82\\xce\\x98\\nK i=1\\n\\xe2\\x88\\x82f\\xcf\\x95k (n) (xi ; \\xce\\x98)\\n\\xe2\\x88\\x82\\xce\\x98\\n\\n(13)\\n\\nk=1 n\\xe2\\x88\\x88Nk\\n\\nwhere Nk and \\xcf\\x95k (\\xc2\\xb7) are the split node set and the index function of Tk , respectively. Note that,\\nthe index function \\xcf\\x95k (\\xc2\\xb7) for each tree is randomly assigned before tree learning, and thus split\\nnodes correspond to a subset of output units of f . This strategy is similar to the random subspace\\nmethod [17], which increases the randomness in training to reduce the risk of overfitting.\\nAs for q, since each tree in the forest F has its own leaf node predictions q, we can update them\\nindependently by Eqn. 12, given by \\xce\\x98. For implementational convenience, we do not conduct this\\nupdate scheme on the whole dataset S but on a set of mini-batches B. The training procedure of a\\nLDLF is shown in Algorithm. 1.\\nAlgorithm 1 The training procedure of a LDLF.\\nRequire: S: training set, nB : the number of mini-batches to update q\\nInitialize \\xce\\x98 randomly and q uniformly, set B = {\\xe2\\x88\\x85}\\nwhile Not converge do\\nwhile |B| < nB do\\nRandomly select a mini-batch B from S\\nUpdateS\\n\\xce\\x98 by computing gradient (Eqn. 13) on B\\nB=B B\\nend while\\nUpdate q by iterating Eqn. 12 on B\\nB = {\\xe2\\x88\\x85}\\nend while\\nIn the testing stage, the output of the forest F is given by averaging the predictions from all the\\nPK\\n1\\nindividual trees: g(x; \\xce\\x98, F) = K\\nk=1 g(x; \\xce\\x98, Tk ).\\n6\\n\\n\\x0c4\\n\\nExperimental Results\\n\\nOur realization of LDLFs is based on \\xe2\\x80\\x9cCaffe\\xe2\\x80\\x9d [18]. It is modular and implemented as a standard\\nneural network layer. We can either use it as a shallow stand-alone model (sLDLFs) or integrate it\\nwith any deep networks (dLDLFs). We evaluate sLDLFs on different LDL tasks and compare it with\\nother stand-alone LDL methods. As dLDLFs can be learned from raw image data in an end-to-end\\nmanner, we verify dLDLFs on a computer vision application, i.e., facial age estimation. The default\\nsettings for the parameters of our forests are: tree number (5), tree depth (7), output unit number of\\nthe feature learning function (64), iteration times to update leaf node predictions (20), the number of\\nmini-batches to update leaf node predictions (100), maximum iteration (25000).\\n4.1\\n\\nComparison of sLDLFs to Stand-alone LDL Methods\\n\\nWe compare our shallow model sLDLFs with other state-of-the-art stand-alone LDL methods.\\nFor sLDLFs, the feature learning function f (x, \\xce\\x98) is a linear transformation of x, i.e., the i-th\\noutput unit fi (x, \\xce\\xb8i ) = \\xce\\xb8i> x, where \\xce\\xb8i is the i-th column of the transformation matrix \\xce\\x98. We\\nused 3 popular LDL datasets in [6], Movie, Human Gene and Natural Scene1 . The samples\\nin these 3 datasets are represented by numerical descriptors, and the ground truths for them are\\nthe rating distributions of crowd opinion on movies, the diseases distributions related to human\\ngenes and label distributions on scenes, such as plant, sky and cloud, respectively. The label\\ndistributions of these 3 datasets are mixture distributions, such as the rating distribution shown in\\nFig. 1(b). Following [7, 27], we use 6 measures to evaluate the performances of LDL methods,\\nwhich compute the average similarity/distance between the predicted rating distributions and the real\\nrating distributions, including 4 distance measures (K-L, Euclidean, S\\xcf\\x86rensen, Squared \\xcf\\x872 ) and two\\nsimilarity measures (Fidelity, Intersection).\\nWe evaluate our shallow model sLDLFs on these 3 datasets and compare it with other state-of-the-art\\nstand-alone LDL methods. The results of sLDLFs and the competitors are summarized in Table 1.\\nFor Movie we quote the results reported in [27], as the code of [27] is not publicly available. For the\\nresults of the others two, we run code that the authors had made available. In all case, following [27, 6],\\nwe split each dataset into 10 fixed folds and do standard ten-fold cross validation, which represents\\nthe result by \\xe2\\x80\\x9cmean\\xc2\\xb1standard deviation\\xe2\\x80\\x9d and matters less how training and testing data get divided.\\nAs can be seen from Table 1, sLDLFs perform best on all of the six measures.\\nTable 1: Comparison results on three LDL datasets [6]. \\xe2\\x80\\x9c\\xe2\\x86\\x91\\xe2\\x80\\x9d and \\xe2\\x80\\x9c\\xe2\\x86\\x93\\xe2\\x80\\x9d indicate the larger and the smaller\\nthe better, respectively.\\nDataset\\n\\nMethod\\n\\nK-L \\xe2\\x86\\x93\\n\\nEuclidean \\xe2\\x86\\x93\\n\\nS\\xcf\\x86rensen \\xe2\\x86\\x93\\n\\nSquared \\xcf\\x872 \\xe2\\x86\\x93\\n\\nFidelity \\xe2\\x86\\x91\\n\\nIntersection \\xe2\\x86\\x91\\n\\nMovie\\n\\nsLDLF (ours)\\nAOSO-LDLogitBoost [27]\\nLDLogitBoost [27]\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.073\\xc2\\xb10.005\\n0.086\\xc2\\xb10.004\\n0.090\\xc2\\xb10.004\\n0.092\\xc2\\xb10.005\\n0.099\\xc2\\xb10.004\\n0.129\\xc2\\xb10.007\\n\\n0.133\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.159\\xc2\\xb10.003\\n0.158\\xc2\\xb10.004\\n0.167\\xc2\\xb10.004\\n0.187\\xc2\\xb10.004\\n\\n0.130\\xc2\\xb10.003\\n0.152\\xc2\\xb10.003\\n0.155\\xc2\\xb10.003\\n0.156\\xc2\\xb10.004\\n0.164\\xc2\\xb10.003\\n0.183\\xc2\\xb10.004\\n\\n0.070\\xc2\\xb10.004\\n0.084\\xc2\\xb10.003\\n0.088\\xc2\\xb10.003\\n0.088\\xc2\\xb10.004\\n0.096\\xc2\\xb10.004\\n0.120\\xc2\\xb10.005\\n\\n0.981\\xc2\\xb10.001\\n0.978\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.977\\xc2\\xb10.001\\n0.974\\xc2\\xb10.001\\n0.967\\xc2\\xb10.001\\n\\n0.870\\xc2\\xb10.003\\n0.848\\xc2\\xb10.003\\n0.845\\xc2\\xb10.003\\n0.844\\xc2\\xb10.004\\n0.836\\xc2\\xb10.003\\n0.817\\xc2\\xb10.004\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.228\\xc2\\xb10.006\\n0.245\\xc2\\xb10.019\\n0.231\\xc2\\xb10.021\\n0.239\\xc2\\xb10.018\\n\\n0.085\\xc2\\xb10.002\\n0.099\\xc2\\xb10.005\\n0.076\\xc2\\xb10.006\\n0.089\\xc2\\xb10.006\\n\\n0.212\\xc2\\xb10.002\\n0.229\\xc2\\xb10.015\\n0.231\\xc2\\xb10.012\\n0.253\\xc2\\xb10.009\\n\\n0.179\\xc2\\xb10.004\\n0.189\\xc2\\xb10.021\\n0.211\\xc2\\xb10.018\\n0.205\\xc2\\xb10.012\\n\\n0.948\\xc2\\xb10.001\\n0.940\\xc2\\xb10.006\\n0.938\\xc2\\xb10.008\\n0.944\\xc2\\xb10.003\\n\\n0.788\\xc2\\xb10.002\\n0.771\\xc2\\xb10.015\\n0.769\\xc2\\xb10.012\\n0.747\\xc2\\xb10.009\\n\\nsLDLF (ours)\\nLDSVR [7]\\nBFGS-LDL [6]\\nIIS-LDL [11]\\n\\n0.534\\xc2\\xb10.013\\n0.852\\xc2\\xb10.023\\n0.856\\xc2\\xb10.061\\n0.879\\xc2\\xb10.023\\n\\n0.317\\xc2\\xb10.014\\n0.511\\xc2\\xb10.021\\n0.475\\xc2\\xb10.029\\n0.458\\xc2\\xb10.014\\n\\n0.336\\xc2\\xb10.010\\n0.492\\xc2\\xb10.016\\n0.508\\xc2\\xb10.026\\n0.539\\xc2\\xb10.011\\n\\n0.448\\xc2\\xb10.017\\n0.595\\xc2\\xb10.026\\n0.716\\xc2\\xb10.041\\n0.792\\xc2\\xb10.019\\n\\n0.824\\xc2\\xb10.008\\n0.813\\xc2\\xb10.008\\n0.722\\xc2\\xb10.021\\n0.686\\xc2\\xb10.009\\n\\n0.664\\xc2\\xb10.010\\n0.509\\xc2\\xb10.016\\n0.492\\xc2\\xb10.026\\n0.461\\xc2\\xb10.011\\n\\nHuman Gene\\n\\nNatural Scene\\n\\n4.2\\n\\nEvaluation of dLDLFs on Facial Age Estimation\\n\\nIn some literature [8, 11, 28, 15, 5], age estimation is formulated as a LDL problem. We conduct\\nfacial age estimation experiments on Morph [24], which contains more than 50,000 facial images\\nfrom about 13,000 people of different races. Each facial image is annotated with a chronological age.\\nTo generate an age distribution for each face image, we follow the same strategy used in [8, 28, 5],\\nwhich uses a Gaussian distribution whose mean is the chronological age of the face image (Fig. 1(a)).\\nThe predicted age for a face image is simply the age having the highest probability in the predicted\\n1\\n\\nWe download these datasets from http://cse.seu.edu.cn/people/xgeng/LDL/index.htm.\\n\\n7\\n\\n\\x0clabel distribution. The performance of age estimation is evaluated by the mean absolute error (MAE)\\nbetween predicted ages and chronological ages. As the current state-of-the-art result on Morph\\nis obtain by fine-tuning DLDL [5] on VGG-Face [23], we also build a dLDLF on VGG-Face, by\\nreplacing the softmax layer in VGGNet by a LDLF. Following [5], we do standard 10 ten-fold cross\\nvalidation and the results are summarized in Table. 2, which shows dLDLF achieve the state-of-the-art\\nperformance on Morph. Note that, the significant performance gain between deep LDL models\\n(DLDL and dLDLF) and non-deep LDL models (IIS-LDL, CPNN, BFGS-LDL) and the superiority\\nof dLDLF compared with DLDL verifies the effectiveness of end-to-end learning and our tree-based\\nmodel for LDL, respectively.\\nTable 2: MAE of age estimation comparison on Morph [24].\\nMethod\\n\\nIIS-LDL [11]\\n\\nCPNN [11]\\n\\nBFGS-LDL [6]\\n\\nDLDL+VGG-Face [5]\\n\\ndLDLF+VGG-Face (ours)\\n\\nMAE\\n\\n5.67\\xc2\\xb10.15\\n\\n4.87\\xc2\\xb10.31\\n\\n3.94\\xc2\\xb10.05\\n\\n2.42\\xc2\\xb10.01\\n\\n2.24\\xc2\\xb10.02\\n\\nAs the distribution of gender and ethnicity is very unbalanced in Morph, many age estimation methods [13, 14, 15] are evaluated on a subset of Morph, called Morph_Sub for short, which consists of\\n20,160 selected facial images to avoid the influence of unbalanced distribution. The best performance\\nreported on Morph_Sub is given by D2LDL [15], a data-dependent LDL method. As D2LDL used\\nthe output of the \\xe2\\x80\\x9cfc7\\xe2\\x80\\x9d layer in AlexNet [21] as the face image features, here we integrate a LDLF\\nwith AlexNet. Following the experiment setting used in D2LDL, we evaluate our dLDLF and the\\ncompetitors, including both SLL and LDL based methods, under six different training set ratios (10%\\nto 60%). All of the competitors are trained on the same deep features used by D2LDL. As can be\\nseen from Table 3, our dLDLFs significantly outperform others for all training set ratios.\\nNote that, the generated age distri- Figure 3: MAE of age estimation comparison on\\nbutions are unimodal distributions Morph_Sub.\\nand the label distributions used in\\nTraining set ratio\\nMethod\\nSec. 4.1 are mixture distributions.\\n10%\\n20%\\n30%\\n40%\\n50%\\n60%\\nThe proposed method LDLFs achieve\\nAAS [22]\\n4.9081\\n4.7616\\n4.6507\\n4.5553\\n4.4690\\n4.4061\\nthe state-of-the-art results on both of\\nLARR [12]\\n4.7501\\n4.6112\\n4.5131\\n4.4273\\n4.3500\\n4.2949\\nIIS-ALDL [9]\\n4.1791\\n4.1683\\n4.1228\\n4.1107\\n4.1024\\n4.0902\\nthem, which verifies that our model\\nD2LDL [15]\\n4.1080\\n3.9857\\n3.9204\\n3.8712\\n3.8560\\n3.8385\\nhas the ability to model any general\\ndLDLF (ours)\\n3.8495\\n3.6220\\n3.3991\\n3.2401\\n3.1917\\n3.1224\\nform of label distributions.\\n4.3\\n\\nTime Complexity\\n\\nLet h and sB be the tree depth and the\\nbatch size, respectively. Each tree has 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1 split nodes and 2h\\xe2\\x88\\x921 leaf nodes. Let D = 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1.\\nFor one tree and one sample, the complexity of a forward pass and a backward pass are O(D +\\nD + 1\\xc3\\x97C) = O(D\\xc3\\x97C) and O(D + 1\\xc3\\x97C + D\\xc3\\x97C) = O(D\\xc3\\x97C), respectively. So for K trees and\\nnB batches, the complexity of a forward and backward pass is O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ). The complexity of an iteration to update leaf nodes are O(nB \\xc3\\x97sB \\xc3\\x97K\\xc3\\x97C\\xc3\\x97D + 1) = O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ).\\nThus, the complexity for the training procedure (one epoch, nB batches) and the testing procedure\\n(one sample) are O(D\\xc3\\x97C\\xc3\\x97K\\xc3\\x97nB \\xc3\\x97sB ) and O(D\\xc3\\x97C\\xc3\\x97K), respectively. LDLFs are efficient: On\\nMorph_Sub (12636 training images, 8424 testing images), our model only takes 5250s for training\\n(25000 iterations) and 8s for testing all 8424 images.\\n4.4\\n\\nParameter Discussion\\n\\nNow we discuss the influence of parameter settings on performance. We report the results of rating\\nprediction on Movie (measured by K-L) and age estimation on Morph_Sub with 60% training set\\nratio (measured by MAE) for different parameter settings in this section.\\nTree number. As a forest is an ensemble model, it is necessary to investigate how performances\\nchange by varying the tree number used in a forest. Note that, as we discussed in Sec. 2, the\\nensemble strategy to learn a forest proposed in dNDFs [20] is different from ours. Therefore, it is\\nnecessary to see which ensemble strategy is better to learn a forest. Towards this end, we replace our\\nensemble strategy in dLDLFs by the one used in dNDFs, and name this method dNDFs-LDL. The\\ncorresponding shallow model is named by sNDFs-LDL. We fix other parameters, i.e., tree depth and\\n8\\n\\n\\x0coutput unit number of the feature learning function, as the default setting. As shown in Fig. 4 (a), our\\nensemble strategy can improve the performance by using more trees, while the one used in dNDFs\\neven leads to a worse performance than one for a single tree.\\nObserved from Fig. 4, the performance of LDLFs can be improved by using more trees, but the\\nimprovement becomes increasingly smaller and smaller. Therefore, using much larger ensembles\\ndoes not yield a big improvement (On Movie, the number of trees K = 100: K-L = 0.070 vs K = 20:\\nK-L = 0.071). Note that, not all random forests based methods use a large number of trees, e.g.,\\nShotton et al. [25] obtained very good pose estimation results from depth images by only 3 decision\\ntrees.\\nTree depth. Tree depth is another important parameter for decision trees. In LDLFs, there is an\\nimplicit constraint between tree depth h and output unit number of the feature learning function \\xcf\\x84 :\\n\\xcf\\x84 \\xe2\\x89\\xa5 2h\\xe2\\x88\\x921 \\xe2\\x88\\x92 1. To discuss the influence of tree depth to the performance of dLDLFs, we set \\xcf\\x84 = 2h\\xe2\\x88\\x921\\nand fix tree number K = 1, and the performance change by varying tree depth is shown in Fig. 4 (b).\\nWe see that the performance first improves then decreases with the increase of the tree depth. The\\nreason is as the tree depth increases, the dimension of learned features increases exponentially, which\\ngreatly increases the training difficulty. So using much larger depths may lead to bad performance\\n(On Movie, tree depth h = 18: K-L = 0.1162 vs h = 9: K-L = 0.0831).\\n\\nFigure 4: The performance change of age estimation on Morph_Sub and rating prediction on Movie\\nby varying (a) tree number and (b) tree depth. Our approach (dLDLFs/sLDLFs) can improve the\\nperformance by using more trees, while using the ensemble strategy proposed in dNDFs (dNDFsLDL/sNDFs-LDL) even leads to a worse performance than one for a single tree.\\n\\n5\\n\\nConclusion\\n\\nWe present label distribution learning forests, a novel label distribution learning algorithm inspired by\\ndifferentiable decision trees. We defined a distribution-based loss function for the forests and found\\nthat the leaf node predictions can be optimized via variational bounding, which enables all the trees\\nand the feature they use to be learned jointly in an end-to-end manner. Experimental results showed\\nthe superiority of our algorithm for several LDL tasks and a related computer vision application, and\\nverified our model has the ability to model any general form of label distributions.\\nAcknowledgement. This work was supported in part by the National Natural Science Foundation of\\nChina No. 61672336, in part by \\xe2\\x80\\x9cChen Guang\\xe2\\x80\\x9d project supported by Shanghai Municipal Education\\nCommission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR\\nN00014-15-1-2356.\\n\\nReferences\\n[1] Y. Amit and D. Geman. Shape quantization and recognition with randomized trees. Neural Computation,\\n9(7):1545\\xe2\\x80\\x931588, 1997.\\n[2] A. L. Berger, S. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing.\\nComputational Linguistics, 22(1):39\\xe2\\x80\\x9371, 1996.\\n[3] L. Breiman. Random forests. Machine Learning, 45(1):5\\xe2\\x80\\x9332, 2001.\\n[4] A. Criminisi and J. Shotton. Decision Forests for Computer Vision and Medical Image Analysis. Springer,\\n2013.\\n[5] B.-B. Gao, C. Xing, C.-W. Xie, J. Wu, and X. Geng. Deep label distribution learning with label ambiguity.\\narXiv:1611.01731, 2017.\\n[6] X. Geng. Label distribution learning. IEEE Trans. Knowl. Data Eng., 28(7):1734\\xe2\\x80\\x931748, 2016.\\n\\n9\\n\\n\\x0c[7] X. Geng and P. Hou. Pre-release prediction of crowd opinion on movies by label distribution learning. In\\nPro. IJCAI, pages 3511\\xe2\\x80\\x933517, 2015.\\n[8] X. Geng, K. Smith-Miles, and Z. Zhou. Facial age estimation by learning from label distributions. In Proc.\\nAAAI, 2010.\\n[9] X. Geng, Q. Wang, and Y. Xia. Facial age estimation by adaptive label distribution learning. In Proc.\\nICPR, pages 4465\\xe2\\x80\\x934470, 2014.\\n[10] X. Geng and Y. Xia. Head pose estimation based on multivariate label distribution. In Proc. CVPR, pages\\n1837\\xe2\\x80\\x931842, 2014.\\n[11] X. Geng, C. Yin, and Z. Zhou. Facial age estimation by learning from label distributions. IEEE Trans.\\nPattern Anal. Mach. Intell., 35(10):2401\\xe2\\x80\\x932412, 2013.\\n[12] G. Guo, Y. Fu, C. R. Dyer, and T. S. Huang. Image-based human age estimation by manifold learning and\\nlocally adjusted robust regression. IEEE Trans. Image Processing, 17(7):1178\\xe2\\x80\\x931188, 2008.\\n[13] G. Guo and G. Mu. Human age estimation: What is the influence across race and gender? In CVPR\\nWorkshops, pages 71\\xe2\\x80\\x9378, 2010.\\n[14] G. Guo and C. Zhang. A study on cross-population age estimation. In Proc. CVPR, pages 4257\\xe2\\x80\\x934263,\\n2014.\\n[15] Z. He, X. Li, Z. Zhang, F. Wu, X. Geng, Y. Zhang, M.-H. Yang, and Y. Zhuang. Data-dependent label\\ndistribution learning for age estimation. IEEE Trans. on Image Processing, 2017.\\n[16] T. K. Ho. Random decision forests. In Proc. ICDAR, pages 278\\xe2\\x80\\x93282, 1995.\\n[17] T. K. Ho. The random subspace method for constructing decision forests. IEEE Trans. Pattern Anal. Mach.\\nIntell., 20(8):832\\xe2\\x80\\x93844, 1998.\\n[18] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe:\\nConvolutional architecture for fast feature embedding. arXiv preprint arXiv:1408.5093, 2014.\\n[19] M. I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K. Saul. An introduction to variational methods for\\ngraphical models. Machine Learning, 37(2):183\\xe2\\x80\\x93233, 1999.\\n[20] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bul\\xc3\\xb2. Deep neural decision forests. In Proc. ICCV,\\npages 1467\\xe2\\x80\\x931475, 2015.\\n[21] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural\\nnetworks. In Proc. NIPS, pages 1106\\xe2\\x80\\x931114, 2012.\\n[22] A. Lanitis, C. Draganova, and C. Christodoulou. Comparing different classifiers for automatic age\\nestimation. IEEE Trans. on Cybernetics,, 34(1):621\\xe2\\x80\\x93628, 2004.\\n[23] O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In Proc. BMVC, pages 41.1\\xe2\\x80\\x9341.12,\\n2015.\\n[24] K. Ricanek and T. Tesafaye. MORPH: A longitudinal image database of normal adult age-progression. In\\nProc. FG, pages 341\\xe2\\x80\\x93345, 2006.\\n[25] J. Shotton, A. W. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A. Kipman, and A. Blake.\\nReal-time human pose recognition in parts from single depth images. In Proc. CVPR, pages 1297\\xe2\\x80\\x931304,\\n2011.\\n[26] G. Tsoumakas and I. Katakis. Multi-label classification: An overview. International Journal of Data\\nWarehousing and Mining, 3(3):1\\xe2\\x80\\x9313, 2007.\\n[27] C. Xing, X. Geng, and H. Xue. Logistic boosting regression for label distribution learning. In Proc. CVPR,\\npages 4489\\xe2\\x80\\x934497, 2016.\\n[28] X. Yang, X. Geng, and D. Zhou. Sparsity conditional energy label distribution learning for age estimation.\\nIn Proc. IJCAI, pages 2259\\xe2\\x80\\x932265, 2016.\\n[29] A. L. Yuille and A. Rangarajan. The concave-convex procedure. Neural Computation, 15(4):915\\xe2\\x80\\x93936,\\n2003.\\n[30] Y. Zhou, H. Xue, and X. Geng. Emotion distribution recognition from facial expressions. In Proc. MM,\\npages 1247\\xe2\\x80\\x931250, 2015.\\n\\n10\\n\\n\\x0c']], shape=(1, 1), dtype=string)\n",
      "Preprocessed:  tf.Tensor([[b'   l       b       e       l       d       r       b       u       n       l       e       r       n       n       g       f       r       e       w       e       s       h       e       n       1       2       k       z       h       1       y       l       u       g       u       1       a       l       n       y       u       l       l       e       2       k       e       l       b       r       r       f       s       p       e       c       l       f       b       e       r       o       p       c       n       o       p       c       l       a       c       c       e       n       e       w       r       k       s       h       n       g       h       i       n       u       e       f       r       a       v       n       c       e       c       u       n       c       n       n       d       s       c       e       n       c       e       s       c       h       l       f       c       u       n       c       n       n       i       n       f       r       n       e       n       g       n       e       e       r       n       g       s       h       n       g       h       u       n       v       e       r       2       d       e       p       r       e       n       f       c       p       u       e       r       s       c       e       n       c       e       j       h       n       h       p       k       n       u       n       v       e       r       r       x       v       1       7       0       2       0       6       0       8       6       v       4       c       l       g       1       6       o       c       2       0       1       7       1       h       e       n       w       e       1       2       3       1       z       h       k       1       2       0       6       g       l       l       u       n       0       l       n       l       u       l       l       e       g       l       c       a       b       r       c       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       g       e       n       e       r       l       l       e       r       n       n       g       f       r       e       w       r       k       w       h       c       h       g       n       n       n       n       c       e       r       b       u       n       v       e       r       e       f       l       b       e       l       r       h       e       r       h       n       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       c       u       r       r       e       n       l       d       l       e       h       h       v       e       e       h       e       r       r       e       r       c       e       u       p       n       n       h       e       e       x       p       r       e       n       f       r       f       h       e       l       b       e       l       r       b       u       n       r       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       t       h       p       p       e       r       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       d       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       b       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       h       c       h       h       v       e       e       v       e       r       l       v       n       g       e       1       d       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       l       e       f       n       e       p       r       e       c       n       2       t       h       e       l       e       r       n       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       f       r       e       e       n       b       l       n       g       l       l       h       e       r       e       e       b       e       l       e       r       n       e       j       n       l       n       h       w       h       n       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       p       r       e       c       n       w       h       c       h       g       u       r       n       e       e       r       c       e       c       r       e       e       f       h       e       l       f       u       n       c       n       c       n       b       e       e       r       v       e       b       v       r       n       l       b       u       n       n       g       t       h       e       e       f       f       e       c       v       e       n       e       f       h       e       p       r       p       e       l       d       l       f       v       e       r       f       e       n       e       v       e       r       l       l       d       l       k       n       c       p       u       e       r       v       n       p       p       l       c       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       d       l       e       h       1       i       n       r       u       c       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       6       1       1       l       e       r       n       n       g       f       r       e       w       r       k       e       l       w       h       p       r       b       l       e       f       l       b       e       l       b       g       u       u       n       l       k       e       n       g       l       e       l       b       e       l       l       e       r       n       n       g       s       l       l       n       u       l       l       b       e       l       l       e       r       n       n       g       m       l       l       2       6       w       h       c       h       u       e       n       n       n       c       e       g       n       e       n       g       l       e       l       b       e       l       r       u       l       p       l       e       l       b       e       l       l       d       l       l       e       r       n       n       g       h       e       r       e       l       v       e       p       r       n       c       e       f       e       c       h       l       b       e       l       n       v       l       v       e       n       h       e       e       c       r       p       n       f       n       n       n       c       e       e       r       b       u       n       v       e       r       h       e       e       f       l       b       e       l       s       u       c       h       l       e       r       n       n       g       r       e       g       u       b       l       e       f       r       n       r       e       l       w       r       l       p       r       b       l       e       w       h       c       h       h       v       e       l       b       e       l       b       g       u       a       n       e       x       p       l       e       f       c       l       g       e       e       n       8       e       v       e       n       h       u       n       c       n       n       p       r       e       c       h       e       p       r       e       c       e       g       e       f       r       n       g       l       e       f       c       l       g       e       t       h       e       h       h       e       p       e       r       n       p       r       b       b       l       n       n       e       g       e       g       r       u       p       n       l       e       l       k       e       l       b       e       n       n       h       e       r       h       e       n       c       e       r       e       n       u       r       l       g       n       r       b       u       n       f       g       e       l       b       e       l       e       c       h       f       c       l       g       e       f       g       1       n       e       f       u       n       g       n       g       l       e       g       e       l       b       e       l       a       n       h       e       r       e       x       p       l       e       v       e       r       n       g       p       r       e       c       n       7       m       n       f       u       v       e       r       e       v       e       w       w       e       b       e       u       c       h       n       e       f       l       x       i       m       d       b       n       d       u       b       n       p       r       v       e       c       r       w       p       n       n       f       r       e       c       h       v       e       p       e       c       f       e       b       h       e       r       b       u       n       f       r       n       g       c       l       l       e       c       e       f       r       h       e       r       u       e       r       f       g       1       b       i       f       e       c       u       l       p       r       e       c       e       l       p       r       e       c       u       c       h       r       n       g       r       b       u       n       f       r       e       v       e       r       v       e       b       e       f       r       e       r       e       l       e       e       v       e       p       r       u       c       e       r       c       n       r       e       u       c       e       h       e       r       n       v       e       e       n       r       k       n       h       e       u       e       n       c       e       c       n       b       e       e       r       c       h       e       w       h       c       h       v       e       w       c       h       m       n       l       d       l       e       h       u       e       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       n       l       e       r       n       b       p       z       n       g       n       e       n       e       r       g       f       u       n       c       n       b       e       n       h       e       e       l       8       1       1       2       8       6       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       e       g       h       f       f       c       u       l       n       r       e       p       r       e       e       n       n       g       x       u       r       e       r       b       u       n       s       e       h       e       r       l       d       l       e       h       e       x       e       n       h       e       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       g       b       b       n       g       n       u       p       p       r       v       e       c       r       r       e       g       r       e       n       e       l       w       h       l       b       e       l       r       b       u       n       7       2       7       w       h       c       h       v       k       n       g       h       u       p       n       b       u       h       v       e       l       n       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       h       e       n       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       1       c       n       f       e       r       e       n       c       e       n       n       e       u       r       l       i       n       f       r       n       p       r       c       e       n       g       s       e       n       i       p       s       2       0       1       7       l       n       g       b       e       c       h       c       a       u       s       a       x       0       c       f       g       u       r       e       1       t       h       e       r       e       l       w       r       l       w       h       c       h       r       e       u       b       l       e       b       e       e       l       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       e       e       f       c       l       g       e       u       n       l       r       b       u       n       b       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       u       l       l       r       b       u       n       i       n       h       p       p       e       r       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       l       d       l       f       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       2       0       e       x       e       n       n       g       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       e       l       w       h       h       e       l       d       l       k       h       w       v       n       g       e       o       n       e       h       e       c       n       r       e       e       h       v       e       h       e       p       e       n       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       b       x       u       r       e       f       h       e       l       e       f       n       e       p       r       e       c       n       w       h       c       h       v       k       n       g       r       n       g       u       p       n       n       h       e       f       r       f       h       e       l       b       e       l       r       b       u       n       t       h       e       e       c       n       h       h       e       p       l       n       e       p       r       e       e       r       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       w       h       c       h       e       n       b       l       e       c       b       n       n       f       r       e       e       l       e       r       n       n       g       n       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       r       e       e       b       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       v       e       r       g       e       n       c       e       k       l       b       e       w       e       e       n       h       e       g       r       u       n       r       u       h       l       b       e       l       r       b       u       n       n       h       e       r       b       u       n       p       r       e       c       e       b       h       e       r       e       e       b       f       x       n       g       p       l       n       e       w       e       h       w       h       h       e       p       z       n       f       l       e       f       n       e       p       r       e       c       n       n       z       e       h       e       l       f       u       n       c       n       f       h       e       r       e       e       c       n       b       e       r       e       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       n       w       h       c       h       h       e       r       g       n       l       l       f       u       n       c       n       b       e       n       z       e       g       e       e       r       v       e       l       r       e       p       l       c       e       b       e       c       r       e       n       g       e       q       u       e       n       c       e       f       u       p       p       e       r       b       u       n       f       l       l       w       n       g       h       p       z       n       r       e       g       w       e       e       r       v       e       c       r       e       e       e       r       v       e       f       u       n       c       n       u       p       e       h       e       l       e       f       n       e       p       r       e       c       n       t       l       e       r       n       f       r       e       w       e       v       e       r       g       e       h       e       l       e       f       l       l       h       e       n       v       u       l       r       e       e       b       e       h       e       l       f       r       h       e       f       r       e       n       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       i       n       h       w       h       e       p       l       n       e       p       r       e       e       r       f       l       l       h       e       n       v       u       l       r       e       e       c       n       b       e       l       e       r       n       e       j       n       l       o       u       r       l       d       l       f       c       n       b       e       u       e       h       l       l       w       n       l       n       e       e       l       n       c       n       l       b       e       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       e       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       c       n       b       e       l       n       e       r       r       n       f       r       n       n       e       e       p       n       e       w       r       k       r       e       p       e       c       v       e       l       f       g       2       l       l       u       r       e       k       e       c       h       c       h       r       f       u       r       l       d       l       f       w       h       e       r       e       f       r       e       c       n       f       w       r       e       e       h       w       n       w       e       v       e       r       f       h       e       e       f       f       e       c       v       e       n       e       f       u       r       e       l       n       e       v       e       r       l       l       d       l       k       u       c       h       c       r       w       p       n       n       p       r       e       c       n       n       v       e       n       e       e       p       r       e       c       n       b       e       n       h       u       n       g       e       n       e       w       e       l       l       n       e       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       h       w       n       g       g       n       f       c       n       p       r       v       e       e       n       h       e       e       f       h       e       r       l       d       l       e       h       t       h       e       l       b       e       l       r       b       u       n       f       r       h       e       e       k       n       c       l       u       e       b       h       u       n       l       r       b       u       n       e       g       h       e       g       e       r       b       u       n       n       f       g       1       n       x       u       r       e       r       b       u       n       h       e       r       n       g       r       b       u       n       n       v       e       n       f       g       1       b       t       h       e       u       p       e       r       r       f       u       r       e       l       n       b       h       f       h       e       v       e       r       f       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       f       g       u       r       e       2       i       l       l       u       r       n       f       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       t       h       e       p       c       r       c       l       e       e       n       e       h       e       u       p       u       u       n       f       h       e       f       u       n       c       n       f       p       r       e       e       r       z       e       b       x       c       e       x       9       8       w       h       c       h       c       n       b       e       f       e       u       r       e       v       e       c       r       r       f       u       l       l       c       n       n       e       c       e       l       e       r       f       e       e       p       n       e       w       r       k       t       h       e       b       l       u       e       n       g       r       e       e       n       c       r       c       l       e       r       e       p       l       n       e       n       l       e       f       n       e       r       e       p       e       c       v       e       l       t       w       n       e       x       f       u       n       c       n       x       c       f       x       9       5       1       n       x       c       f       x       9       5       2       r       e       g       n       e       h       e       e       w       r       e       e       r       e       p       e       c       v       e       l       t       h       e       b       l       c       k       h       r       r       w       n       c       e       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       f       h       e       e       w       r       e       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       e       h       n       e       u       p       u       u       n       c       r       r       e       p       n       h       e       p       l       n       e       b       e       l       n       g       n       g       f       f       e       r       e       n       r       e       e       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       e       n       e       b       h       g       r       n       l       e       f       n       e       t       h       e       u       p       u       f       h       e       f       r       e       x       u       r       e       f       h       e       r       e       e       p       r       e       c       n       f       x       c       2       x       b       7       x       c       e       x       9       8       n       q       r       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       2       x       0       c       2       r       e       l       e       w       r       k       s       n       c       e       u       r       l       d       l       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       n       e       c       e       r       f       r       r       e       v       e       w       e       p       c       l       e       c       h       n       q       u       e       f       e       c       n       r       e       e       t       h       e       n       w       e       c       u       c       u       r       r       e       n       l       d       l       e       h       d       e       c       n       r       e       e       r       n       f       r       e       r       r       n       z       e       e       c       n       r       e       e       1       6       1       3       4       r       e       p       p       u       l       r       e       n       e       b       l       e       p       r       e       c       v       e       e       l       u       b       l       e       f       r       n       c       h       n       e       l       e       r       n       n       g       k       i       n       h       e       p       l       e       r       n       n       g       f       e       c       n       r       e       e       w       b       e       n       h       e       u       r       c       u       c       h       g       r       e       e       l       g       r       h       w       h       e       r       e       l       c       l       l       p       l       h       r       e       c       n       r       e       e       e       c       h       p       l       n       e       1       n       h       u       c       n       n       b       e       n       e       g       r       e       n       n       e       e       p       l       e       r       n       n       g       f       r       e       w       r       k       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       t       h       e       n       e       w       l       p       r       p       e       e       e       p       n       e       u       r       l       e       c       n       f       r       e       n       d       f       2       0       v       e       r       c       e       h       p       r       b       l       e       b       n       r       u       c       n       g       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       h       e       p       l       n       e       n       g       l       b       l       l       f       u       n       c       n       e       f       n       e       n       r       e       e       t       h       e       n       u       r       e       h       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       b       c       k       p       r       p       g       n       n       l       e       f       n       e       p       r       e       c       n       c       n       b       e       u       p       e       b       c       r       e       e       e       r       v       e       f       u       n       c       n       o       u       r       e       h       e       x       e       n       n       d       f       r       e       l       d       l       p       r       b       l       e       b       u       h       e       x       e       n       n       n       n       r       v       l       b       e       c       u       e       l       e       r       n       n       g       l       e       f       n       e       p       r       e       c       n       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       a       l       h       u       g       h       e       p       z       e       f       r       e       e       u       p       e       f       u       n       c       n       w       g       v       e       n       n       n       d       f       u       p       e       l       e       f       n       e       p       r       e       c       n       w       n       l       p       r       v       e       c       n       v       e       r       g       e       f       r       c       l       f       c       n       l       c       n       e       q       u       e       n       l       w       u       n       c       l       e       r       h       w       b       n       u       c       h       n       u       p       e       f       u       n       c       n       f       r       h       e       r       l       e       w       e       b       e       r       v       e       h       w       e       v       e       r       h       h       e       u       p       e       f       u       n       c       n       n       n       d       f       c       n       b       e       e       r       v       e       f       r       v       r       n       l       b       u       n       n       g       w       h       c       h       l       l       w       u       e       x       e       n       u       r       l       d       l       l       i       n       n       h       e       r       e       g       e       u       e       n       l       d       l       f       n       n       d       f       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       f       r       e       r       e       f       f       e       r       e       n       1       w       e       e       x       p       l       c       l       e       f       n       e       l       f       u       n       c       n       f       r       f       r       e       w       h       l       e       n       l       h       e       l       f       u       n       c       n       f       r       n       g       l       e       r       e       e       w       e       f       n       e       n       n       d       f       2       w       e       l       l       w       h       e       p       l       n       e       f       r       f       f       e       r       e       n       r       e       e       b       e       c       n       n       e       c       e       h       e       e       u       p       u       u       n       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       w       h       l       e       n       d       f       n       3       l       l       r       e       e       n       l       d       l       f       c       n       b       e       l       e       r       n       e       j       n       l       w       h       l       e       r       e       e       n       n       d       f       w       e       r       e       l       e       r       n       e       l       e       r       n       v       e       l       t       h       e       e       c       h       n       g       e       n       h       e       e       n       e       b       l       e       l       e       r       n       n       g       r       e       p       r       n       b       e       c       u       e       h       w       n       n       u       r       e       x       p       e       r       e       n       s       e       c       4       4       l       d       l       f       c       n       g       e       b       e       e       r       r       e       u       l       b       u       n       g       r       e       r       e       e       b       u       b       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       d       f       h       e       r       e       u       l       f       f       r       e       r       e       e       v       e       n       w       r       e       h       n       h       e       f       r       n       g       l       e       r       e       e       t       u       u       p       w       r       n       d       f       2       0       h       e       c       n       r       b       u       n       f       l       d       l       f       r       e       f       r       w       e       e       x       e       n       f       r       c       l       f       c       n       2       0       r       b       u       n       l       e       r       n       n       g       b       p       r       p       n       g       r       b       u       n       b       e       l       f       r       h       e       f       r       e       n       e       r       v       e       h       e       g       r       e       n       l       e       r       n       p       l       n       e       w       r       h       l       e       c       n       w       e       e       r       v       e       h       e       u       p       e       f       u       n       c       n       f       r       l       e       f       n       e       b       v       r       n       l       b       u       n       n       g       h       v       n       g       b       e       r       v       e       h       h       e       u       p       e       f       u       n       c       n       n       2       0       w       p       e       c       l       c       e       f       v       r       n       l       b       u       n       n       g       l       b       u       n       h       e       l       e       w       e       p       r       p       e       b       v       e       h       r       e       e       r       e       g       e       l       e       r       n       n       g       h       e       e       n       e       b       l       e       f       u       l       p       l       e       r       e       e       w       h       c       h       r       e       f       f       e       r       e       n       f       r       2       0       b       u       w       e       h       w       r       e       e       f       f       e       c       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       a       n       u       b       e       r       f       p       e       c       l       z       e       l       g       r       h       h       v       e       b       e       e       n       p       r       p       e       r       e       h       e       l       d       l       k       n       h       v       e       h       w       n       h       e       r       e       f       f       e       c       v       e       n       e       n       n       c       p       u       e       r       v       n       p       p       l       c       n       u       c       h       f       c       l       g       e       e       n       8       1       1       2       8       e       x       p       r       e       n       r       e       c       g       n       n       3       0       n       h       n       r       e       n       n       e       n       1       0       g       e       n       g       e       l       8       e       f       n       e       h       e       l       b       e       l       r       b       u       n       f       r       n       n       n       c       e       v       e       c       r       c       n       n       n       g       h       e       p       r       b       b       l       e       f       h       e       n       n       c       e       h       v       n       g       e       c       h       l       b       e       l       t       h       e       l       g       v       e       r       e       g       g       n       p       r       p       e       r       l       b       e       l       r       b       u       n       n       n       n       c       e       w       h       n       g       l       e       l       b       e       l       e       g       n       n       g       g       u       n       r       t       r       n       g       l       e       r       b       u       n       w       h       e       p       e       k       h       e       n       g       l       e       l       b       e       l       n       p       r       p       e       n       l       g       r       h       c       l       l       e       i       i       s       l       l       d       w       h       c       h       n       e       r       v       e       p       z       n       p       r       c       e       b       e       n       w       l       e       r       e       n       e       r       g       b       e       e       l       y       n       g       e       l       2       8       h       e       n       e       f       n       e       h       r       e       e       l       e       r       e       n       e       r       g       b       e       e       l       c       l       l       e       s       c       e       l       d       l       n       w       h       c       h       h       e       b       l       p       e       r       f       r       f       e       u       r       e       l       e       r       n       n       g       p       r       v       e       b       n       g       h       e       e       x       r       h       e       n       l       e       r       n       p       r       c       n       r       n       r       e       l       n       c       r       p       r       e       e       l       r       e       h       e       e       l       g       e       n       g       6       e       v       e       l       p       e       n       c       c       e       l       e       r       e       v       e       r       n       f       i       i       s       l       l       d       c       l       l       e       b       f       g       s       l       d       l       b       u       n       g       q       u       n       e       w       n       p       z       n       a       l       l       h       e       b       v       e       l       d       l       e       h       u       e       h       h       e       l       b       e       l       r       b       u       n       c       n       b       e       r       e       p       r       e       e       n       e       b       x       u       e       n       r       p       e       l       2       b       u       h       e       e       x       p       n       e       n       l       p       r       f       h       e       l       r       e       r       c       h       e       g       e       n       e       r       l       f       h       e       r       b       u       n       f       r       a       n       h       e       r       w       r       e       h       e       l       d       l       k       e       x       e       n       e       x       n       g       l       e       r       n       n       g       l       g       r       h       e       l       w       h       l       b       e       l       r       b       u       n       g       e       n       g       n       h       u       7       p       r       p       e       l       d       s       v       r       l       d       l       e       h       b       e       x       e       n       n       g       u       p       p       r       v       e       c       r       r       e       g       r       e       r       w       h       c       h       f       g       f       u       n       c       n       e       c       h       c       p       n       e       n       f       h       e       r       b       u       n       u       l       n       e       u       l       b       u       p       p       r       v       e       c       r       c       h       n       e       x       n       g       e       l       2       7       h       e       n       e       x       e       n       e       b       n       g       r       e       h       e       l       d       l       k       b       v       e       w       e       g       h       e       r       e       g       r       e       r       t       h       e       h       w       e       h       u       n       g       h       e       v       e       c       r       r       e       e       e       l       h       e       w       e       k       r       e       g       r       e       r       c       n       l       e       b       e       e       r       p       e       r       f       r       n       c       e       n       n       e       h       e       h       a       o       s       o       l       d       l       l       g       b       a       h       e       l       e       r       n       n       g       f       h       r       e       e       e       l       b       e       n       l       c       l       l       p       l       h       r       p       r       n       f       u       n       c       n       e       c       h       p       l       n       e       a       o       s       o       l       d       l       l       g       b       u       n       b       l       e       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       x       e       n       n       g       c       u       r       r       e       n       e       e       p       l       e       r       n       n       g       l       g       r       h       3       x       0       c       r       e       h       e       l       d       l       k       n       n       e       r       e       n       g       p       c       b       u       h       e       e       x       n       g       u       c       h       e       h       c       l       l       e       d       l       d       l       5       l       l       f       c       u       e       n       x       u       e       n       r       p       e       l       b       e       l       d       l       o       u       r       e       h       l       d       l       f       e       x       e       n       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       r       e       l       d       l       k       n       w       h       c       h       h       e       p       r       e       c       e       l       b       e       l       r       b       u       n       f       r       p       l       e       c       n       b       e       e       x       p       r       e       e       b       l       n       e       r       c       b       n       n       f       h       e       l       b       e       l       r       b       u       n       f       h       e       r       n       n       g       n       h       u       h       v       e       n       r       e       r       c       n       n       h       e       r       b       u       n       e       g       n       r       e       q       u       r       e       e       n       f       h       e       x       u       e       n       r       p       e       l       i       n       n       h       n       k       h       e       n       r       u       c       n       f       f       f       e       r       e       n       b       l       e       e       c       n       f       u       n       c       n       l       d       l       f       c       n       b       e       c       b       n       e       w       h       r       e       p       r       e       e       n       n       l       e       r       n       n       g       e       g       l       e       r       n       e       e       p       f       e       u       r       e       n       n       e       n       e       n       n       n       e       r       3       l       b       e       l       d       r       b       u       n       l       e       r       n       n       g       f       r       e       a       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       w       e       f       r       n       r       u       c       e       h       w       l       e       r       n       n       g       l       e       e       c       n       r       e       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       h       e       n       e       c       r       b       e       h       e       l       e       r       n       n       g       f       f       r       e       3       1       p       r       b       l       e       f       r       u       l       n       l       e       x       r       e       n       e       h       e       n       p       u       p       c       e       n       y       1       2       c       e       n       e       h       e       c       p       l       e       e       e       f       l       b       e       l       w       h       e       r       e       c       h       e       n       u       b       e       r       f       p       b       l       e       l       b       e       l       v       l       u       e       w       e       c       n       e       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       d       l       p       r       b       l       e       w       h       e       r       e       f       r       e       c       h       n       p       u       p       l       e       x       x       e       2       x       8       8       x       8       8       x       h       e       r       e       l       b       e       l       r       b       u       n       x       1       x       2       x       c       x       e       2       x       8       8       x       8       8       r       c       h       e       r       e       x       c       e       x       p       r       e       e       h       e       p       r       b       b       l       f       h       e       p       l       e       x       h       v       n       g       h       e       c       h       l       b       e       l       c       n       h       u       h       h       e       p       c       c       n       r       n       h       x       c       x       e       2       x       8       8       x       8       8       0       1       n       c       1       x       c       1       t       h       e       g       l       f       h       e       l       d       l       p       r       b       l       e       l       e       r       n       p       p       n       g       f       u       n       c       n       g       x       x       e       2       x       8       6       x       9       2       b       e       w       e       e       n       n       n       p       u       p       l       e       x       n       c       r       r       e       p       n       n       g       l       b       e       l       r       b       u       n       h       e       r       e       w       e       w       n       l       e       r       n       h       e       p       p       n       g       f       u       n       c       n       g       x       b       e       c       n       r       e       e       b       e       e       l       t       a       e       c       n       r       e       e       c       n       f       e       f       p       l       n       e       n       n       e       f       l       e       f       n       e       l       e       c       h       p       l       n       e       n       x       e       2       x       8       8       x       8       8       n       e       f       n       e       p       l       f       u       n       c       n       n       x       c       2       x       b       7       x       c       e       x       9       8       x       x       e       2       x       8       6       x       9       2       0       1       p       r       e       e       r       z       e       b       x       c       e       x       9       8       e       e       r       n       e       w       h       e       h       e       r       p       l       e       e       n       h       e       l       e       f       r       r       g       h       u       b       r       e       e       e       c       h       l       e       f       n       e       x       e       2       x       8       8       x       8       8       l       h       l       r       b       u       n       q       q       1       q       2       q       c       p       c       v       e       r       y       e       q       c       x       e       2       x       8       8       x       8       8       0       1       n       c       1       q       c       1       t       b       u       l       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       f       l       l       w       n       g       2       0       w       e       u       e       p       r       b       b       l       c       p       l       f       u       n       c       n       n       x       x       c       e       x       9       8       x       c       f       x       8       3       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       w       h       e       r       e       x       c       f       x       8       3       x       c       2       x       b       7       g       f       u       n       c       n       x       c       f       x       9       5       x       c       2       x       b       7       n       n       e       x       f       u       n       c       n       b       r       n       g       h       e       x       c       f       x       9       5       n       h       u       p       u       f       f       u       n       c       n       f       x       x       c       e       x       9       8       n       c       r       r       e       p       n       e       n       c       e       w       h       p       l       n       e       n       n       f       x       x       e       2       x       8       6       x       9       2       r       m       r       e       l       v       l       u       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       e       p       e       n       n       g       n       h       e       p       l       e       x       n       h       e       p       r       e       e       r       x       c       e       x       9       8       n       c       n       k       e       n       f       r       f       r       p       l       e       f       r       c       n       b       e       l       n       e       r       r       n       f       r       n       f       x       w       h       e       r       e       x       c       e       x       9       8       h       e       r       n       f       r       n       r       x       f       r       c       p       l       e       x       f       r       c       n       b       e       e       e       p       n       e       w       r       k       p       e       r       f       r       r       e       p       r       e       e       n       n       l       e       r       n       n       g       n       n       e       n       e       n       n       n       e       r       h       e       n       x       c       e       x       9       8       h       e       n       e       w       r       k       p       r       e       e       r       t       h       e       c       r       r       e       p       n       e       n       c       e       b       e       w       e       e       n       h       e       p       l       n       e       n       h       e       u       p       u       u       n       f       f       u       n       c       n       f       n       c       e       b       x       c       f       x       9       5       x       c       2       x       b       7       h       r       n       l       g       e       n       e       r       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       e       w       h       c       h       u       p       u       u       n       f       r       x       e       2       x       8       0       x       9       c       f       x       e       2       x       8       0       x       9       d       r       e       u       e       f       r       c       n       r       u       c       n       g       r       e       e       e       e       r       n       e       r       n       l       a       n       e       x       p       l       e       e       n       r       e       x       c       f       x       9       5       x       c       2       x       b       7       h       w       n       n       f       g       2       t       h       e       n       h       e       p       r       b       b       l       f       h       e       p       l       e       x       f       l       l       n       g       n       l       e       f       n       e       g       v       e       n       b       y       l       r       p       x       x       c       e       x       9       8       n       x       x       c       e       x       9       8       1       x       e       2       x       8       8       x       8       8       l       n       1       x       e       2       x       8       8       x       9       2       n       x       x       c       e       x       9       8       1       x       e       2       x       8       8       x       8       8       l       n       1       n       x       e       2       x       8       8       x       8       8       n       w       h       e       r       e       1       x       c       2       x       b       7       n       n       c       r       f       u       n       c       n       n       l       l       n       n       l       r       n       e       n       e       h       e       e       f       l       e       f       n       e       h       e       l       b       h       e       l       e       f       n       r       g       h       u       b       r       e       e       f       n       e       n       t       n       l       n       t       n       r       r       e       p       e       c       v       e       l       t       h       e       u       p       u       f       h       e       r       e       e       t       w       r       x       e       h       e       p       p       n       g       f       u       n       c       n       g       e       f       n       e       b       x       g       x       x       c       e       x       9       8       t       p       x       x       c       e       x       9       8       q       2       x       e       2       x       8       8       x       8       8       l       3       2       t       r       e       e       o       p       z       n       g       v       e       n       r       n       n       g       e       s       x       n       1       u       r       g       l       l       e       r       n       e       c       n       r       e       e       t       e       c       r       b       e       n       s       e       c       3       1       w       h       c       h       c       n       u       p       u       r       b       u       n       g       x       x       c       e       x       9       8       t       l       r       f       r       e       c       h       p       l       e       x       t       h       e       n       r       g       h       f       r       w       r       w       n       z       e       h       e       k       u       l       l       b       c       k       l       e       b       l       e       r       k       l       v       e       r       g       e       n       c       e       b       e       w       e       e       n       e       c       h       g       x       x       c       e       x       9       8       t       n       r       e       q       u       v       l       e       n       l       n       z       e       h       e       f       l       l       w       n       g       c       r       e       n       r       p       l       r       q       x       c       e       x       9       8       s       x       e       2       x       8       8       x       9       2       n       c       n       c       x       1       0       x       x       1       1       1       x       x       c       1       x       x       c       x       l       g       g       c       x       x       c       e       x       9       8       t       x       e       2       x       8       8       x       9       2       x       l       g       p       x       x       c       e       x       9       8       q       c       3       n       1       c       1       n       1       c       1       x       e       2       x       8       8       x       8       8       l       4       x       0       c       w       h       e       r       e       q       e       n       e       h       e       r       b       u       n       h       e       l       b       l       l       h       e       l       e       f       n       e       l       n       g       c       x       x       c       e       x       9       8       t       h       e       c       h       u       p       u       u       n       f       g       x       x       c       e       x       9       8       t       l       e       r       n       n       g       h       e       r       e       e       t       r       e       q       u       r       e       h       e       e       n       f       w       p       r       e       e       r       1       h       e       p       l       n       e       p       r       e       e       r       x       c       e       x       9       8       n       2       h       e       r       b       u       n       q       h       e       l       b       h       e       l       e       f       n       e       t       h       e       b       e       p       r       e       e       r       x       c       e       x       9       8       x       e       2       x       8       8       x       9       7       q       x       e       2       x       8       8       x       9       7       r       e       e       e       r       n       e       b       x       c       e       x       9       8       x       e       2       x       8       8       x       9       7       q       x       e       2       x       8       8       x       9       7       r       g       n       r       q       x       c       e       x       9       8       s       4       x       c       e       x       9       8       q       t       l       v       e       e       q       n       4       w       e       c       n       e       r       n       l       e       r       n       n       g       p       z       n       r       e       g       f       r       w       e       f       x       q       n       p       z       e       x       c       e       x       9       8       t       h       e       n       w       e       f       x       x       c       e       x       9       8       n       p       z       e       q       t       h       e       e       w       l       e       r       n       n       g       e       p       r       e       l       e       r       n       v       e       l       p       e       r       f       r       e       u       n       l       c       n       v       e       r       g       e       n       c       e       r       x       u       n       u       b       e       r       f       e       r       n       r       e       c       h       e       e       f       n       e       n       h       e       e       x       p       e       r       e       n       3       2       1       l       e       r       n       n       g       s       p       l       n       e       i       n       h       e       c       n       w       e       e       c       r       b       e       h       w       l       e       r       n       h       e       p       r       e       e       r       x       c       e       x       9       8       f       r       p       l       n       e       w       h       e       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       r       e       f       x       e       w       e       c       p       u       e       h       e       g       r       e       n       f       h       e       l       r       q       x       c       e       x       9       8       s       w       r       x       c       e       x       9       8       b       h       e       c       h       n       r       u       l       e       n       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       s       x       x       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       s       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       5       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       1       n       x       e       2       x       8       8       x       8       8       n       w       h       e       r       e       n       l       h       e       f       r       e       r       e       p       e       n       n       h       e       r       e       e       n       h       e       e       c       n       e       r       e       p       e       n       n       h       e       p       e       c       f       c       p       e       f       h       e       f       u       n       c       n       f       x       c       f       x       9       5       n       t       h       e       f       r       e       r       g       v       e       n       b       c       x       0       1       g       c       x       x       c       e       x       9       8       t       n       l       x       1       1       g       c       x       x       c       e       x       9       8       t       n       r       1       x       c       x       1       0       x       e       2       x       8       8       x       8       2       r       q       x       c       e       x       9       8       s       x       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       n       x       x       c       e       x       9       8       6       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       n       x       x       c       e       x       9       8       n       c       1       g       c       x       x       c       e       x       9       8       t       g       c       x       x       c       e       x       9       8       t       p       p       w       h       e       r       e       g       c       x       x       c       e       x       9       8       t       n       l       x       e       2       x       8       8       x       8       8       l       l       n       p       x       x       c       e       x       9       8       q       c       n       g       c       x       x       c       e       x       9       8       t       n       r       x       e       2       x       8       8       x       8       8       l       r       n       p       x       x       c       e       x       9       8       q       c       n       e       h       l       e       t       n       b       e       h       e       r       e       e       r       e       h       e       n       e       n       h       e       n       w       e       h       v       e       g       c       x       x       c       e       x       9       8       t       n       g       c       x       x       c       e       x       9       8       t       n       l       g       c       x       x       c       e       x       9       8       t       n       r       t       h       e       n       h       e       g       r       e       n       c       p       u       n       n       e       q       n       6       c       n       b       e       r       e       h       e       l       e       f       n       e       n       c       r       r       e       u       n       b       u       p       n       n       e       r       t       h       u       h       e       p       l       n       e       p       r       e       e       r       c       n       b       e       l       e       r       n       e       b       n       r       b       c       k       p       r       p       g       n       3       2       2       l       e       r       n       n       g       l       e       f       n       e       n       w       f       x       n       g       h       e       p       r       e       e       r       x       c       e       x       9       8       w       e       h       w       h       w       l       e       r       n       h       e       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       q       w       h       c       h       c       n       r       n       e       p       z       n       p       r       b       l       e       n       r       q       x       c       e       x       9       8       s       x       e       2       x       8       8       x       8       0       q       c       x       q       c       1       7       c       1       h       e       r       e       w       e       p       r       p       e       r       e       h       c       n       r       n       e       c       n       v       e       x       p       z       n       p       r       b       l       e       b       v       r       n       l       b       u       n       n       g       1       9       2       9       w       h       c       h       l       e       e       p       z       e       f       r       e       e       n       f       c       n       v       e       r       g       e       u       p       e       r       u       l       e       f       r       q       i       n       v       r       n       l       b       u       n       n       g       n       r       g       n       l       b       j       e       c       v       e       f       u       n       c       n       b       e       n       z       e       g       e       r       e       p       l       c       e       b       b       u       n       n       n       e       r       v       e       n       n       e       r       a       u       p       p       e       r       b       u       n       f       r       h       e       l       f       u       n       c       n       r       q       x       c       e       x       9       8       s       c       n       b       e       b       n       e       b       j       e       n       e       n       x       e       2       x       8       0       x       9       9       n       e       q       u       l       r       q       x       c       e       x       9       8       s       x       e       2       x       8       8       x       9       2       n       c       x       1       0       x       x       1       1       1       x       x       c       x       l       g       p       x       x       c       e       x       9       8       q       c       n       1       c       1       x       e       2       x       8       8       x       8       8       l       x       e       2       x       8       9       x       a       4       x       e       2       x       8       8       x       9       2       w       h       e       r       e       x       c       e       x       b       e       q       c       x       1       n       n       x       c       x       1       c       1       p       x       x       c       e       x       9       8       q       c       g       c       x       x       c       e       x       9       8       t       x       c       f       x       8       6       q       q       x       c       c       x       8       4       x       e       2       x       8       8       x       9       2       x       c       x       x       c       e       x       b       e       q       x       c       c       x       8       4       c       x       l       g       x       e       2       x       8       8       x       8       8       l       x       1       0       p       x       x       c       e       x       9       8       q       x       1       1       c       x       c       e       x       b       e       q       x       c       c       x       8       4       c       x       8       w       e       e       f       n       e       n       c       x       1       0       p       x       x       c       e       x       9       8       q       x       1       1       1       x       x       c       x       c       x       x       c       e       x       b       e       q       x       c       c       x       8       4       c       x       l       g       n       1       c       1       x       c       e       x       b       e       q       x       c       c       x       8       4       c       x       9       x       e       2       x       8       8       x       8       8       l       t       h       e       n       x       c       f       x       8       6       q       q       x       c       c       x       8       4       n       u       p       p       e       r       b       u       n       f       r       r       q       x       c       e       x       9       8       s       w       h       c       h       h       h       e       p       r       p       e       r       h       f       r       n       q       n       q       x       c       c       x       8       4       x       c       f       x       8       6       q       q       x       c       c       x       8       4       x       e       2       x       8       9       x       a       5       r       q       x       c       e       x       9       8       s       n       x       c       f       x       8       6       q       q       r       q       x       c       e       x       9       8       s       a       u       e       h       w       e       r       e       p       n       q       c       r       r       e       p       n       n       g       h       e       h       e       r       n       h       e       n       x       c       f       x       8       6       q       q       n       u       p       p       e       r       b       u       n       f       r       r       q       x       c       e       x       9       8       s       i       n       h       e       n       e       x       e       r       n       q       1       c       h       e       n       u       c       h       h       x       c       f       x       8       6       q       1       q       x       e       2       x       8       9       x       a       4       r       q       x       c       e       x       9       8       s       w       h       c       h       p       l       e       r       q       1       x       c       e       x       9       8       s       x       e       2       x       8       9       x       a       4       r       q       x       c       e       x       9       8       s       5       x       0       c       c       n       e       q       u       e       n       l       w       e       c       n       n       z       e       x       c       f       x       8       6       q       q       x       c       c       x       8       4       n       e       f       r       q       x       c       e       x       9       8       s       f       e       r       e       n       u       r       n       g       h       r       q       x       c       e       x       9       8       s       x       c       f       x       8       6       q       q       x       c       c       x       8       4       e       q       x       c       c       x       8       4       q       s       w       e       h       v       e       q       1       r       g       n       x       c       f       x       8       6       q       q       x       e       2       x       8       8       x       8       0       q       c       x       q       c       1       1       0       c       1       w       h       c       h       l       e       n       z       n       g       h       e       l       g       r       n       g       n       e       f       n       e       b       x       c       f       x       9       5       q       q       x       c       f       x       8       6       q       q       x       x       c       e       x       b       b       x       e       2       x       8       8       x       8       8       l       w       h       e       r       e       x       c       e       x       b       b       h       e       l       g       r       n       g       e       u       l       p       l       e       r       b       e       n       g       x       c       e       x       b       b       1       n       e       h       q       c       1       x       e       2       x       8       8       x       8       8       0       1       n       r       b       u       n       h       e       l       b       h       e       l       e       f       n       e       t       h       e       r       n       g       0       r       b       u       n       q       c       c       1       3       3       q       c       x       e       2       x       8       8       x       9       2       1       1       1       c       1       x       e       2       x       8       8       x       8       2       x       c       f       x       9       5       q       q       x       e       2       x       8       8       x       8       2       q       c       n       c       1       x       x       c       1       x       c       e       x       b       e       q       c       x       n       q       c       n       1       c       1       x       f       e       h       q       c       c       x       0       w       e       h       v       e       p       n       c       x       x       c       e       x       b       e       q       c       x       p       c       1       p       n       c       x       c       e       x       b       e       q       x       x       c       1       1       c       1       2       1       1       e       q       n       1       2       h       e       u       p       e       c       h       e       e       f       r       c       1       q       c       0       p       n       q       c       n       b       e       p       l       n       l       z       e       b       h       e       u       n       f       r       p       c       l       e       r       n       n       g       f       r       e       a       f       r       e       n       e       n       e       b       l       e       f       e       c       n       r       e       e       f       t       1       t       k       i       n       h       e       r       n       n       g       g       e       l       l       r       e       e       n       h       e       f       r       e       f       u       e       h       e       e       p       r       e       e       r       x       c       e       x       9       8       f       r       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x       c       2       x       b       7       x       c       e       x       9       8       b       u       c       r       r       e       p       n       f       f       e       r       e       n       u       p       u       u       n       f       f       g       n       e       b       x       c       f       x       9       5       e       e       f       g       2       b       u       e       c       h       r       e       e       h       n       e       p       e       n       e       n       l       e       f       n       e       p       r       e       c       n       q       t       h       e       l       f       u       n       c       n       f       r       f       r       e       g       v       e       n       b       v       e       r       g       n       g       h       e       l       f       u       n       c       n       f       r       l       l       n       v       u       l       r       e       e       p       k       1       r       f       k       k       1       r       t       k       w       h       e       r       e       r       t       k       h       e       l       f       u       n       c       n       f       r       r       e       e       t       k       e       f       n       e       b       e       q       n       3       t       l       e       r       n       x       c       e       x       9       8       b       f       x       n       g       h       e       l       e       f       n       e       p       r       e       c       n       q       f       l       l       h       e       r       e       e       n       h       e       f       r       e       f       b       e       n       h       e       e       r       v       n       n       s       e       c       3       2       n       r       e       f       e       r       r       n       g       f       g       2       w       e       h       v       e       n       k       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       k       n       x       x       c       e       x       9       8       1       x       x       x       x       e       2       x       8       8       x       8       2       r       f       x       e       2       x       8       8       x       8       2       r       t       k       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       k       1       x       e       2       x       8       8       x       8       2       f       x       c       f       x       9       5       k       n       x       x       c       e       x       9       8       x       e       2       x       8       8       x       8       2       x       c       e       x       9       8       1       3       k       1       n       x       e       2       x       8       8       x       8       8       n       k       w       h       e       r       e       n       k       n       x       c       f       x       9       5       k       x       c       2       x       b       7       r       e       h       e       p       l       n       e       e       n       h       e       n       e       x       f       u       n       c       n       f       t       k       r       e       p       e       c       v       e       l       n       e       h       h       e       n       e       x       f       u       n       c       n       x       c       f       x       9       5       k       x       c       2       x       b       7       f       r       e       c       h       r       e       e       r       n       l       g       n       e       b       e       f       r       e       r       e       e       l       e       r       n       n       g       n       h       u       p       l       n       e       c       r       r       e       p       n       u       b       e       f       u       p       u       u       n       f       f       t       h       r       e       g       l       r       h       e       r       n       u       b       p       c       e       e       h       1       7       w       h       c       h       n       c       r       e       e       h       e       r       n       n       e       n       r       n       n       g       r       e       u       c       e       h       e       r       k       f       v       e       r       f       n       g       a       f       r       q       n       c       e       e       c       h       r       e       e       n       h       e       f       r       e       f       h       w       n       l       e       f       n       e       p       r       e       c       n       q       w       e       c       n       u       p       e       h       e       n       e       p       e       n       e       n       l       b       e       q       n       1       2       g       v       e       n       b       x       c       e       x       9       8       f       r       p       l       e       e       n       n       l       c       n       v       e       n       e       n       c       e       w       e       n       c       n       u       c       h       u       p       e       c       h       e       e       n       h       e       w       h       l       e       e       s       b       u       n       e       f       n       b       c       h       e       b       t       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       d       l       f       h       w       n       n       a       l       g       r       h       1       a       l       g       r       h       1       t       h       e       r       n       n       g       p       r       c       e       u       r       e       f       l       d       l       f       r       e       q       u       r       e       s       r       n       n       g       e       n       b       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       q       i       n       l       z       e       x       c       e       x       9       8       r       n       l       n       q       u       n       f       r       l       e       b       x       e       2       x       8       8       x       8       5       w       h       l       e       n       c       n       v       e       r       g       e       w       h       l       e       b       n       b       r       n       l       e       l       e       c       n       b       c       h       b       f       r       s       u       p       e       s       x       c       e       x       9       8       b       c       p       u       n       g       g       r       e       n       e       q       n       1       3       n       b       b       b       b       e       n       w       h       l       e       u       p       e       q       b       e       r       n       g       e       q       n       1       2       n       b       b       x       e       2       x       8       8       x       8       5       e       n       w       h       l       e       i       n       h       e       e       n       g       g       e       h       e       u       p       u       f       h       e       f       r       e       f       g       v       e       n       b       v       e       r       g       n       g       h       e       p       r       e       c       n       f       r       l       l       h       e       p       k       1       n       v       u       l       r       e       e       g       x       x       c       e       x       9       8       f       k       k       1       g       x       x       c       e       x       9       8       t       k       6       x       0       c       4       e       x       p       e       r       e       n       l       r       e       u       l       o       u       r       r       e       l       z       n       f       l       d       l       f       b       e       n       x       e       2       x       8       0       x       9       c       c       f       f       e       x       e       2       x       8       0       x       9       d       1       8       i       u       l       r       n       p       l       e       e       n       e       n       r       n       e       u       r       l       n       e       w       r       k       l       e       r       w       e       c       n       e       h       e       r       u       e       h       l       l       w       n       l       n       e       e       l       l       d       l       f       r       n       e       g       r       e       w       h       n       e       e       p       n       e       w       r       k       l       d       l       f       w       e       e       v       l       u       e       l       d       l       f       n       f       f       e       r       e       n       l       d       l       k       n       c       p       r       e       w       h       h       e       r       n       l       n       e       l       d       l       e       h       a       l       d       l       f       c       n       b       e       l       e       r       n       e       f       r       r       w       g       e       n       n       e       n       e       n       n       n       e       r       w       e       v       e       r       f       l       d       l       f       n       c       p       u       e       r       v       n       p       p       l       c       n       e       f       c       l       g       e       e       n       t       h       e       e       f       u       l       e       n       g       f       r       h       e       p       r       e       e       r       f       u       r       f       r       e       r       e       r       e       e       n       u       b       e       r       5       r       e       e       e       p       h       7       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       6       4       e       r       n       e       u       p       e       l       e       f       n       e       p       r       e       c       n       2       0       h       e       n       u       b       e       r       f       n       b       c       h       e       u       p       e       l       e       f       n       e       p       r       e       c       n       1       0       0       x       u       e       r       n       2       5       0       0       0       4       1       c       p       r       n       f       l       d       l       f       s       n       l       n       e       l       d       l       m       e       h       w       e       c       p       r       e       u       r       h       l       l       w       e       l       l       d       l       f       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       d       l       e       h       f       r       l       d       l       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       f       x       x       c       e       x       9       8       l       n       e       r       r       n       f       r       n       f       x       e       h       e       h       u       p       u       u       n       f       x       x       c       e       x       b       8       x       c       e       x       b       8       x       w       h       e       r       e       x       c       e       x       b       8       h       e       h       c       l       u       n       f       h       e       r       n       f       r       n       r       x       x       c       e       x       9       8       w       e       u       e       3       p       p       u       l       r       l       d       l       e       n       6       m       v       e       h       u       n       g       e       n       e       n       n       u       r       l       s       c       e       n       e       1       t       h       e       p       l       e       n       h       e       e       3       e       r       e       r       e       p       r       e       e       n       e       b       n       u       e       r       c       l       e       c       r       p       r       n       h       e       g       r       u       n       r       u       h       f       r       h       e       r       e       h       e       r       n       g       r       b       u       n       f       c       r       w       p       n       n       n       v       e       h       e       e       e       r       b       u       n       r       e       l       e       h       u       n       g       e       n       e       n       l       b       e       l       r       b       u       n       n       c       e       n       e       u       c       h       p       l       n       k       n       c       l       u       r       e       p       e       c       v       e       l       t       h       e       l       b       e       l       r       b       u       n       f       h       e       e       3       e       r       e       x       u       r       e       r       b       u       n       u       c       h       h       e       r       n       g       r       b       u       n       h       w       n       n       f       g       1       b       f       l       l       w       n       g       7       2       7       w       e       u       e       6       e       u       r       e       e       v       l       u       e       h       e       p       e       r       f       r       n       c       e       f       l       d       l       e       h       w       h       c       h       c       p       u       e       h       e       v       e       r       g       e       l       r       n       c       e       b       e       w       e       e       n       h       e       p       r       e       c       e       r       n       g       r       b       u       n       n       h       e       r       e       l       r       n       g       r       b       u       n       n       c       l       u       n       g       4       n       c       e       e       u       r       e       k       l       e       u       c       l       e       n       s       x       c       f       x       8       6       r       e       n       e       n       s       q       u       r       e       x       c       f       x       8       7       2       n       w       l       r       e       u       r       e       f       e       l       i       n       e       r       e       c       n       w       e       e       v       l       u       e       u       r       h       l       l       w       e       l       l       d       l       f       n       h       e       e       3       e       n       c       p       r       e       w       h       h       e       r       e       f       h       e       r       n       l       n       e       l       d       l       e       h       t       h       e       r       e       u       l       f       l       d       l       f       n       h       e       c       p       e       r       r       e       u       r       z       e       n       t       b       l       e       1       f       r       m       v       e       w       e       q       u       e       h       e       r       e       u       l       r       e       p       r       e       n       2       7       h       e       c       e       f       2       7       n       p       u       b       l       c       l       v       l       b       l       e       f       r       h       e       r       e       u       l       f       h       e       h       e       r       w       w       e       r       u       n       c       e       h       h       e       u       h       r       h       e       v       l       b       l       e       i       n       l       l       c       e       f       l       l       w       n       g       2       7       6       w       e       p       l       e       c       h       e       n       1       0       f       x       e       f       l       n       n       r       e       n       f       l       c       r       v       l       n       w       h       c       h       r       e       p       r       e       e       n       h       e       r       e       u       l       b       x       e       2       x       8       0       x       9       c       e       n       x       c       2       x       b       1       n       r       e       v       n       x       e       2       x       8       0       x       9       d       n       e       r       l       e       h       w       r       n       n       g       n       e       n       g       g       e       v       e       a       c       n       b       e       e       e       n       f       r       t       b       l       e       1       l       d       l       f       p       e       r       f       r       b       e       n       l       l       f       h       e       x       e       u       r       e       t       b       l       e       1       c       p       r       n       r       e       u       l       n       h       r       e       e       l       d       l       e       6       x       e       2       x       8       0       x       9       c       x       e       2       x       8       6       x       9       1       x       e       2       x       8       0       x       9       d       n       x       e       2       x       8       0       x       9       c       x       e       2       x       8       6       x       9       3       x       e       2       x       8       0       x       9       d       n       c       e       h       e       l       r       g       e       r       n       h       e       l       l       e       r       h       e       b       e       e       r       r       e       p       e       c       v       e       l       d       e       m       e       h       k       l       x       e       2       x       8       6       x       9       3       e       u       c       l       e       n       x       e       2       x       8       6       x       9       3       s       x       c       f       x       8       6       r       e       n       e       n       x       e       2       x       8       6       x       9       3       s       q       u       r       e       x       c       f       x       8       7       2       x       e       2       x       8       6       x       9       3       f       e       l       x       e       2       x       8       6       x       9       1       i       n       e       r       e       c       n       x       e       2       x       8       6       x       9       1       m       v       e       l       d       l       f       u       r       a       o       s       o       l       d       l       g       b       2       7       l       d       l       g       b       2       7       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       0       7       3       x       c       2       x       b       1       0       0       0       5       0       0       8       6       x       c       2       x       b       1       0       0       0       4       0       0       9       0       x       c       2       x       b       1       0       0       0       4       0       0       9       2       x       c       2       x       b       1       0       0       0       5       0       0       9       9       x       c       2       x       b       1       0       0       0       4       0       1       2       9       x       c       2       x       b       1       0       0       0       7       0       1       3       3       x       c       2       x       b       1       0       0       0       3       0       1       5       5       x       c       2       x       b       1       0       0       0       3       0       1       5       9       x       c       2       x       b       1       0       0       0       3       0       1       5       8       x       c       2       x       b       1       0       0       0       4       0       1       6       7       x       c       2       x       b       1       0       0       0       4       0       1       8       7       x       c       2       x       b       1       0       0       0       4       0       1       3       0       x       c       2       x       b       1       0       0       0       3       0       1       5       2       x       c       2       x       b       1       0       0       0       3       0       1       5       5       x       c       2       x       b       1       0       0       0       3       0       1       5       6       x       c       2       x       b       1       0       0       0       4       0       1       6       4       x       c       2       x       b       1       0       0       0       3       0       1       8       3       x       c       2       x       b       1       0       0       0       4       0       0       7       0       x       c       2       x       b       1       0       0       0       4       0       0       8       4       x       c       2       x       b       1       0       0       0       3       0       0       8       8       x       c       2       x       b       1       0       0       0       3       0       0       8       8       x       c       2       x       b       1       0       0       0       4       0       0       9       6       x       c       2       x       b       1       0       0       0       4       0       1       2       0       x       c       2       x       b       1       0       0       0       5       0       9       8       1       x       c       2       x       b       1       0       0       0       1       0       9       7       8       x       c       2       x       b       1       0       0       0       1       0       9       7       7       x       c       2       x       b       1       0       0       0       1       0       9       7       7       x       c       2       x       b       1       0       0       0       1       0       9       7       4       x       c       2       x       b       1       0       0       0       1       0       9       6       7       x       c       2       x       b       1       0       0       0       1       0       8       7       0       x       c       2       x       b       1       0       0       0       3       0       8       4       8       x       c       2       x       b       1       0       0       0       3       0       8       4       5       x       c       2       x       b       1       0       0       0       3       0       8       4       4       x       c       2       x       b       1       0       0       0       4       0       8       3       6       x       c       2       x       b       1       0       0       0       3       0       8       1       7       x       c       2       x       b       1       0       0       0       4       l       d       l       f       u       r       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       2       2       8       x       c       2       x       b       1       0       0       0       6       0       2       4       5       x       c       2       x       b       1       0       0       1       9       0       2       3       1       x       c       2       x       b       1       0       0       2       1       0       2       3       9       x       c       2       x       b       1       0       0       1       8       0       0       8       5       x       c       2       x       b       1       0       0       0       2       0       0       9       9       x       c       2       x       b       1       0       0       0       5       0       0       7       6       x       c       2       x       b       1       0       0       0       6       0       0       8       9       x       c       2       x       b       1       0       0       0       6       0       2       1       2       x       c       2       x       b       1       0       0       0       2       0       2       2       9       x       c       2       x       b       1       0       0       1       5       0       2       3       1       x       c       2       x       b       1       0       0       1       2       0       2       5       3       x       c       2       x       b       1       0       0       0       9       0       1       7       9       x       c       2       x       b       1       0       0       0       4       0       1       8       9       x       c       2       x       b       1       0       0       2       1       0       2       1       1       x       c       2       x       b       1       0       0       1       8       0       2       0       5       x       c       2       x       b       1       0       0       1       2       0       9       4       8       x       c       2       x       b       1       0       0       0       1       0       9       4       0       x       c       2       x       b       1       0       0       0       6       0       9       3       8       x       c       2       x       b       1       0       0       0       8       0       9       4       4       x       c       2       x       b       1       0       0       0       3       0       7       8       8       x       c       2       x       b       1       0       0       0       2       0       7       7       1       x       c       2       x       b       1       0       0       1       5       0       7       6       9       x       c       2       x       b       1       0       0       1       2       0       7       4       7       x       c       2       x       b       1       0       0       0       9       l       d       l       f       u       r       l       d       s       v       r       7       b       f       g       s       l       d       l       6       i       i       s       l       d       l       1       1       0       5       3       4       x       c       2       x       b       1       0       0       1       3       0       8       5       2       x       c       2       x       b       1       0       0       2       3       0       8       5       6       x       c       2       x       b       1       0       0       6       1       0       8       7       9       x       c       2       x       b       1       0       0       2       3       0       3       1       7       x       c       2       x       b       1       0       0       1       4       0       5       1       1       x       c       2       x       b       1       0       0       2       1       0       4       7       5       x       c       2       x       b       1       0       0       2       9       0       4       5       8       x       c       2       x       b       1       0       0       1       4       0       3       3       6       x       c       2       x       b       1       0       0       1       0       0       4       9       2       x       c       2       x       b       1       0       0       1       6       0       5       0       8       x       c       2       x       b       1       0       0       2       6       0       5       3       9       x       c       2       x       b       1       0       0       1       1       0       4       4       8       x       c       2       x       b       1       0       0       1       7       0       5       9       5       x       c       2       x       b       1       0       0       2       6       0       7       1       6       x       c       2       x       b       1       0       0       4       1       0       7       9       2       x       c       2       x       b       1       0       0       1       9       0       8       2       4       x       c       2       x       b       1       0       0       0       8       0       8       1       3       x       c       2       x       b       1       0       0       0       8       0       7       2       2       x       c       2       x       b       1       0       0       2       1       0       6       8       6       x       c       2       x       b       1       0       0       0       9       0       6       6       4       x       c       2       x       b       1       0       0       1       0       0       5       0       9       x       c       2       x       b       1       0       0       1       6       0       4       9       2       x       c       2       x       b       1       0       0       2       6       0       4       6       1       x       c       2       x       b       1       0       0       1       1       h       u       n       g       e       n       e       n       u       r       l       s       c       e       n       e       4       2       e       v       l       u       n       f       l       d       l       f       n       f       c       l       a       g       e       e       n       i       n       e       l       e       r       u       r       e       8       1       1       2       8       1       5       5       g       e       e       n       f       r       u       l       e       l       d       l       p       r       b       l       e       w       e       c       n       u       c       f       c       l       g       e       e       n       e       x       p       e       r       e       n       n       m       r       p       h       2       4       w       h       c       h       c       n       n       r       e       h       n       5       0       0       0       0       f       c       l       g       e       f       r       b       u       1       3       0       0       0       p       e       p       l       e       f       f       f       e       r       e       n       r       c       e       e       c       h       f       c       l       g       e       n       n       e       w       h       c       h       r       n       l       g       c       l       g       e       t       g       e       n       e       r       e       n       g       e       r       b       u       n       f       r       e       c       h       f       c       e       g       e       w       e       f       l       l       w       h       e       e       r       e       g       u       e       n       8       2       8       5       w       h       c       h       u       e       g       u       n       r       b       u       n       w       h       e       e       n       h       e       c       h       r       n       l       g       c       l       g       e       f       h       e       f       c       e       g       e       f       g       1       t       h       e       p       r       e       c       e       g       e       f       r       f       c       e       g       e       p       l       h       e       g       e       h       v       n       g       h       e       h       g       h       e       p       r       b       b       l       n       h       e       p       r       e       c       e       1       w       e       w       n       l       h       e       e       e       f       r       h       p       c       e       e       u       e       u       c       n       p       e       p       l       e       x       g       e       n       g       l       d       l       n       e       x       h       7       x       0       c       l       b       e       l       r       b       u       n       t       h       e       p       e       r       f       r       n       c       e       f       g       e       e       n       e       v       l       u       e       b       h       e       e       n       b       l       u       e       e       r       r       r       m       a       e       b       e       w       e       e       n       p       r       e       c       e       g       e       n       c       h       r       n       l       g       c       l       g       e       a       h       e       c       u       r       r       e       n       e       f       h       e       r       r       e       u       l       n       m       r       p       h       b       n       b       f       n       e       u       n       n       g       d       l       d       l       5       n       v       g       g       f       c       e       2       3       w       e       l       b       u       l       l       d       l       f       n       v       g       g       f       c       e       b       r       e       p       l       c       n       g       h       e       f       x       l       e       r       n       v       g       g       n       e       b       l       d       l       f       f       l       l       w       n       g       5       w       e       n       r       1       0       e       n       f       l       c       r       v       l       n       n       h       e       r       e       u       l       r       e       u       r       z       e       n       t       b       l       e       2       w       h       c       h       h       w       l       d       l       f       c       h       e       v       e       h       e       e       f       h       e       r       p       e       r       f       r       n       c       e       n       m       r       p       h       n       e       h       h       e       g       n       f       c       n       p       e       r       f       r       n       c       e       g       n       b       e       w       e       e       n       e       e       p       l       d       l       e       l       d       l       d       l       n       l       d       l       f       n       n       n       e       e       p       l       d       l       e       l       i       i       s       l       d       l       c       p       n       n       b       f       g       s       l       d       l       n       h       e       u       p       e       r       r       f       l       d       l       f       c       p       r       e       w       h       d       l       d       l       v       e       r       f       e       h       e       e       f       f       e       c       v       e       n       e       f       e       n       e       n       l       e       r       n       n       g       n       u       r       r       e       e       b       e       e       l       f       r       l       d       l       r       e       p       e       c       v       e       l       t       b       l       e       2       m       a       e       f       g       e       e       n       c       p       r       n       n       m       r       p       h       2       4       m       e       h       i       i       s       l       d       l       1       1       c       p       n       n       1       1       b       f       g       s       l       d       l       6       d       l       d       l       v       g       g       f       c       e       5       l       d       l       f       v       g       g       f       c       e       u       r       m       a       e       5       6       7       x       c       2       x       b       1       0       1       5       4       8       7       x       c       2       x       b       1       0       3       1       3       9       4       x       c       2       x       b       1       0       0       5       2       4       2       x       c       2       x       b       1       0       0       1       2       2       4       x       c       2       x       b       1       0       0       2       a       h       e       r       b       u       n       f       g       e       n       e       r       n       e       h       n       c       v       e       r       u       n       b       l       n       c       e       n       m       r       p       h       n       g       e       e       n       e       h       1       3       1       4       1       5       r       e       e       v       l       u       e       n       u       b       e       f       m       r       p       h       c       l       l       e       m       r       p       h       s       u       b       f       r       h       r       w       h       c       h       c       n       f       2       0       1       6       0       e       l       e       c       e       f       c       l       g       e       v       h       e       n       f       l       u       e       n       c       e       f       u       n       b       l       n       c       e       r       b       u       n       t       h       e       b       e       p       e       r       f       r       n       c       e       r       e       p       r       e       n       m       r       p       h       s       u       b       g       v       e       n       b       d       2       l       d       l       1       5       e       p       e       n       e       n       l       d       l       e       h       a       d       2       l       d       l       u       e       h       e       u       p       u       f       h       e       x       e       2       x       8       0       x       9       c       f       c       7       x       e       2       x       8       0       x       9       d       l       e       r       n       a       l       e       x       n       e       2       1       h       e       f       c       e       g       e       f       e       u       r       e       h       e       r       e       w       e       n       e       g       r       e       l       d       l       f       w       h       a       l       e       x       n       e       f       l       l       w       n       g       h       e       e       x       p       e       r       e       n       e       n       g       u       e       n       d       2       l       d       l       w       e       e       v       l       u       e       u       r       l       d       l       f       n       h       e       c       p       e       r       n       c       l       u       n       g       b       h       s       l       l       n       l       d       l       b       e       e       h       u       n       e       r       x       f       f       e       r       e       n       r       n       n       g       e       r       1       0       6       0       a       l       l       f       h       e       c       p       e       r       r       e       r       n       e       n       h       e       e       e       e       p       f       e       u       r       e       u       e       b       d       2       l       d       l       a       c       n       b       e       e       e       n       f       r       t       b       l       e       3       u       r       l       d       l       f       g       n       f       c       n       l       u       p       e       r       f       r       h       e       r       f       r       l       l       r       n       n       g       e       r       n       e       h       h       e       g       e       n       e       r       e       g       e       r       f       g       u       r       e       3       m       a       e       f       g       e       e       n       c       p       r       n       n       b       u       n       r       e       u       n       l       r       b       u       n       m       r       p       h       s       u       b       n       h       e       l       b       e       l       r       b       u       n       u       e       n       t       r       n       n       g       e       r       m       e       h       s       e       c       4       1       r       e       x       u       r       e       r       b       u       n       1       0       2       0       3       0       4       0       5       0       6       0       t       h       e       p       r       p       e       e       h       l       d       l       f       c       h       e       v       e       a       a       s       2       2       4       9       0       8       1       4       7       6       1       6       4       6       5       0       7       4       5       5       5       3       4       4       6       9       0       4       4       0       6       1       h       e       e       f       h       e       r       r       e       u       l       n       b       h       f       l       a       r       r       1       2       4       7       5       0       1       4       6       1       1       2       4       5       1       3       1       4       4       2       7       3       4       3       5       0       0       4       2       9       4       9       i       i       s       a       l       d       l       9       4       1       7       9       1       4       1       6       8       3       4       1       2       2       8       4       1       1       0       7       4       1       0       2       4       4       0       9       0       2       h       e       w       h       c       h       v       e       r       f       e       h       u       r       e       l       d       2       l       d       l       1       5       4       1       0       8       0       3       9       8       5       7       3       9       2       0       4       3       8       7       1       2       3       8       5       6       0       3       8       3       8       5       h       h       e       b       l       e       l       n       g       e       n       e       r       l       l       d       l       f       u       r       3       8       4       9       5       3       6       2       2       0       3       3       9       9       1       3       2       4       0       1       3       1       9       1       7       3       1       2       2       4       f       r       f       l       b       e       l       r       b       u       n       4       3       t       e       c       p       l       e       x       l       e       h       n       b       b       e       h       e       r       e       e       e       p       h       n       h       e       b       c       h       z       e       r       e       p       e       c       v       e       l       e       c       h       r       e       e       h       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       p       l       n       e       n       2       h       x       e       2       x       8       8       x       9       2       1       l       e       f       n       e       l       e       d       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       f       r       n       e       r       e       e       n       n       e       p       l       e       h       e       c       p       l       e       x       f       f       r       w       r       p       n       b       c       k       w       r       p       r       e       o       d       d       1       x       c       3       x       9       7       c       o       d       x       c       3       x       9       7       c       n       o       d       1       x       c       3       x       9       7       c       d       x       c       3       x       9       7       c       o       d       x       c       3       x       9       7       c       r       e       p       e       c       v       e       l       s       f       r       k       r       e       e       n       n       b       b       c       h       e       h       e       c       p       l       e       x       f       f       r       w       r       n       b       c       k       w       r       p       o       d       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       t       h       e       c       p       l       e       x       f       n       e       r       n       u       p       e       l       e       f       n       e       r       e       o       n       b       x       c       3       x       9       7       b       x       c       3       x       9       7       k       x       c       3       x       9       7       c       x       c       3       x       9       7       d       1       o       d       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       t       h       u       h       e       c       p       l       e       x       f       r       h       e       r       n       n       g       p       r       c       e       u       r       e       n       e       e       p       c       h       n       b       b       c       h       e       n       h       e       e       n       g       p       r       c       e       u       r       e       n       e       p       l       e       r       e       o       d       x       c       3       x       9       7       c       x       c       3       x       9       7       k       x       c       3       x       9       7       n       b       x       c       3       x       9       7       b       n       o       d       x       c       3       x       9       7       c       x       c       3       x       9       7       k       r       e       p       e       c       v       e       l       l       d       l       f       r       e       e       f       f       c       e       n       o       n       m       r       p       h       s       u       b       1       2       6       3       6       r       n       n       g       g       e       8       4       2       4       e       n       g       g       e       u       r       e       l       n       l       k       e       5       2       5       0       f       r       r       n       n       g       2       5       0       0       0       e       r       n       n       8       f       r       e       n       g       l       l       8       4       2       4       g       e       4       4       p       r       e       e       r       d       c       u       n       n       w       w       e       c       u       h       e       n       f       l       u       e       n       c       e       f       p       r       e       e       r       e       n       g       n       p       e       r       f       r       n       c       e       w       e       r       e       p       r       h       e       r       e       u       l       f       r       n       g       p       r       e       c       n       n       m       v       e       e       u       r       e       b       k       l       n       g       e       e       n       n       m       r       p       h       s       u       b       w       h       6       0       r       n       n       g       e       r       e       u       r       e       b       m       a       e       f       r       f       f       e       r       e       n       p       r       e       e       r       e       n       g       n       h       e       c       n       t       r       e       e       n       u       b       e       r       a       f       r       e       n       e       n       e       b       l       e       e       l       n       e       c       e       r       n       v       e       g       e       h       w       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       h       e       r       e       e       n       u       b       e       r       u       e       n       f       r       e       n       e       h       w       e       c       u       e       n       s       e       c       2       h       e       e       n       e       b       l       e       r       e       g       l       e       r       n       f       r       e       p       r       p       e       n       n       d       f       2       0       f       f       e       r       e       n       f       r       u       r       t       h       e       r       e       f       r       e       n       e       c       e       r       e       e       w       h       c       h       e       n       e       b       l       e       r       e       g       b       e       e       r       l       e       r       n       f       r       e       t       w       r       h       e       n       w       e       r       e       p       l       c       e       u       r       e       n       e       b       l       e       r       e       g       n       l       d       l       f       b       h       e       n       e       u       e       n       n       d       f       n       n       e       h       e       h       n       d       f       l       d       l       t       h       e       c       r       r       e       p       n       n       g       h       l       l       w       e       l       n       e       b       n       d       f       l       d       l       w       e       f       x       h       e       r       p       r       e       e       r       e       r       e       e       e       p       h       n       8       x       0       c       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       h       e       e       f       u       l       e       n       g       a       h       w       n       n       f       g       4       u       r       e       n       e       b       l       e       r       e       g       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       h       e       n       e       u       e       n       n       d       f       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       o       b       e       r       v       e       f       r       f       g       4       h       e       p       e       r       f       r       n       c       e       f       l       d       l       f       c       n       b       e       p       r       v       e       b       u       n       g       r       e       r       e       e       b       u       h       e       p       r       v       e       e       n       b       e       c       e       n       c       r       e       n       g       l       l       l       e       r       n       l       l       e       r       t       h       e       r       e       f       r       e       u       n       g       u       c       h       l       r       g       e       r       e       n       e       b       l       e       e       n       e       l       b       g       p       r       v       e       e       n       o       n       m       v       e       h       e       n       u       b       e       r       f       r       e       e       k       1       0       0       k       l       0       0       7       0       v       k       2       0       k       l       0       0       7       1       n       e       h       n       l       l       r       n       f       r       e       b       e       e       h       u       e       l       r       g       e       n       u       b       e       r       f       r       e       e       e       g       s       h       n       e       l       2       5       b       n       e       v       e       r       g       p       e       e       n       r       e       u       l       f       r       e       p       h       g       e       b       n       l       3       e       c       n       r       e       e       t       r       e       e       e       p       h       t       r       e       e       e       p       h       n       h       e       r       p       r       n       p       r       e       e       r       f       r       e       c       n       r       e       e       i       n       l       d       l       f       h       e       r       e       n       p       l       c       c       n       r       n       b       e       w       e       e       n       r       e       e       e       p       h       h       n       u       p       u       u       n       n       u       b       e       r       f       h       e       f       e       u       r       e       l       e       r       n       n       g       f       u       n       c       n       x       c       f       x       8       4       x       c       f       x       8       4       x       e       2       x       8       9       x       a       5       2       h       x       e       2       x       8       8       x       9       2       1       x       e       2       x       8       8       x       9       2       1       t       c       u       h       e       n       f       l       u       e       n       c       e       f       r       e       e       e       p       h       h       e       p       e       r       f       r       n       c       e       f       l       d       l       f       w       e       e       x       c       f       x       8       4       2       h       x       e       2       x       8       8       x       9       2       1       n       f       x       r       e       e       n       u       b       e       r       k       1       n       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       b       v       r       n       g       r       e       e       e       p       h       h       w       n       n       f       g       4       b       w       e       e       e       h       h       e       p       e       r       f       r       n       c       e       f       r       p       r       v       e       h       e       n       e       c       r       e       e       w       h       h       e       n       c       r       e       e       f       h       e       r       e       e       e       p       h       t       h       e       r       e       n       h       e       r       e       e       e       p       h       n       c       r       e       e       h       e       e       n       n       f       l       e       r       n       e       f       e       u       r       e       n       c       r       e       e       e       x       p       n       e       n       l       l       w       h       c       h       g       r       e       l       n       c       r       e       e       h       e       r       n       n       g       f       f       c       u       l       s       u       n       g       u       c       h       l       r       g       e       r       e       p       h       l       e       b       p       e       r       f       r       n       c       e       o       n       m       v       e       r       e       e       e       p       h       h       1       8       k       l       0       1       1       6       2       v       h       9       k       l       0       0       8       3       1       f       g       u       r       e       4       t       h       e       p       e       r       f       r       n       c       e       c       h       n       g       e       f       g       e       e       n       n       m       r       p       h       s       u       b       n       r       n       g       p       r       e       c       n       n       m       v       e       b       v       r       n       g       r       e       e       n       u       b       e       r       n       b       r       e       e       e       p       h       o       u       r       p       p       r       c       h       l       d       l       f       l       d       l       f       c       n       p       r       v       e       h       e       p       e       r       f       r       n       c       e       b       u       n       g       r       e       r       e       e       w       h       l       e       u       n       g       h       e       e       n       e       b       l       e       r       e       g       p       r       p       e       n       n       d       f       n       d       f       l       d       l       n       d       f       l       d       l       e       v       e       n       l       e       w       r       e       p       e       r       f       r       n       c       e       h       n       n       e       f       r       n       g       l       e       r       e       e       5       c       n       c       l       u       n       w       e       p       r       e       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       e       n       v       e       l       l       b       e       l       r       b       u       n       l       e       r       n       n       g       l       g       r       h       n       p       r       e       b       f       f       e       r       e       n       b       l       e       e       c       n       r       e       e       w       e       e       f       n       e       r       b       u       n       b       e       l       f       u       n       c       n       f       r       h       e       f       r       e       n       f       u       n       h       h       e       l       e       f       n       e       p       r       e       c       n       c       n       b       e       p       z       e       v       v       r       n       l       b       u       n       n       g       w       h       c       h       e       n       b       l       e       l       l       h       e       r       e       e       n       h       e       f       e       u       r       e       h       e       u       e       b       e       l       e       r       n       e       j       n       l       n       n       e       n       e       n       n       n       e       r       e       x       p       e       r       e       n       l       r       e       u       l       h       w       e       h       e       u       p       e       r       r       f       u       r       l       g       r       h       f       r       e       v       e       r       l       l       d       l       k       n       r       e       l       e       c       p       u       e       r       v       n       p       p       l       c       n       n       v       e       r       f       e       u       r       e       l       h       h       e       b       l       e       l       n       g       e       n       e       r       l       f       r       f       l       b       e       l       r       b       u       n       a       c       k       n       w       l       e       g       e       e       n       t       h       w       r       k       w       u       p       p       r       e       n       p       r       b       h       e       n       n       l       n       u       r       l       s       c       e       n       c       e       f       u       n       n       f       c       h       n       n       6       1       6       7       2       3       3       6       n       p       r       b       x       e       2       x       8       0       x       9       c       c       h       e       n       g       u       n       g       x       e       2       x       8       0       x       9       d       p       r       j       e       c       u       p       p       r       e       b       s       h       n       g       h       m       u       n       c       p       l       e       u       c       n       c       n       n       s       h       n       g       h       e       u       c       n       d       e       v       e       l       p       e       n       f       u       n       n       n       1       5       c       g       4       3       n       n       p       r       b       o       n       r       n       0       0       0       1       4       1       5       1       2       3       5       6       r       e       f       e       r       e       n       c       e       1       y       a       n       d       g       e       n       s       h       p       e       q       u       n       z       n       n       r       e       c       g       n       n       w       h       r       n       z       e       r       e       e       n       e       u       r       l       c       p       u       n       9       7       1       5       4       5       x       e       2       x       8       0       x       9       3       1       5       8       8       1       9       9       7       2       a       l       b       e       r       g       e       r       s       d       p       e       r       n       v       j       d       p       e       r       a       x       u       e       n       r       p       p       p       r       c       h       n       u       r       l       l       n       g       u       g       e       p       r       c       e       n       g       c       p       u       n       l       l       n       g       u       c       2       2       1       3       9       x       e       2       x       8       0       x       9       3       7       1       1       9       9       6       3       l       b       r       e       n       r       n       f       r       e       m       c       h       n       e       l       e       r       n       n       g       4       5       1       5       x       e       2       x       8       0       x       9       3       3       2       2       0       0       1       4       a       c       r       n       n       j       s       h       n       d       e       c       n       f       r       e       f       r       c       p       u       e       r       v       n       n       m       e       c       l       i       g       e       a       n       l       s       p       r       n       g       e       r       2       0       1       3       5       b       b       g       c       x       n       g       c       w       x       e       j       w       u       n       x       g       e       n       g       d       e       e       p       l       b       e       l       r       b       u       n       l       e       r       n       n       g       w       h       l       b       e       l       b       g       u       r       x       v       1       6       1       1       0       1       7       3       1       2       0       1       7       6       x       g       e       n       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       e       e       e       t       r       n       k       n       w       l       d       e       n       g       2       8       7       1       7       3       4       x       e       2       x       8       0       x       9       3       1       7       4       8       2       0       1       6       9       x       0       c       7       x       g       e       n       g       n       p       h       u       p       r       e       r       e       l       e       e       p       r       e       c       n       f       c       r       w       p       n       n       n       v       e       b       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       i       j       c       a       i       p       g       e       3       5       1       1       x       e       2       x       8       0       x       9       3       3       5       1       7       2       0       1       5       8       x       g       e       n       g       k       s       h       m       l       e       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       i       n       p       r       c       a       a       a       i       2       0       1       0       9       x       g       e       n       g       q       w       n       g       n       y       x       f       c       l       g       e       e       n       b       p       v       e       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       c       i       c       p       r       p       g       e       4       4       6       5       x       e       2       x       8       0       x       9       3       4       4       7       0       2       0       1       4       1       0       x       g       e       n       g       n       y       x       h       e       p       e       e       n       b       e       n       u       l       v       r       e       l       b       e       l       r       b       u       n       i       n       p       r       c       c       v       p       r       p       g       e       1       8       3       7       x       e       2       x       8       0       x       9       3       1       8       4       2       2       0       1       4       1       1       x       g       e       n       g       c       y       n       n       z       z       h       u       f       c       l       g       e       e       n       b       l       e       r       n       n       g       f       r       l       b       e       l       r       b       u       n       i       e       e       e       t       r       n       p       e       r       n       a       n       l       m       c       h       i       n       e       l       l       3       5       1       0       2       4       0       1       x       e       2       x       8       0       x       9       3       2       4       1       2       2       0       1       3       1       2       g       g       u       y       f       u       c       r       d       e       r       n       t       s       h       u       n       g       i       g       e       b       e       h       u       n       g       e       e       n       b       n       f       l       l       e       r       n       n       g       n       l       c       l       l       j       u       e       r       b       u       r       e       g       r       e       n       i       e       e       e       t       r       n       i       g       e       p       r       c       e       n       g       1       7       7       1       1       7       8       x       e       2       x       8       0       x       9       3       1       1       8       8       2       0       0       8       1       3       g       g       u       n       g       m       u       h       u       n       g       e       e       n       w       h       h       e       n       f       l       u       e       n       c       e       c       r       r       c       e       n       g       e       n       e       r       i       n       c       v       p       r       w       r       k       h       p       p       g       e       7       1       x       e       2       x       8       0       x       9       3       7       8       2       0       1       0       1       4       g       g       u       n       c       z       h       n       g       a       u       n       c       r       p       p       u       l       n       g       e       e       n       i       n       p       r       c       c       v       p       r       p       g       e       4       2       5       7       x       e       2       x       8       0       x       9       3       4       2       6       3       2       0       1       4       1       5       z       h       e       x       l       z       z       h       n       g       f       w       u       x       g       e       n       g       y       z       h       n       g       m       h       y       n       g       n       y       z       h       u       n       g       d       e       p       e       n       e       n       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       i       e       e       e       t       r       n       n       i       g       e       p       r       c       e       n       g       2       0       1       7       1       6       t       k       h       r       n       e       c       n       f       r       e       i       n       p       r       c       i       c       d       a       r       p       g       e       2       7       8       x       e       2       x       8       0       x       9       3       2       8       2       1       9       9       5       1       7       t       k       h       t       h       e       r       n       u       b       p       c       e       e       h       f       r       c       n       r       u       c       n       g       e       c       n       f       r       e       i       e       e       e       t       r       n       p       e       r       n       a       n       l       m       c       h       i       n       e       l       l       2       0       8       8       3       2       x       e       2       x       8       0       x       9       3       8       4       4       1       9       9       8       1       8       y       j       e       s       h       e       l       h       e       r       j       d       n       h       u       e       s       k       r       e       v       j       l       n       g       r       g       r       h       c       k       s       g       u       r       r       n       t       d       r       r       e       l       l       c       f       f       e       c       n       v       l       u       n       l       r       c       h       e       c       u       r       e       f       r       f       f       e       u       r       e       e       b       e       n       g       r       x       v       p       r       e       p       r       n       r       x       v       1       4       0       8       5       0       9       3       2       0       1       4       1       9       m       i       j       r       n       z       g       h       h       r       n       t       s       j       k       k       l       n       l       k       s       u       l       a       n       n       r       u       c       n       v       r       n       l       e       h       f       r       g       r       p       h       c       l       e       l       m       c       h       n       e       l       e       r       n       n       g       3       7       2       1       8       3       x       e       2       x       8       0       x       9       3       2       3       3       1       9       9       9       2       0       p       k       n       c       h       e       e       r       m       f       e       r       u       a       c       r       n       n       s       r       b       u       l       x       c       3       x       b       2       d       e       e       p       n       e       u       r       l       e       c       n       f       r       e       i       n       p       r       c       i       c       c       v       p       g       e       1       4       6       7       x       e       2       x       8       0       x       9       3       1       4       7       5       2       0       1       5       2       1       a       k       r       z       h       e       v       k       i       s       u       k       e       v       e       r       n       g       e       h       n       n       i       g       e       n       e       c       l       f       c       n       w       h       e       e       p       c       n       v       l       u       n       l       n       e       u       r       l       n       e       w       r       k       i       n       p       r       c       n       i       p       s       p       g       e       1       1       0       6       x       e       2       x       8       0       x       9       3       1       1       1       4       2       0       1       2       2       2       a       l       n       c       d       r       g       n       v       n       c       c       h       r       u       l       u       c       p       r       n       g       f       f       e       r       e       n       c       l       f       e       r       f       r       u       c       g       e       e       n       i       e       e       e       t       r       n       n       c       b       e       r       n       e       c       3       4       1       6       2       1       x       e       2       x       8       0       x       9       3       6       2       8       2       0       0       4       2       3       o       m       p       r       k       h       a       v       e       l       n       a       z       e       r       n       d       e       e       p       f       c       e       r       e       c       g       n       n       i       n       p       r       c       b       m       v       c       p       g       e       4       1       1       x       e       2       x       8       0       x       9       3       4       1       1       2       2       0       1       5       2       4       k       r       c       n       e       k       n       t       t       e       f       e       m       o       r       p       h       a       l       n       g       u       n       l       g       e       b       e       f       n       r       l       u       l       g       e       p       r       g       r       e       n       i       n       p       r       c       f       g       p       g       e       3       4       1       x       e       2       x       8       0       x       9       3       3       4       5       2       0       0       6       2       5       j       s       h       n       a       w       f       z       g       b       b       n       m       c       k       t       s       h       r       p       m       f       n       c       c       h       r       m       r       e       a       k       p       n       n       a       b       l       k       e       r       e       l       e       h       u       n       p       e       r       e       c       g       n       n       n       p       r       f       r       n       g       l       e       e       p       h       g       e       i       n       p       r       c       c       v       p       r       p       g       e       1       2       9       7       x       e       2       x       8       0       x       9       3       1       3       0       4       2       0       1       1       2       6       g       t       u       k       n       i       k       k       m       u       l       l       b       e       l       c       l       f       c       n       a       n       v       e       r       v       e       w       i       n       e       r       n       n       l       j       u       r       n       l       f       d       w       r       e       h       u       n       g       n       m       n       n       g       3       3       1       x       e       2       x       8       0       x       9       3       1       3       2       0       0       7       2       7       c       x       n       g       x       g       e       n       g       n       h       x       u       e       l       g       c       b       n       g       r       e       g       r       e       n       f       r       l       b       e       l       r       b       u       n       l       e       r       n       n       g       i       n       p       r       c       c       v       p       r       p       g       e       4       4       8       9       x       e       2       x       8       0       x       9       3       4       4       9       7       2       0       1       6       2       8       x       y       n       g       x       g       e       n       g       n       d       z       h       u       s       p       r       c       n       n       l       e       n       e       r       g       l       b       e       l       r       b       u       n       l       e       r       n       n       g       f       r       g       e       e       n       i       n       p       r       c       i       j       c       a       i       p       g       e       2       2       5       9       x       e       2       x       8       0       x       9       3       2       2       6       5       2       0       1       6       2       9       a       l       y       u       l       l       e       n       a       r       n       g       r       j       n       t       h       e       c       n       c       v       e       c       n       v       e       x       p       r       c       e       u       r       e       n       e       u       r       l       c       p       u       n       1       5       4       9       1       5       x       e       2       x       8       0       x       9       3       9       3       6       2       0       0       3       3       0       y       z       h       u       h       x       u       e       n       x       g       e       n       g       e       n       r       b       u       n       r       e       c       g       n       n       f       r       f       c       l       e       x       p       r       e       n       i       n       p       r       c       m       m       p       g       e       1       2       4       7       x       e       2       x       8       0       x       9       3       1       2       5       0       2       0       1       5       1       0       x       0       c   ']], shape=(1, 1), dtype=string)\n",
      "AdaBoost Accuracy on Test set -> 0.12\n",
      "XGBoost Accuracy on Test set -> 0.32\n",
      "RandomForest Accuracy on Test set -> 0.36\n",
      "DecisionTree Accuracy on Test set -> 0.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for key in prepro_functions_dict_comb:\n",
    "    print(\"\\n\\n* * * * EVALUATION USING\", key, \"AS PREPROCESSING FUNCTION * * * *\")\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test_text, y_test)).shuffle(buffer_size=len(test_df), seed=1, reshuffle_each_iteration=False).batch(1).take(50)\n",
    "\n",
    "    # Preprocess training set to build a dictionary.\n",
    "    vectorize_layer = preprocess_and_adapt_ts(prepro_functions_dict_comb[key],train_ds)\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "    \n",
    "    print(\"\\n\\n***** FINISHED PROCESSING AND ADAPTING THE TRAINING SET, THE SIMULATION BEGINS *******\")\n",
    "    # Print a raw and a preprocessed sample.\n",
    "    for element in train_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "\n",
    "      print(\"Sample considered is: \", author_batch)\n",
    "      print(\"Preprocessed: \", str(prepro_functions_dict_comb[key](author_batch.numpy())))\n",
    "      break\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train_text, y_train)).shuffle(buffer_size=len(train_df), seed=1, reshuffle_each_iteration=False).batch(1).take(200)\n",
    "\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "    model.add(vectorize_layer)\n",
    "\n",
    "    training_labels=[]\n",
    "    training_samples=[]\n",
    "\n",
    "    max_features=len(vectorize_layer.get_vocabulary()) + 1\n",
    "\n",
    "    for element in train_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "        \n",
    "      text_vect_out = vectorize_layer(author_batch)\n",
    "\n",
    "      training_labels.append(label.numpy())\n",
    "      current_sample=np.zeros(max_features)\n",
    "      for current_token in text_vect_out[0][:].numpy():\n",
    "        current_sample[current_token]+=1\n",
    "      training_samples.append(current_sample)\n",
    "\n",
    "    training_labels=np.array(training_labels)\n",
    "    training_samples=np.array(training_samples)\n",
    "\n",
    "    test_labels=[]\n",
    "    test_samples=[]\n",
    "\n",
    "    for element in test_ds:\n",
    "      authorDocument=element[0]\n",
    "      label=element[1]\n",
    "      author_batch = tf.expand_dims(authorDocument, 0)\n",
    "\n",
    "      text_vect_out = vectorize_layer(author_batch)\n",
    "\n",
    "      test_labels.append(label.numpy())\n",
    "      current_sample=np.zeros(max_features)\n",
    "      for current_token in text_vect_out[0][:].numpy():\n",
    "        current_sample[current_token]+=1\n",
    "      test_samples.append(current_sample)\n",
    "\n",
    "    test_labels=np.array(test_labels)\n",
    "    test_samples=np.array(test_samples)\n",
    "\n",
    "    models = {\n",
    "        \"AdaBoost\": AdaBoostClassifier(random_state=0),\n",
    "        \"XGBoost\": XGBClassifier(random_state=0),\n",
    "        \"RandomForest\": RandomForestClassifier(random_state=0),\n",
    "        \"DecisionTree\": DecisionTreeClassifier(random_state=0)\n",
    "    }\n",
    "\n",
    "    for name, model in models.items():\n",
    "        model.fit(training_samples, training_labels.ravel())\n",
    "        acc = model.score(test_samples, test_labels)\n",
    "        model_results[key][name].append(acc)\n",
    "        print(f\"{name} Accuracy on Test set ->\", acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a141e6c-b4ef-4fe2-95d2-9652dd0b9801",
   "metadata": {},
   "source": [
    "# Результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e46a58-d968-429c-90a2-a5553b89ee49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPRO FUNCTION         | MODEL NAME        | ACCURANCY   \n",
      "------------------------------------------------------------\n",
      "DON                     | AdaBoost          | 0.1400\n",
      "DON                     | XGBoost           | 0.2200\n",
      "DON                     | RandomForest      | 0.3000\n",
      "DON                     | DecisionTree      | 0.1800\n",
      "------------------------------------------------------------\n",
      "LOW                     | AdaBoost          | 0.1400\n",
      "LOW                     | XGBoost           | 0.3200\n",
      "LOW                     | RandomForest      | 0.4200\n",
      "LOW                     | DecisionTree      | 0.2800\n",
      "------------------------------------------------------------\n",
      "RSW                     | AdaBoost          | 0.1200\n",
      "RSW                     | XGBoost           | 0.3600\n",
      "RSW                     | RandomForest      | 0.3000\n",
      "RSW                     | DecisionTree      | 0.1600\n",
      "------------------------------------------------------------\n",
      "STM                     | AdaBoost          | 0.1000\n",
      "STM                     | XGBoost           | 0.2600\n",
      "STM                     | RandomForest      | 0.3000\n",
      "STM                     | DecisionTree      | 0.2600\n",
      "------------------------------------------------------------\n",
      "LOW_RSW                 | AdaBoost          | 0.1200\n",
      "LOW_RSW                 | XGBoost           | 0.2800\n",
      "LOW_RSW                 | RandomForest      | 0.2600\n",
      "LOW_RSW                 | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "LOW_STM                 | AdaBoost          | 0.1000\n",
      "LOW_STM                 | XGBoost           | 0.2600\n",
      "LOW_STM                 | RandomForest      | 0.3000\n",
      "LOW_STM                 | DecisionTree      | 0.2600\n",
      "------------------------------------------------------------\n",
      "RSW_LOW                 | AdaBoost          | 0.2200\n",
      "RSW_LOW                 | XGBoost           | 0.3600\n",
      "RSW_LOW                 | RandomForest      | 0.3400\n",
      "RSW_LOW                 | DecisionTree      | 0.2400\n",
      "------------------------------------------------------------\n",
      "RSW_STM                 | AdaBoost          | 0.1200\n",
      "RSW_STM                 | XGBoost           | 0.3200\n",
      "RSW_STM                 | RandomForest      | 0.3600\n",
      "RSW_STM                 | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "STM_LOW                 | AdaBoost          | 0.1200\n",
      "STM_LOW                 | XGBoost           | 0.2000\n",
      "STM_LOW                 | RandomForest      | 0.4000\n",
      "STM_LOW                 | DecisionTree      | 0.2000\n",
      "------------------------------------------------------------\n",
      "STM_RSW                 | AdaBoost          | 0.1800\n",
      "STM_RSW                 | XGBoost           | 0.3400\n",
      "STM_RSW                 | RandomForest      | 0.3200\n",
      "STM_RSW                 | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "LOW_STM_RSW             | AdaBoost          | 0.1800\n",
      "LOW_STM_RSW             | XGBoost           | 0.3400\n",
      "LOW_STM_RSW             | RandomForest      | 0.3200\n",
      "LOW_STM_RSW             | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "LOW_RSW_STM             | AdaBoost          | 0.1200\n",
      "LOW_RSW_STM             | XGBoost           | 0.3400\n",
      "LOW_RSW_STM             | RandomForest      | 0.3000\n",
      "LOW_RSW_STM             | DecisionTree      | 0.1800\n",
      "------------------------------------------------------------\n",
      "STM_LOW_RSW             | AdaBoost          | 0.1800\n",
      "STM_LOW_RSW             | XGBoost           | 0.3400\n",
      "STM_LOW_RSW             | RandomForest      | 0.3200\n",
      "STM_LOW_RSW             | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "STM_RSW_LOW             | AdaBoost          | 0.1800\n",
      "STM_RSW_LOW             | XGBoost           | 0.3400\n",
      "STM_RSW_LOW             | RandomForest      | 0.3200\n",
      "STM_RSW_LOW             | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "RSW_LOW_STM             | AdaBoost          | 0.1200\n",
      "RSW_LOW_STM             | XGBoost           | 0.3200\n",
      "RSW_LOW_STM             | RandomForest      | 0.3600\n",
      "RSW_LOW_STM             | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n",
      "RSW_STM_LOW             | AdaBoost          | 0.1200\n",
      "RSW_STM_LOW             | XGBoost           | 0.3200\n",
      "RSW_STM_LOW             | RandomForest      | 0.3600\n",
      "RSW_STM_LOW             | DecisionTree      | 0.2200\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_result_per_model = {}\n",
    "\n",
    "print(\"PREPRO FUNCTION         | MODEL NAME        | ACCURANCY   \")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for prepro_func in prepro_functions_dict_comb:\n",
    "    for model_name, scores in model_results[prepro_func].items():\n",
    "        acc = round(scores[-1], 4)\n",
    "        if model_name not in best_result_per_model or acc > best_result_per_model[model_name][1]:\n",
    "            best_result_per_model[model_name] = (prepro_func, acc)\n",
    "        print(f\"{prepro_func:23} | {model_name:17} | {acc:>6.4f}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8067c5c2-1044-4907-a1ea-27a0947352fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL NAME         | BEST PREPRO FUNCTION   | MAX ACCURACY\n",
      "------------------------------------------------------------\n",
      "AdaBoost           | RSW_LOW                | 0.2200\n",
      "XGBoost            | RSW                    | 0.3600\n",
      "RandomForest       | LOW                    | 0.4200\n",
      "DecisionTree       | LOW                    | 0.2800\n"
     ]
    }
   ],
   "source": [
    "print(\"MODEL NAME         | BEST PREPRO FUNCTION   | MAX ACCURACY\")\n",
    "print(\"-\" * 60)\n",
    "for model_name, (best_func, acc) in best_result_per_model.items():\n",
    "    print(f\"{model_name:18} | {best_func:22} | {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf89c4-6d92-4c96-b322-82c0f3fe3b64",
   "metadata": {},
   "source": [
    "# Сохраним результаты в отдельную таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bbdd74e-45bf-4701-9d07-c2fdd9f930fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepo_path = \"../reports/preprocessing_combinations/ccdv_arxiv-classification/\"\n",
    "os.makedirs(prepo_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d1a93e0d-d8b3-4252-958f-e8d13753e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for prepo_func, model_scores in model_results.items():\n",
    "    for model_name, acc in model_scores.items():\n",
    "        rows.append({\n",
    "            \"prepo_func\": prepo_func,\n",
    "            \"model\": model_name,\n",
    "            \"accuracy\": round(acc[-1], 4)\n",
    "        })\n",
    "\n",
    "df_full = pd.DataFrame(rows)\n",
    "df_full.to_csv(f\"{prepo_path}full.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80983d8d-b1aa-4189-b327-62c49c2abe09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = pd.DataFrame([\n",
    "    {\"model\": model, \"best_prepo_func\": func, \"max_accuracy\": acc}\n",
    "    for model, (func, acc) in best_result_per_model.items()\n",
    "])\n",
    "df_best.to_csv(f\"{prepo_path}best.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b5ad11-e34c-4d83-a115-cae1a65e5fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
