{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb4762aa-9748-409f-81c9-f5458b5ae4d6",
   "metadata": {},
   "source": [
    "# Курсовая"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b371350-1a98-488c-82e3-0b5069cd16ea",
   "metadata": {},
   "source": [
    "## Шаг 0: **Загрузка данных**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8d0da03-9327-4cfd-b20c-c36e1f50f73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "from datasets import load_dataset\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba1ca744-a08b-4d0b-ad36-986c0465c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset принимает исходный файл с набором данных\n",
    "# split принимает часть датасета, которую нужно сохранить (train, test)\n",
    "# folder_name создает папку с заданным именем, где хранятся таблицы из одного набора данных (обычно train и test)\n",
    "def save_dataset(dataset, split, folder_name, base_dir=\"data/raw\"): # по умолчанию сохраняем в директорию с \"сырыми\" датасетами\n",
    "    base_path = os.path.join(base_dir, folder_name)\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    csv_path = os.path.join(base_path, f\"{split}.csv\")\n",
    "    df = pd.DataFrame(dataset[split])     \n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"{folder_name}/{split}.csv успешно сохранен\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc775b4b-7f72-4082-ae33-b726e92b635d",
   "metadata": {},
   "source": [
    "### Датасет: **TweetEval: Sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eadc084c-8f77-430c-a62d-1527f3ac807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета TweetEval: Sentiment:\n",
      "Мининимум: 1 слов; Среднее: 18.308606366258484 слов; Максимум: 35 слов\n"
     ]
    }
   ],
   "source": [
    "tweet_ds = load_dataset(\"tweet_eval\", \"sentiment\", trust_remote_code=True) \n",
    "\n",
    "tweet_train = pd.DataFrame(tweet_ds[\"train\"])\n",
    "tweet_test = pd.DataFrame(tweet_ds[\"test\"])\n",
    "tweet_df = pd.concat([tweet_train, tweet_test], ignore_index=True)\n",
    "\n",
    "word_counts = tweet_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"Статистика по текстам датасета TweetEval: Sentiment:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "585170c3-fb27-4940-8499-6b36f2cd6418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропорции классов: до и после\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>До, %</th>\n",
       "      <th>После, %</th>\n",
       "      <th>Отклонение, %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.96</td>\n",
       "      <td>45.92</td>\n",
       "      <td>-0.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34.93</td>\n",
       "      <td>34.94</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19.11</td>\n",
       "      <td>19.13</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       До, %  После, %  Отклонение, %\n",
       "label                                \n",
       "1      45.96     45.92          -0.04\n",
       "2      34.93     34.94           0.01\n",
       "0      19.11     19.13           0.02"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# необходимо сузить диапазон до заданного в курсовой\n",
    "before = tweet_df[\"label\"].value_counts(normalize=True) * 100 # пропорции до\n",
    "tweet_df = tweet_df[tweet_df[\"text\"].str.strip().str.split().str.len().between(5, 30)].reset_index(drop=True) # сужаем диапазон до заданного\n",
    "after = tweet_df[\"label\"].value_counts(normalize=True) * 100 # пропорции после\n",
    "\n",
    "comparison = pd.concat([before, after], axis=1, keys=[\"До, %\", \"После, %\"])\n",
    "comparison[\"Отклонение, %\"] = (comparison[\"После, %\"] - comparison[\"До, %\"])\n",
    "comparison = comparison.round(2)\n",
    "\n",
    "print(\"Пропорции классов: до и после\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "400a5ab1-06c1-496f-bff7-b6a1bbdb245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета TweetEval: Sentiment:\n",
      "Мининимум: 5 слов; Среднее: 18.35560041065618 слов; Максимум: 30 слов\n"
     ]
    }
   ],
   "source": [
    "# проверим что фильтрация применилась\n",
    "word_counts = tweet_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"Статистика по текстам датасета TweetEval: Sentiment:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f2a95e69-3d9c-4c85-b649-6a4ea3bb0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/579phdqx14xch2yg0qz2qr940000gn/T/ipykernel_93474/2345237729.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  tweet_balanced = tweet_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    33.333333\n",
       "1    33.333333\n",
       "2    33.333333\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# дополнительно сбланарсируем классы\n",
    "min_count = tweet_df[\"label\"].value_counts().min()\n",
    "\n",
    "tweet_balanced = tweet_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "tweet_balanced[\"label\"].value_counts(normalize=True) * 100 # пропорции после балансировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0a5f82d2-2235-4c85-afd6-abce8754b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_eval_sentiment/train.csv успешно сохранен\n",
      "tweet_eval_sentiment/test.csv успешно сохранен\n"
     ]
    }
   ],
   "source": [
    "# сохраним результат в таблицы\n",
    "tweet_train, tweet_test = train_test_split(tweet_balanced, test_size=0.3, stratify=tweet_balanced[\"label\"], random_state=42) # вручную разбиваем на train и test\n",
    "dataset = {\"train\": tweet_train, \"test\": tweet_test} # создаем общий датасет чтобы передать в функцию\n",
    "\n",
    "save_dataset(dataset, \"train\", \"tweet_eval_sentiment\") \n",
    "save_dataset(dataset, \"test\", \"tweet_eval_sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffd6dad-e7a1-4e45-a0b2-57151dfd4df9",
   "metadata": {},
   "source": [
    "### Датасет **AG News**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0925e34e-ae80-4478-bfa9-8b157dfd3281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета AG News:\n",
      "Мининимум: 8 слов; Среднее: 37.84 слов; Максимум: 177 слов\n"
     ]
    }
   ],
   "source": [
    "agn_ds = load_dataset(\"ag_news\", trust_remote_code=True) \n",
    "\n",
    "agn_train = pd.DataFrame(agn_ds[\"train\"])\n",
    "agn_test = pd.DataFrame(agn_ds[\"test\"])\n",
    "agn_df = pd.concat([agn_train, agn_test], ignore_index=True)\n",
    "\n",
    "word_counts = agn_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"Статистика по текстам датасета AG News:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed7a5afd-b714-40b8-81af-7f744e1ff591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропорции классов: до и после\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>До, %</th>\n",
       "      <th>После, %</th>\n",
       "      <th>Отклонение, %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.95</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.0</td>\n",
       "      <td>23.09</td>\n",
       "      <td>-1.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.63</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.0</td>\n",
       "      <td>25.33</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       До, %  После, %  Отклонение, %\n",
       "label                                \n",
       "2       25.0     25.95           0.95\n",
       "3       25.0     23.09          -1.91\n",
       "1       25.0     25.63           0.63\n",
       "0       25.0     25.33           0.33"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# необходимо сузить диапазон до заданного в курсовой\n",
    "before = agn_df[\"label\"].value_counts(normalize=True) * 100 # пропорции до\n",
    "agn_df = agn_df[agn_df[\"text\"].str.strip().str.split().str.len().between(30, 120)].reset_index(drop=True) # сужаем диапазон до заданного\n",
    "after = agn_df[\"label\"].value_counts(normalize=True) * 100 # пропорции после\n",
    "\n",
    "comparison = pd.concat([before, after], axis=1, keys=[\"До, %\", \"После, %\"])\n",
    "comparison[\"Отклонение, %\"] = (comparison[\"После, %\"] - comparison[\"До, %\"])\n",
    "comparison = comparison.round(2)\n",
    "\n",
    "print(\"Пропорции классов: до и после\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17a9881a-632f-4af4-b868-dba282ccf81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета AG News:\n",
      "Мининимум: 30 слов; Среднее: 40.465166049563926 слов; Максимум: 120 слов\n"
     ]
    }
   ],
   "source": [
    "# проверим что фильтрация применилась\n",
    "word_counts = agn_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"Статистика по текстам датасета AG News:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8c4e066e-b465-4533-a024-4b409e2245b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/579phdqx14xch2yg0qz2qr940000gn/T/ipykernel_93474/1152321853.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  agn_balanced = agn_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    25.0\n",
       "1    25.0\n",
       "2    25.0\n",
       "3    25.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# дополнительно сбланарсируем классы\n",
    "min_count = agn_df[\"label\"].value_counts().min()\n",
    "\n",
    "agn_balanced = agn_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "agn_balanced[\"label\"].value_counts(normalize=True) * 100 # пропорции после балансировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6ef38fa-c1b9-4d8f-b15f-8d43e55d4c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ag_news/train.csv успешно сохранен\n",
      "ag_news/test.csv успешно сохранен\n"
     ]
    }
   ],
   "source": [
    "# сохраним результат в таблицы\n",
    "agn_train, agn_test = train_test_split(agn_balanced, test_size=0.3, stratify=agn_balanced[\"label\"], random_state=42) # вручную разбиваем на train и test\n",
    "dataset = {\"train\": agn_train, \"test\": agn_test} # создаем общий датасет чтобы передать в функцию\n",
    "\n",
    "save_dataset(dataset, \"train\", \"ag_news\") \n",
    "save_dataset(dataset, \"test\", \"ag_news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773e75dd-f6ad-4f53-9ec8-3bc495ef6d26",
   "metadata": {},
   "source": [
    "### Датасет **20_Newsgroups**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f40b4734-7ae6-4dc6-a946-241ece479027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета 20newsgroups:\n",
      "Мининимум: 0 слов; Среднее: 181.6377480632495 слов; Максимум: 11765 слов\n"
     ]
    }
   ],
   "source": [
    "ng_ds = fetch_20newsgroups(subset=\"all\", remove=('headers','footers','quotes')) # очищаем от нерелевантной информации\n",
    "\n",
    "ng_df = pd.DataFrame({\n",
    "    \"text\": ng_ds.data,\n",
    "    \"label\": [ng_ds.target_names[i] for i in ng_ds.target]\n",
    "})\n",
    "\n",
    "word_counts = ng_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "print(f\"Статистика по текстам датасета 20newsgroups:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1098ae31-41c4-470c-80c4-2426842e0663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропорции классов: до и после\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>До, %</th>\n",
       "      <th>После, %</th>\n",
       "      <th>Отклонение, %</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rec.sport.hockey</th>\n",
       "      <td>5.30</td>\n",
       "      <td>5.43</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soc.religion.christian</th>\n",
       "      <td>5.29</td>\n",
       "      <td>6.51</td>\n",
       "      <td>1.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.motorcycles</th>\n",
       "      <td>5.28</td>\n",
       "      <td>4.79</td>\n",
       "      <td>-0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.sport.baseball</th>\n",
       "      <td>5.27</td>\n",
       "      <td>4.60</td>\n",
       "      <td>-0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.crypt</th>\n",
       "      <td>5.26</td>\n",
       "      <td>6.59</td>\n",
       "      <td>1.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rec.autos</th>\n",
       "      <td>5.25</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.med</th>\n",
       "      <td>5.25</td>\n",
       "      <td>5.95</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.windows.x</th>\n",
       "      <td>5.24</td>\n",
       "      <td>5.80</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.space</th>\n",
       "      <td>5.24</td>\n",
       "      <td>5.03</td>\n",
       "      <td>-0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.os.ms-windows.misc</th>\n",
       "      <td>5.23</td>\n",
       "      <td>4.34</td>\n",
       "      <td>-0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sci.electronics</th>\n",
       "      <td>5.22</td>\n",
       "      <td>5.71</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.ibm.pc.hardware</th>\n",
       "      <td>5.21</td>\n",
       "      <td>5.41</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>misc.forsale</th>\n",
       "      <td>5.17</td>\n",
       "      <td>3.67</td>\n",
       "      <td>-1.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.graphics</th>\n",
       "      <td>5.16</td>\n",
       "      <td>3.63</td>\n",
       "      <td>-1.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comp.sys.mac.hardware</th>\n",
       "      <td>5.11</td>\n",
       "      <td>4.42</td>\n",
       "      <td>-0.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.mideast</th>\n",
       "      <td>4.99</td>\n",
       "      <td>5.67</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.guns</th>\n",
       "      <td>4.83</td>\n",
       "      <td>5.39</td>\n",
       "      <td>0.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alt.atheism</th>\n",
       "      <td>4.24</td>\n",
       "      <td>4.73</td>\n",
       "      <td>0.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.politics.misc</th>\n",
       "      <td>4.11</td>\n",
       "      <td>4.85</td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talk.religion.misc</th>\n",
       "      <td>3.33</td>\n",
       "      <td>2.66</td>\n",
       "      <td>-0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          До, %  После, %  Отклонение, %\n",
       "label                                                   \n",
       "rec.sport.hockey           5.30      5.43           0.13\n",
       "soc.religion.christian     5.29      6.51           1.22\n",
       "rec.motorcycles            5.28      4.79          -0.50\n",
       "rec.sport.baseball         5.27      4.60          -0.68\n",
       "sci.crypt                  5.26      6.59           1.34\n",
       "rec.autos                  5.25      4.81          -0.44\n",
       "sci.med                    5.25      5.95           0.70\n",
       "comp.windows.x             5.24      5.80           0.56\n",
       "sci.space                  5.24      5.03          -0.21\n",
       "comp.os.ms-windows.misc    5.23      4.34          -0.89\n",
       "sci.electronics            5.22      5.71           0.49\n",
       "comp.sys.ibm.pc.hardware   5.21      5.41           0.20\n",
       "misc.forsale               5.17      3.67          -1.50\n",
       "comp.graphics              5.16      3.63          -1.53\n",
       "comp.sys.mac.hardware      5.11      4.42          -0.69\n",
       "talk.politics.mideast      4.99      5.67           0.68\n",
       "talk.politics.guns         4.83      5.39           0.56\n",
       "alt.atheism                4.24      4.73           0.49\n",
       "talk.politics.misc         4.11      4.85           0.74\n",
       "talk.religion.misc         3.33      2.66          -0.67"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сократим диапазон до заданного в курсовой\n",
    "before = ng_df[\"label\"].value_counts(normalize=True) * 100 # пропорции до\n",
    "ng_df = ng_df[ng_df[\"text\"].str.strip().str.split().str.len().between(120, 300)].reset_index(drop=True) # сужаем диапазон до заданного\n",
    "after = ng_df[\"label\"].value_counts(normalize=True) * 100 # пропорции после\n",
    "\n",
    "comparison = pd.concat([before, after], axis=1, keys=[\"До, %\", \"После, %\"]) # общая таблица\n",
    "comparison[\"Отклонение, %\"] = (comparison[\"После, %\"] - comparison[\"До, %\"]) # столбик с отклонением\n",
    "comparison = comparison.round(2) # округляем до 2 знаков после запятой\n",
    "\n",
    "print(\"Пропорции классов: до и после\")\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44689246-c004-41d6-a396-754ac5fe0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Статистика по текстам датасета 20_Newsgroups:\n",
      "Мининимум: 120 слов; Среднее: 183.87306701030928 слов; Максимум: 300 слов\n"
     ]
    }
   ],
   "source": [
    "# проверим что фильтрация применилась\n",
    "word_counts = ng_df[\"text\"].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(f\"Статистика по текстам датасета 20_Newsgroups:\")\n",
    "print(f\"Мининимум: {word_counts.min()} слов; Среднее: {word_counts.mean()} слов; Максимум: {word_counts.max()} слов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8cc4d52a-967c-49b1-be00-58e3491e6c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wv/579phdqx14xch2yg0qz2qr940000gn/T/ipykernel_93474/761208721.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  ng_balanced = ng_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "label\n",
       "alt.atheism                 5.0\n",
       "comp.graphics               5.0\n",
       "talk.politics.misc          5.0\n",
       "talk.politics.mideast       5.0\n",
       "talk.politics.guns          5.0\n",
       "soc.religion.christian      5.0\n",
       "sci.space                   5.0\n",
       "sci.med                     5.0\n",
       "sci.electronics             5.0\n",
       "sci.crypt                   5.0\n",
       "rec.sport.hockey            5.0\n",
       "rec.sport.baseball          5.0\n",
       "rec.motorcycles             5.0\n",
       "rec.autos                   5.0\n",
       "misc.forsale                5.0\n",
       "comp.windows.x              5.0\n",
       "comp.sys.mac.hardware       5.0\n",
       "comp.sys.ibm.pc.hardware    5.0\n",
       "comp.os.ms-windows.misc     5.0\n",
       "talk.religion.misc          5.0\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# дополнительно сбланарсируем классы\n",
    "min_count = ng_df[\"label\"].value_counts().min()\n",
    "\n",
    "ng_balanced = ng_df.groupby(\"label\", group_keys=False).apply(lambda x: x.sample(n=min_count, random_state=42)).reset_index(drop=True)\n",
    "\n",
    "ng_balanced[\"label\"].value_counts(normalize=True) * 100 # пропорции после балансировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "806c88ad-6dc5-422d-a38e-8f196635a8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20newsgroups/train.csv успешно сохранен\n",
      "20newsgroups/test.csv успешно сохранен\n"
     ]
    }
   ],
   "source": [
    "ng_train, ng_test = train_test_split(ng_balanced, test_size=0.3, stratify=ng_balanced[\"label\"], random_state=42) # вручную разбиваем на train и test\n",
    "dataset = {\"train\": ng_train, \"test\": ng_test} # создаем общий датасет чтобы передать в функцию\n",
    "\n",
    "save_dataset(dataset, \"train\", \"20newsgroups\") \n",
    "save_dataset(dataset, \"test\", \"20newsgroups\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a4d993-cff0-4baa-b6b2-b457176020b3",
   "metadata": {},
   "source": [
    "## Шаг 0.5: получение оптимальных комбинаций препроцессинга для недостающих моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3b46a1-ee36-41d3-872b-a69772eefd5a",
   "metadata": {},
   "source": [
    "Для получения данных для наших датасетов и моделей модифицируем [исходный код](https://github.com/marco-siino/text_preprocessing_impact/tree/main) авторов [статьи](https://www.sciencedirect.com/science/article/pii/S0306437923001783?ref=cra_js_challenge&fr=RR-1 ) и запустим собственный эксперимент.\n",
    "\n",
    "С целью не перегружать текущий ноутбук перебор функций препроцессинга для каждого датасета был выполнен в отдельных ноутбуках. Посмотреть их можно, перейдя по ссылкам ниже:\n",
    "1. [TweetEval: Sentiment](preprocessing/TweetEval_Sentiment.ipynb);\n",
    "2. [AG News](preprocessing/AG_News.ipynb);\n",
    "3. [20_Newsgroups](preprocessing/20newsgroups.ipynb);\n",
    "\n",
    "Кроме того, результаты перебора сохранены в таблицах. Посмотреть их также можно, перейдя по ссылкам ниже:\n",
    "1. [TweetEval: Sentiment](reports/preprocessing_combinations/tweet_eval_sentiment/full.csv);\n",
    "2. [AG News](reports/preprocessing_combinations/20newsgroups/full.csv);\n",
    "3. [20_Newsgroups](reports/preprocessing_combinations/reuters21578/full.csv);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530a3e20-625e-49d5-971b-e2c06d3b3c24",
   "metadata": {},
   "source": [
    "## Шаг 1: Применение оптимальных методов предобработки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f121858-df07-47bd-b08d-d41d30ef870c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/stepan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import unicodedata\n",
    "import contractions\n",
    "from spellchecker import SpellChecker\n",
    "import demoji\n",
    "import wordninja\n",
    "\n",
    "from pathlib import Path\n",
    "import html\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ff182f4d-f72d-46c0-a80e-26352d7869b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\" # загружаем словарь\n",
    ")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b84031f-8f4a-4e39-88dd-cbddd8320889",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def correct_spell(text):\n",
    "    words = text.split()\n",
    "    corrected = []\n",
    "    for word in words:\n",
    "        # получаем предложение по исправлению (VERBOSITY вернет наиболее вероятное)\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2) \n",
    "        # если найдено исправление - берем, нет - оставляем исходное слово\n",
    "        corrected_word = suggestions[0].term if suggestions else word \n",
    "        corrected.append(corrected_word)\n",
    "    return \" \".join(corrected)\n",
    "\n",
    "# нормализация текста (применяется ко всем следующим методам по умолчанию)\n",
    "def DON(text, dataset_name=None):\n",
    "    text = html.unescape(text) # конвертируем html-теги в нормальные символы\n",
    "    text = unicodedata.normalize(\"NFKD\", text).encode(\"ascii\", \"ignore\").decode() # удаляем все не-ASCII символы\n",
    "    text = re.sub(r\"<\\!\\[CDATA\\[\", \" \", text) # удаляем открывающий тег <![CDATA[ из XML\n",
    "    text = re.sub(r\"\\]{1,}>\", \" \", text) # удаляем закрывающие символы ]]\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text) # удаляем все HTML и XML теги\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \"URL\", text) # заменяем ссылки на обозначение URL\n",
    "    text = re.sub(r\"@\\w+\", \"USER\", text)  # заменяем упоминания\n",
    "    text = re.sub(r\"#(\\w+)\", lambda m: ' '.join(wordninja.split(m.group(1))), text) # обрабатываем хэштеги\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text) # удаляем спецсимволы и пунктуацию\n",
    "    text = re.sub(r\"\\n|\\t\", \" \", text) # удаляем перевод строк и табуляцию\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text) # удаляем множетсвенные пробелы\n",
    "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text) # удаляем растяжения\n",
    "\n",
    "    if dataset_name != \"ccdv_arxiv-classification\": # для научных статей следующие операции бессмысленны + тратят очень ресурсов\n",
    "        text = demoji.replace_with_desc(text) # заменяем эмодзи текстовым описанием\n",
    "        text = contractions.fix(text) # заменяем сокращения\n",
    "        text = correct_spell(text) # исправляем орфографию\n",
    "    return text.strip()\n",
    "\n",
    "# приведение к нижнему регистру\n",
    "def LOW(text):\n",
    "    return text.lower()\n",
    "\n",
    "# удаление стоп-слов\n",
    "def RSW(text):\n",
    "    words = text.split()\n",
    "    clear = [w for w in words if w.lower() not in stop_words]\n",
    "    return \" \".join(clear)\n",
    "\n",
    "# стемминг (приведение к основе слова)\n",
    "def STM(text):\n",
    "    words = text.split()\n",
    "    stemmed = [stemmer.stem(w) for w in words]\n",
    "    return \" \".join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "db1517b3-3485-4b43-9c81-c36175d4c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepo_funcs = {\n",
    "    \"DON\": DON,\n",
    "    \"LOW\": LOW,\n",
    "    \"RSW\": RSW,\n",
    "    \"STM\": STM\n",
    "}\n",
    "\n",
    "optimal_prepo = {\n",
    "    \"tweet_eval_sentiment\": {\n",
    "        \"NB\": \"RSW - LOW\",\n",
    "        \"SVM\": \"RSW\",\n",
    "        \"LR\": \"RSW - STM - LOW\",\n",
    "        \"AdaBoost\": \"RSW - LOW\",\n",
    "        \"XGBoost\": \"RSW - LOW\",\n",
    "        \"RF\": \"DON\",\n",
    "        \"DT\": \"RSW - STM - LOW\"\n",
    "    },\n",
    "    \"ag_news\": {\n",
    "        \"NB\": \"LOW\",\n",
    "        \"SVM\": \"DON\",\n",
    "        \"LR\": \"DON\",\n",
    "        \"AdaBoost\": \"STM\",\n",
    "        \"XGBoost\": \"RSW\",\n",
    "        \"RF\": \"RSW\",\n",
    "        \"DT\": \"DON\"\n",
    "    },\n",
    "    \"reuters21578_ModLewis\": {\n",
    "        \"NB\": \"RSW - LOW\",\n",
    "        \"SVM\": \"RSW - LOW\",\n",
    "        \"LR\": \"LOW - RSW\",\n",
    "        \"AdaBoost\": \"RSW - LOW\",\n",
    "        \"XGBoost\": \"DON\",\n",
    "        \"RF\": \"RSW - LOW\",\n",
    "        \"DT\": \"RSW\"\n",
    "    },\n",
    "    \"ccdv_arxiv-classification\": {\n",
    "        \"NB\": \"DON\",\n",
    "        \"SVM\": \"DON\",\n",
    "        \"LR\": \"DON\",\n",
    "        \"AdaBoost\": \"RSW - LOW\",\n",
    "        \"XGBoost\": \"RSW\",\n",
    "        \"RF\": \"LOW\",\n",
    "        \"DT\": \"LOW\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"reports/preprocessing_combinations/optimal_prepo.json\", \"w\") as f:\n",
    "    json.dump(optimal_prepo, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "410bc209-2163-4f2a-8a6d-acc712a59409",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = Path(\"data/raw\")\n",
    "save_dir = Path(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e84ff6-277f-4da0-ab9b-06cb9ec4a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_model(dataset_name, model_name, prepo_combs):\n",
    "    out_dir = save_dir / dataset_name / model_name\n",
    "    train_path = out_dir / \"train_processed.csv\"\n",
    "    test_path = out_dir / \"test_processed.csv\"\n",
    "\n",
    "    if train_path.exists() and test_path.exists():\n",
    "        return f\"{dataset_name}: {model_name}: предобработка завершена. Примененные методы: {prepo_combs}\"\n",
    "\n",
    "    train_df = pd.read_csv(source_dir / dataset_name / \"train.csv\")\n",
    "    test_df = pd.read_csv(source_dir / dataset_name / \"test.csv\")\n",
    "\n",
    "    # парсим наши техники предобработки из словаря\n",
    "    steps = [s.strip() for s in prepo_combs.split('-')]\n",
    "    funcs = [prepo_funcs[s] for s in steps]\n",
    "\n",
    "    # функция применяет каждый метод предобработки, который спарсили из словаря, для текста\n",
    "    def apply_prepo(text, funcs, dataset_name): \n",
    "        text = DON(text, dataset_name=dataset_name)\n",
    "        if DON not in funcs:\n",
    "            for f in funcs:\n",
    "                text = f(text)\n",
    "        return text\n",
    "\n",
    "    train_proc = train_df.copy()\n",
    "    test_proc = test_df.copy()\n",
    "\n",
    "    # применяем препроцессинг к колонке с текстом\n",
    "    train_proc[\"text\"] = train_proc[\"text\"].astype(str).apply(lambda x: apply_prepo(x, funcs, dataset_name))\n",
    "    test_proc[\"text\"] = test_proc[\"text\"].astype(str).apply(lambda x: apply_prepo(x, funcs, dataset_name))\n",
    "\n",
    "    # сохраняем результат предобработки\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    train_proc.to_csv(train_path, index=False)\n",
    "    test_proc.to_csv(test_path, index=False)\n",
    "\n",
    "    return f\"{dataset_name}: {model_name}: предобработка завершена. Примененные методы: {prepo_combs}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2370160-b0bc-4d69-8cd3-f56c8db77b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet_eval_sentiment: NB: предобработка завершена. Примененные методы: RSW - LOW\n",
      "tweet_eval_sentiment: SVM: предобработка завершена. Примененные методы: RSW\n",
      "tweet_eval_sentiment: LR: предобработка завершена. Примененные методы: RSW - STM - LOW\n",
      "tweet_eval_sentiment: AdaBoost: предобработка завершена. Примененные методы: RSW - LOW\n",
      "tweet_eval_sentiment: XGBoost: предобработка завершена. Примененные методы: RSW - LOW\n",
      "tweet_eval_sentiment: RF: предобработка завершена. Примененные методы: DON\n",
      "tweet_eval_sentiment: DT: предобработка завершена. Примененные методы: RSW - STM - LOW\n",
      "ag_news: NB: предобработка завершена. Примененные методы: LOW\n",
      "ag_news: SVM: предобработка завершена. Примененные методы: DON\n",
      "ag_news: LR: предобработка завершена. Примененные методы: DON\n",
      "ag_news: AdaBoost: предобработка завершена. Примененные методы: STM\n",
      "ag_news: XGBoost: предобработка завершена. Примененные методы: RSW\n",
      "ag_news: RF: предобработка завершена. Примененные методы: RSW\n",
      "ag_news: DT: предобработка завершена. Примененные методы: DON\n",
      "reuters21578_ModLewis: NB: предобработка завершена. Примененные методы: RSW - LOW\n",
      "reuters21578_ModLewis: SVM: предобработка завершена. Примененные методы: RSW - LOW\n",
      "reuters21578_ModLewis: LR: предобработка завершена. Примененные методы: LOW - RSW\n",
      "reuters21578_ModLewis: AdaBoost: предобработка завершена. Примененные методы: RSW - LOW\n",
      "reuters21578_ModLewis: XGBoost: предобработка завершена. Примененные методы: DON\n",
      "reuters21578_ModLewis: RF: предобработка завершена. Примененные методы: RSW - LOW\n",
      "reuters21578_ModLewis: DT: предобработка завершена. Примененные методы: RSW\n",
      "ccdv_arxiv-classification: NB: предобработка завершена. Примененные методы: DON\n",
      "ccdv_arxiv-classification: SVM: предобработка завершена. Примененные методы: DON\n",
      "ccdv_arxiv-classification: LR: предобработка завершена. Примененные методы: DON\n",
      "ccdv_arxiv-classification: AdaBoost: предобработка завершена. Примененные методы: RSW - LOW\n",
      "ccdv_arxiv-classification: XGBoost: предобработка завершена. Примененные методы: RSW\n",
      "ccdv_arxiv-classification: RF: предобработка завершена. Примененные методы: LOW\n",
      "ccdv_arxiv-classification: DT: предобработка завершена. Примененные методы: LOW\n"
     ]
    }
   ],
   "source": [
    "# создаем список задач для параллельного запуска\n",
    "# изначально планировалось использовать хотя 2 ядра, но при n_job=2 озу быстро заканчивается из-за чего машина уходит в перезагрузку\n",
    "tasks = [\n",
    "    delayed(process_model)(dataset_name, model_name, prepo_comb)\n",
    "    for dataset_name, model_map in optimal_prepo.items() # проходимся по датасетам\n",
    "    for model_name, prepo_comb in model_map.items() # проходимся по моделям и методам предобработки\n",
    "]\n",
    "\n",
    "results = Parallel(n_jobs=1, backend=\"loky\")(tasks)\n",
    "for res in results:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f7607-aa0e-47ee-a7b1-c35f695e6df0",
   "metadata": {},
   "source": [
    "## Шаг 2: векторизация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "517fadab-2da2-4094-baf4-7cc1ed0a0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from joblib import load\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "import gensim.downloader as api\n",
    "from scipy import sparse\n",
    "\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b4d52fe-6bd4-4060-9fd9-4134502dc9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = Path(\"data/processed\")\n",
    "save_dir = Path(\"data/vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c5785f7-0236-4cb1-b113-3bfe25c7df85",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 50000\n",
    "emb_size = 300\n",
    "min_count = 5\n",
    "workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "780f0678-7036-4038-8f16-b9a132af8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_weighted_avg(tokenized_texts, keyed_vectors, tfidf_vectorizer, dim):\n",
    "    tfidf_matrix = tfidf_vectorizer.transform([\" \".join(tokens) for tokens in tokenized_texts])\n",
    "    vocab = tfidf_vectorizer.vocabulary_\n",
    "    result = np.zeros((len(tokenized_texts), dim), dtype=np.float32)\n",
    "\n",
    "    for i, tokens in enumerate(tokenized_texts):\n",
    "        vecs = []\n",
    "        weights = []\n",
    "        for word in tokens:\n",
    "            if word in keyed_vectors and word in vocab:\n",
    "                tfidf_weight = tfidf_matrix[i, vocab[word]]\n",
    "                vecs.append(keyed_vectors[word])\n",
    "                weights.append(tfidf_weight)\n",
    "        if vecs:\n",
    "            vecs = np.array(vecs)\n",
    "            weights = np.array(weights)\n",
    "            result[i] = np.average(vecs, axis=0, weights=weights)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "387492d4-59d2-4aaf-a791-44d1ca7a4de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Датасет tweet_eval_sentiment\n",
      "\n",
      "\tDT: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "[===-----------------------------------------------] 6.2% 23.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[========------------------------------------------] 17.0% 63.9/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===========---------------------------------------] 22.7% 85.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================---------------------------------] 34.7% 130.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[====================------------------------------] 41.8% 157.1/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================------------------------] 53.3% 200.5/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=============================---------------------] 59.4% 223.4/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=================================-----------------] 67.8% 255.2/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=====================================-------------] 75.6% 284.2/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==========================================--------] 85.2% 320.6/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==============================================----] 93.7% 352.2/376.1MB downloaded"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tXGBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tRF: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tSVM: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tAdaBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tNB: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tLR: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "Датасет reuters21578_ModLewis\n",
      "\n",
      "\tDT: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tXGBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tRF: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tSVM: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tAdaBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tNB: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tLR: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "Датасет ag_news\n",
      "\n",
      "\tDT: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tXGBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tRF: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tSVM: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tAdaBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tNB: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tLR: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "Датасет ccdv_arxiv-classification\n",
      "\n",
      "\tDT: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tXGBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tRF: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tSVM: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tAdaBoost: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tNB: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n",
      "\n",
      "\tLR: начало перебора эмбеддингов\n",
      "\n",
      "\t\tТип векторизации: bow: запущен\n",
      "\t\tТип векторизации: bow: завершен\n",
      "\n",
      "\t\tТип векторизации: tfidf: запущен\n",
      "\t\tТип векторизации: tfidf: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_cbow: запущен\n",
      "\t\tТип векторизации: word2vec_cbow: завершен\n",
      "\n",
      "\t\tТип векторизации: word2vec_sg: запущен\n",
      "\t\tТип векторизации: word2vec_sg: завершен\n",
      "\n",
      "\t\tТип векторизации: GloVe: запущен\n",
      "\t\tТип векторизации: GloVe: завершен\n",
      "\n",
      "\t\tТип векторизации: FastText: запущен\n",
      "\t\tТип векторизации: FastText: завершен\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dataset_dir in source_dir.iterdir():\n",
    "    if not dataset_dir.is_dir():\n",
    "        continue\n",
    "        \n",
    "    dataset_name = dataset_dir.name\n",
    "    print(f\"Датасет {dataset_name}\")\n",
    "\n",
    "    for model_dir in dataset_dir.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        model_name = model_dir.name\n",
    "\n",
    "        df_train = pd.read_csv(model_dir / \"train_processed.csv\")\n",
    "        df_test = pd.read_csv(model_dir / \"test_processed.csv\")\n",
    "\n",
    "        text_train = df_train[\"text\"].astype(str).tolist()\n",
    "        text_test = df_test[\"text\"].astype(str).tolist()\n",
    "\n",
    "        base_target_dir = save_dir / dataset_name / model_name\n",
    "\n",
    "        print(f\"\\n\\t{model_name}: начало перебора эмбеддингов\\n\")\n",
    "\n",
    "        # BOW и TF-IDF\n",
    "        for name, vec in [\n",
    "            (\"bow\", CountVectorizer(max_features=max_features, ngram_range=(1,2))),\n",
    "            (\"tfidf\", TfidfVectorizer(max_features=max_features, ngram_range=(1,2)))\n",
    "        ]:\n",
    "            print(f\"\\t\\tТип векторизации: {name}: запущен\")\n",
    "\n",
    "            out_dir = base_target_dir / name\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            if ((out_dir / \"train_vectorized.npz\").exists() and \n",
    "                (out_dir / \"test_vectorized.npz\").exists() and \n",
    "                (out_dir / f\"{name}.joblib\").exists()):\n",
    "                print(f\"\\t\\tТип векторизации: {name}: завершен\\n\")\n",
    "                continue\n",
    "                            \n",
    "            X_train = vec.fit_transform(text_train)\n",
    "            X_test = vec.transform(text_test)\n",
    "\n",
    "            sparse.save_npz(out_dir / \"train_vectorized.npz\", X_train)\n",
    "            sparse.save_npz(out_dir / \"test_vectorized.npz\", X_test)\n",
    "            joblib.dump(vec, out_dir / f\"{name}.joblib\")\n",
    "\n",
    "            print(f\"\\t\\tТип векторизации: {name}: завершен\\n\")\n",
    "\n",
    "        # Токенизируем текст для перед плотной векторизацией\n",
    "        tokens_train = [word_tokenize(t) for t in text_train]\n",
    "        tokens_test = [word_tokenize(t) for t in text_test]\n",
    "\n",
    "        # WORD2VEC\n",
    "        for name, sg in [(\"word2vec_cbow\", 0), (\"word2vec_sg\", 1)]:\n",
    "            print(f\"\\t\\tТип векторизации: {name}: запущен\")\n",
    "\n",
    "            out_dir = base_target_dir / name\n",
    "            os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "            if ((out_dir / \"train_vectorized.npy\").exists() and \n",
    "                (out_dir / \"test_vectorized.npy\").exists() and \n",
    "                (out_dir / f\"{name}.kv\").exists()):\n",
    "                print(f\"\\t\\tТип векторизации: {name}: завершен\\n\")\n",
    "                continue\n",
    "            \n",
    "            window = 4 if sg == 0 else 10\n",
    "            negative = 5 if sg == 0 else 10\n",
    "            \n",
    "            w2v = Word2Vec(\n",
    "                sentences=tokens_train,\n",
    "                vector_size=emb_size,\n",
    "                window=window,\n",
    "                min_count=min_count,\n",
    "                workers=workers,\n",
    "                sg=sg,\n",
    "                epochs=5,\n",
    "                negative=negative,\n",
    "                sample=1e-3\n",
    "            )\n",
    "\n",
    "            tfidf_vec = load(base_target_dir / \"tfidf\" / \"tfidf.joblib\")\n",
    "\n",
    "            X_train = tfidf_weighted_avg(tokens_train, w2v.wv, tfidf_vec, emb_size)\n",
    "            X_test = tfidf_weighted_avg(tokens_test, w2v.wv, tfidf_vec, emb_size)\n",
    "\n",
    "            np.save(out_dir / \"train_vectorized.npy\", X_train)\n",
    "            np.save(out_dir / \"test_vectorized.npy\", X_test)\n",
    "            w2v.wv.save(str(out_dir / f\"{name}.kv\"))\n",
    "\n",
    "            print(f\"\\t\\tТип векторизации: {name}: завершен\\n\")\n",
    "\n",
    "        # GLOVE\n",
    "        print(f\"\\t\\tТип векторизации: GloVe: запущен\")\n",
    "\n",
    "        out_dir = base_target_dir / \"glove\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        if ((out_dir / \"train_vectorized.npy\").exists() and \n",
    "            (out_dir / \"test_vectorized.npy\").exists() and \n",
    "            (out_dir / \"glove.kv\").exists()):\n",
    "            print(f\"\\t\\tТип векторизации: glove: завершен\\n\")\n",
    "            continue\n",
    "        \n",
    "        glove = api.load(\"glove-wiki-gigaword-300\")\n",
    "        tfidf_vec = load(base_target_dir / \"tfidf\" / \"tfidf.joblib\")\n",
    "\n",
    "        X_train = tfidf_weighted_avg(tokens_train, glove, tfidf_vec, emb_size)\n",
    "        X_test = tfidf_weighted_avg(tokens_test, glove, tfidf_vec, emb_size)\n",
    "\n",
    "        np.save(out_dir / \"train_vectorized.npy\", X_train)\n",
    "        np.save(out_dir / \"test_vectorized.npy\", X_test)\n",
    "        glove.save(str(out_dir / f\"glove.kv\"))\n",
    "        print(f\"\\t\\tТип векторизации: GloVe: завершен\\n\")\n",
    "\n",
    "        # FASTTEXT\n",
    "        print(f\"\\t\\tТип векторизации: FastText: запущен\")\n",
    "\n",
    "        out_dir = base_target_dir / \"fasttext\"\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "        if ((out_dir / \"train_vectorized.npy\").exists() and \n",
    "            (out_dir / \"test_vectorized.npy\").exists() and \n",
    "            (out_dir / \"fasttext.kv\").exists()):\n",
    "            print(f\"\\t\\tТип векторизации: FastText: завершен\\n\")\n",
    "            continue\n",
    "        \n",
    "        ft = FastText(\n",
    "            sentences=tokens_train,\n",
    "            vector_size=emb_size,\n",
    "            window=10,\n",
    "            min_count=min_count,\n",
    "            workers=workers,\n",
    "            sg=1,\n",
    "            epochs=5\n",
    "        )\n",
    "\n",
    "        tfidf_vec = load(base_target_dir / \"tfidf\" / \"tfidf.joblib\")\n",
    "\n",
    "        X_train = tfidf_weighted_avg(tokens_train, ft.wv, tfidf_vec, emb_size)\n",
    "        X_test = tfidf_weighted_avg(tokens_test, ft.wv, tfidf_vec, emb_size)\n",
    "\n",
    "        np.save(out_dir / \"train_vectorized.npy\", X_train)\n",
    "        np.save(out_dir / \"test_vectorized.npy\", X_test)\n",
    "        ft.wv.save(str(out_dir / \"fasttext.kv\"))\n",
    "        print(f\"\\t\\tТип векторизации: FastText: завершен\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e1a5e-aa9d-4851-acb1-adc1402aceb0",
   "metadata": {},
   "source": [
    "## Шаг 3: гибридизация эмбеддингов + обучение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "432c1714-4428-417e-9f1d-d4ecd89950a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2118e4fe-37ad-4e44-93bd-23773b713646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загружает векторное представление\n",
    "# возвращает загруженное векторное представление + метку: 1 - разреженный вектор, 0 - плотный вектор\n",
    "def load_vector(path):\n",
    "    if path.suffix == \".npz\":\n",
    "        X = sparse.load_npz(path)\n",
    "        return X, 1\n",
    "    elif path.suffix == \".npy\":\n",
    "        X = np.load(path)\n",
    "        return X, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e021ff46-dcb6-4435-870d-a1e19ee996a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# конкатенируют два векторных представления в зависимости от типа\n",
    "def convec(X1, vectype1, X2, vectype2):\n",
    "    if vectype1 and vectype2: # если два векторных представления - разреженные\n",
    "        return sparse.hstack([X1, X2])\n",
    "    elif vectype1 and not vectype2: # если первое векторное представление - разреженное, а второе - плотное\n",
    "        return sparse.hstack([X1, sparse.csr_matrix(X2)])\n",
    "    elif not vectype1 and vectype2: # если первое векторное представление - плотное, а второе - разреженное\n",
    "        return sparse.hstack([sparse.csr_matrix(X1), X2])\n",
    "    else: # если два векторных представления - плотные\n",
    "        return np.hstack([X1, X2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e30d21b-6c5c-40e6-90d6-e5e69a2743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model_name):\n",
    "    models = {\n",
    "        \"NB\": MultinomialNB(),\n",
    "        \"SVM\": LinearSVC(random_state=0),\n",
    "        \"LR\": LogisticRegression(random_state=0, max_iter=1000),\n",
    "        \"AdaBoost\": AdaBoostClassifier(n_estimators=200, random_state=0),\n",
    "        \"XGBoost\": XGBClassifier(n_estimators=200, learning_rate=0.1, max_depth=6, subsample=0.9, tree_method=\"hist\", random_state=0),\n",
    "        \"RF\": RandomForestClassifier(n_estimators=200, max_depth=20, random_state=0),\n",
    "        \"DT\": DecisionTreeClassifier(max_depth=20, random_state=0)\n",
    "    }\n",
    "    return models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "971ba7e9-8364-40bf-8d54-5738fbca969a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция загружает метки классов\n",
    "# для датасета reuters21578 создается маска для фильтрации пустых значений меток\n",
    "def load_labels(csv_path, dataset_name):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if dataset_name == \"reuters21578_ModLewis\": # единственный многоклассовый (multi-label) датасет, у которого метки в колонке с названием topics \n",
    "        # преобразуем строку с темами в массив\n",
    "        df[\"topics\"] = df[\"topics\"].apply(\n",
    "            lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith(\"[\") else []\n",
    "        )\n",
    "        # маска позволяет отфильтровать пустые строки, оставив только те, где topics не пуст\n",
    "        mask = df[\"topics\"].apply(lambda x: isinstance(x, list) and len(x) > 0)\n",
    "        df = df[mask].copy()\n",
    "\n",
    "        # берем первую тему как метку\n",
    "        df[\"label\"] = df[\"topics\"].apply(lambda x: x[0])\n",
    "        labels = df[\"label\"].astype(str).values\n",
    "        return labels, mask.values # маска для фильтрации пустых строк\n",
    "    else:\n",
    "        labels = df[\"label\"].astype(str).values\n",
    "        return labels, np.ones(len(labels), dtype=bool) # маска состоящая из единиц, то есть без фильтрации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "27b533dc-2905-4065-9d16-a87daabf72f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "solo_vecs = [\"bow\", \"tfidf\", \"word2vec_cbow\", \"word2vec_sg\", \"glove\", \"fasttext\"]\n",
    "hybrid_pairs = list(itertools.combinations(solo_vecs, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "48d4f966-c16b-4097-a55e-794751a26b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = {\n",
    "    \"tweet_eval_sentiment\": \"very_small\",\n",
    "    \"ag_news\": \"small\",\n",
    "    \"reuters21578_ModLewis\": \"medium\",\n",
    "    \"ccdv_arxiv-classification\": \"large\"\n",
    "}\n",
    "with open(\"reports/preprocessing_combinations/optimal_prepo.json\") as f:\n",
    "    optimal_prepo = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4e1c64e6-b81b-4202-af0a-e5ce6ad290e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_root = Path(\"data/vectorized\")\n",
    "results_dir = Path(\"reports/vectorizing_combinations\")\n",
    "models_dir = Path(\"models\")\n",
    "processed_root = Path(\"data/processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b32b56e4-c58e-485a-9d3e-f79abd25eca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dataset_name, model_name, prepo_comb, vecs,\n",
    "              y_train_full, y_test_full, mask_train, mask_test,\n",
    "              vec1, vec2):\n",
    "    \n",
    "    model_path = models_dir / dataset_name / model_name\n",
    "    model_file = model_path / f\"{model_name}_{prepo_comb}_{vec1}-{vec2}.joblib\"\n",
    "    if model_file.exists(): # пропускаем обучение для уже существующих моделей\n",
    "        return None\n",
    "        \n",
    "    vec1_X_train, vec1_X_test, vectype1 = vecs[vec1]\n",
    "    vec1_X_train = vec1_X_train[mask_train] # фильтруем строки с пустыми темами текстов\n",
    "    vec1_X_test = vec1_X_test[mask_test]\n",
    "\n",
    "    vec2_X_train, vec2_X_test, vectype2 = vecs[vec2]\n",
    "    vec2_X_train = vec2_X_train[mask_train] # фильтруем строки с пустыми темами текстов\n",
    "    vec2_X_test = vec2_X_test[mask_test]\n",
    "\n",
    "    y_train = y_train_full[mask_train]\n",
    "    y_test = y_test_full[mask_test]\n",
    "\n",
    "    # создаем гибрид путем конкатенации векторных представлений\n",
    "    X_train = convec(vec1_X_train, vectype1, vec2_X_train, vectype2)\n",
    "    X_test = convec(vec1_X_test, vectype1, vec2_X_test, vectype2)\n",
    "\n",
    "    if model_name == \"NB\": # на всякий случай обрабатываем отрицательные значения \n",
    "        scaler = MaxAbsScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "\n",
    "    # обучаем модель\n",
    "    model = build_model(model_name)\n",
    "    start_time = time.perf_counter() # наиболее точный замер времени\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.perf_counter() - start_time\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "\n",
    "    # сохраняем обученную модель\n",
    "    os.makedirs(model_path, exist_ok=True)\n",
    "    joblib.dump(model, model_file, compress=3)\n",
    "\n",
    "    gc.collect() # освобождаем оперативную память после обучения модели на гибриде\n",
    "\n",
    "    # делаем запись в будущую таблицу\n",
    "    return {\n",
    "            \"dataset\": dataset_name,\n",
    "            \"size\": size,\n",
    "            \"model\": model_name,\n",
    "            \"prepo_funcs\": prepo_comb,\n",
    "            \"hybrid\": f\"{vec1}+{vec2}\",\n",
    "            \"accuracy\": round(accuracy, 3),\n",
    "            \"recall\": round(recall, 3),\n",
    "            \"f1\": round(f1, 3),\n",
    "            \"train_time\": round(train_time, 5)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d8992f52-38ae-489b-af7a-a47b3c2290d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tweet_eval_sentiment: начало обучения\n",
      "\n",
      "\tDT: обработано: 0/15 гибридов, пропущено: 15/15 гибридов\n",
      "\tXGBoost: обработано: 0/15 гибридов, пропущено: 15/15 гибридов\n",
      "\tRF: обработано: 0/15 гибридов, пропущено: 15/15 гибридов\n",
      "\tSVM: обработано: 0/15 гибридов, пропущено: 15/15 гибридов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/conda/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m valid_pairs \u001b[38;5;241m=\u001b[39m [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m hybrid_pairs \u001b[38;5;28;01mif\u001b[39;00m p[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m vecs \u001b[38;5;129;01mand\u001b[39;00m p[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m vecs] \u001b[38;5;66;03m# считаем количество возможных гибридных пар\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# параллельно обучаем все пары, так как последовательно получается долго\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloky\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepo_comb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvecs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_full\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec2\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvec1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalid_pairs\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m results \u001b[38;5;241m=\u001b[39m [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     46\u001b[0m records\u001b[38;5;241m.\u001b[39mextend(results)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/conda/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/conda/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/conda/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dataset_dir in vec_root.iterdir():\n",
    "    if not dataset_dir.is_dir(): # пропускаем системные файлы (macos создает скрытые .DS_Store)\n",
    "        continue\n",
    "    dataset_name = dataset_dir.name\n",
    "    size = dataset_size[dataset_name]\n",
    "    records = []\n",
    "\n",
    "    print(f\"\\n{dataset_name}: начало обучения\\n\")\n",
    "\n",
    "    for model_dir in dataset_dir.iterdir():\n",
    "        if not model_dir.is_dir(): # пропускаем системные файлы (macos создает скрытые .DS_Store)\n",
    "            continue\n",
    "        model_name = model_dir.name\n",
    "        prepo_comb = optimal_prepo[dataset_name][model_name].replace(\" - \", \"-\")\n",
    "        vecs = {} # словарь всех векторных представлений для данной модели\n",
    "\n",
    "        # загружаем целевые переменные и маску\n",
    "        y_train_full, mask_train = load_labels(processed_root / dataset_name / model_name / \"train_processed.csv\", dataset_name)\n",
    "        y_test_full, mask_test = load_labels(processed_root / dataset_name / model_name / \"test_processed.csv\", dataset_name)\n",
    "\n",
    "        # кодируем метки\n",
    "        le = LabelEncoder()\n",
    "        y_train_full = le.fit_transform(y_train_full)\n",
    "        y_test_full  = le.transform(y_test_full)\n",
    "        \n",
    "        for vec_name in solo_vecs: # проходимся по всем одиночным представлениям и записываем в словарь с меткой\n",
    "            vec_dir = model_dir / vec_name\n",
    "            \n",
    "            X_train, vectype1 = load_vector(vec_dir / \"train_vectorized.npz\" if (vec_dir / \"train_vectorized.npz\").exists() else vec_dir / \"train_vectorized.npy\")\n",
    "            X_test, vectype2 = load_vector(vec_dir / \"test_vectorized.npz\" if (vec_dir / \"test_vectorized.npz\").exists() else vec_dir / \"test_vectorized.npy\")\n",
    "            \n",
    "            vecs[vec_name] = (X_train, X_test, vectype1)\n",
    "\n",
    "        valid_pairs = [p for p in hybrid_pairs if p[0] in vecs and p[1] in vecs] # считаем количество возможных гибридных пар\n",
    "\n",
    "        # параллельно обучаем все пары, так как последовательно получается долго\n",
    "        results = Parallel(n_jobs=2, backend=\"loky\")(\n",
    "            delayed(train_model)(\n",
    "                dataset_name, model_name, prepo_comb, vecs,\n",
    "                y_train_full, y_test_full, mask_train, mask_test,\n",
    "                vec1, vec2\n",
    "            )\n",
    "            for vec1, vec2 in valid_pairs\n",
    "        )\n",
    "        results = [r for r in results if r is not None]\n",
    "        records.extend(results)\n",
    "\n",
    "        skipped = len(valid_pairs) - len(results)\n",
    "        print(f\"\\t{model_name}: обработано: {len(results)}/{len(valid_pairs)} гибридов, пропущено: {skipped}/{len(valid_pairs)} гибридов\")\n",
    "\n",
    "    # сохраняем таблицу результатов для датасета\n",
    "    df = pd.DataFrame(records)\n",
    "    df.sort_values([\"model\", \"f1\"], ascending=[True, False], inplace=True)\n",
    "    df.to_csv(results_dir / f\"{dataset_name}_results.csv\", index=False)\n",
    "    print(f\"\\n{dataset_name}: результаты обучения успешно сохранены\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f09f60d-914b-4b6b-a5bc-68d656501726",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (conda)",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
